Project Path: d.a.t.a

Source Tree:

```
d.a.t.a
├── eliza
│   ├── plugin-d.a.t.a
│   │   ├── dist
│   │   │   ├── index.d.mts
│   │   │   ├── index.js
│   │   │   ├── index.mjs
│   │   │   └── index.d.ts
│   │   ├── node_modules
│   │   │   └── @elizaos
│   │   ├── package.json
│   │   ├── tsconfig.json
│   │   ├── vitest.config.ts
│   │   └── src
│   │       ├── providers
│   │       │   └── ethereum
│   │       │       ├── sequelize.ts
│   │       │       ├── block.ts
│   │       │       ├── txs.ts
│   │       │       └── database.ts
│   │       ├── evaluators
│   │       │   └── data_evaluator.ts
│   │       ├── actions
│   │       │   └── fetchTransaction.ts
│   │       ├── templates
│   │       │   └── index.ts
│   │       ├── index.ts
│   │       └── data_service.ts
│   └── README.md
├── script
├── README.md
├── d.a.t.a.json
└── zerepy
    └── d.a.t.a_connection.py

```

`/Users/tuyukai/go/src/github.com/carv-protocol/d.a.t.a/eliza/plugin-d.a.t.a/dist/index.d.mts`:

```mts

export {  }

```

`/Users/tuyukai/go/src/github.com/carv-protocol/d.a.t.a/eliza/plugin-d.a.t.a/dist/index.js`:

```js
var __defProp = Object.defineProperty;
var __getOwnPropDesc = Object.getOwnPropertyDescriptor;
var __getOwnPropNames = Object.getOwnPropertyNames;
var __hasOwnProp = Object.prototype.hasOwnProperty;
var __export = (target, all) => {
  for (var name in all)
    __defProp(target, name, { get: all[name], enumerable: true });
};
var __copyProps = (to, from, except, desc) => {
  if (from && typeof from === "object" || typeof from === "function") {
    for (let key of __getOwnPropNames(from))
      if (!__hasOwnProp.call(to, key) && key !== except)
        __defProp(to, key, { get: () => from[key], enumerable: !(desc = __getOwnPropDesc(from, key)) || desc.enumerable });
  }
  return to;
};
var __toCommonJS = (mod) => __copyProps(__defProp({}, "__esModule", { value: true }), mod);

// src/index.ts
var index_exports = {};
__export(index_exports, {
  onchainDataPlugin: () => onchainDataPlugin
});
module.exports = __toCommonJS(index_exports);

// src/actions/fetchTransaction.ts
var import_core = require("@elizaos/core");
var fetchTransactionAction = {
  name: "fetch_transactions",
  description: "Fetch Ethereum transactions based on various criteria",
  similes: [
    "get transactions",
    "show transfers",
    "display eth transactions",
    "find transactions",
    "search transfers",
    "check transactions",
    "view transfers",
    "list transactions",
    "recent transactions",
    "transaction history",
    "transfer records",
    "eth movements",
    "wallet activity",
    "transaction lookup",
    "transfer search"
  ],
  examples: [
    [
      {
        user: "user",
        content: {
          text: "Show me the latest 10 Ethereum transactions",
          action: "FETCH_TRANSACTIONS"
        }
      }
    ],
    [
      {
        user: "user",
        content: {
          text: "Get transactions for address 0x742d35Cc6634C0532925a3b844Bc454e4438f44e",
          action: "FETCH_TRANSACTIONS"
        }
      }
    ],
    [
      {
        user: "user",
        content: {
          text: "Show me transactions above 10 ETH from last week",
          action: "FETCH_TRANSACTIONS"
        }
      }
    ],
    [
      {
        user: "user",
        content: {
          text: "Find failed transactions with highest gas fees",
          action: "FETCH_TRANSACTIONS"
        }
      }
    ],
    [
      {
        user: "user",
        content: {
          text: "Show me transactions between blocks 1000000 and 1000100",
          action: "FETCH_TRANSACTIONS"
        }
      }
    ]
  ],
  validate: async (runtime) => {
    return true;
  },
  handler: async (runtime, message, state, _options, callback) => {
    try {
      import_core.elizaLogger.log("$$$$ Fetching Ethereum transactions...");
      import_core.elizaLogger.log("$$$$message", message);
      if (callback) {
        const mockText = "This is the transaction details of ethereum";
        const mockQuery = "SELECT * FROM ethereum_transactions";
        const mockParams = {
          limit: 10,
          orderBy: "timestamp",
          orderDirection: "DESC"
        };
        callback({
          // text: `Here's the SQL query to retrieve Ethereum transactions:\n${sqlQuery}\nThis query will return the specified transactions, including details like transaction hash, block number, sender/receiver addresses, value, and gas used. Let me know if you'd like further analysis or specific details about any of these transactions!`,
          // content: {
          //     success: true,
          //     query: sqlQuery,
          //     params: queryParams,
          // },
          text: mockText,
          content: {
            success: true,
            query: mockQuery,
            params: mockParams
          }
        });
      }
      return true;
    } catch (error) {
      import_core.elizaLogger.error("Error in fetch transaction action:", error);
      if (callback) {
        callback({
          text: `Error fetching transactions: ${error.message}`,
          content: { error: error.message }
        });
      }
      return false;
    }
  }
};

// src/providers/ethereum/database.ts
var import_core2 = require("@elizaos/core");
var DatabaseProvider = class {
  chain;
  API_URL = "https://dev-interface.carv.io/ai-agent-backend/sql_query";
  // fake data
  MOCK_RESPONSE = {
    code: 0,
    msg: "Success",
    data: {
      column_infos: [
        "hash",
        "nonce",
        "transaction_index",
        "from_address",
        "to_address",
        "value",
        "gas",
        "gas_price",
        "input",
        "receipt_cumulative_gas_used",
        "receipt_gas_used",
        "receipt_contract_address",
        "receipt_root",
        "receipt_status",
        "block_timestamp",
        "block_number",
        "block_hash",
        "max_fee_per_gas",
        "max_priority_fee_per_gas",
        "transaction_type",
        "receipt_effective_gas_price",
        "date"
      ],
      rows: [
        {
          items: [
            "0xb9f2c4dd816305a29471f7e843f33b8a4f52c24bfac58dba5f6dd703bdcc347d",
            "131",
            "0",
            "0xb7b3690efa6b3f08d4ec289ff655c4b7bb15ee39",
            "0x32be343b94f860124dc4fee278fdcbd38c102d88",
            "5.06132703E18",
            "21000",
            "58587049895",
            "0x",
            "21000",
            "21000",
            "",
            "",
            "1",
            "2015-10-18 09:01:42.000",
            "401609",
            "0xab8cf7f52769cb62a8a970347a116368c2ae08581d412573dd09a8a4af99b4cd",
            "0",
            "0",
            "0",
            "58587049895",
            "2015-10-18"
          ]
        },
        {
          items: [
            "0xbca5c575aa36f6164dbd98bf7c1008791d615cb0f255c19db7c6b45b06367ba0",
            "9573",
            "0",
            "0x2a65aca4d5fc5b5c859090a6c34d164135398226",
            "0xff3a70d8d5692dd05d71175fa29e8565f7450f57",
            "2.7443251E18",
            "90000",
            "50000000000",
            "0x",
            "21000",
            "21000",
            "",
            "",
            "1",
            "2015-10-18 12:21:43.000",
            "402253",
            "0x290dfc39bec9918fca28c8cebe2beaa19f9303f3b432d62d1e3409b1195556d6",
            "0",
            "0",
            "0",
            "50000000000",
            "2015-10-18"
          ]
        }
      ]
    }
  };
  constructor(chain) {
    this.chain = chain;
  }
  extractSQLQuery(preResponse) {
    try {
      let jsonData = preResponse;
      if (typeof preResponse === "string") {
        try {
          jsonData = JSON.parse(preResponse);
        } catch (e) {
          import_core2.elizaLogger.error(
            "Failed to parse preResponse as JSON:",
            e
          );
          return null;
        }
      }
      const findSQLQuery = (obj) => {
        if (!obj) return null;
        if (typeof obj === "string") {
          const sqlPattern = /^\s*(SELECT|WITH)\s+[\s\S]+?(?:;|$)/i;
          const commentPattern = /--.*$|\/\*[\s\S]*?\*\//gm;
          const cleanStr = obj.replace(commentPattern, "").trim();
          if (sqlPattern.test(cleanStr)) {
            const unsafeKeywords = [
              "drop",
              "delete",
              "update",
              "insert",
              "alter",
              "create"
            ];
            const isUnsafe = unsafeKeywords.some(
              (keyword) => cleanStr.toLowerCase().includes(keyword)
            );
            if (!isUnsafe) {
              return cleanStr;
            }
          }
          return null;
        }
        if (Array.isArray(obj)) {
          for (const item of obj) {
            const result = findSQLQuery(item);
            if (result) return result;
          }
          return null;
        }
        if (typeof obj === "object") {
          for (const key of Object.keys(obj)) {
            if (key.toLowerCase() === "query" && obj.sql) {
              const result = findSQLQuery(obj[key]);
              if (result) return result;
            }
          }
          for (const key of Object.keys(obj)) {
            const result = findSQLQuery(obj[key]);
            if (result) return result;
          }
        }
        return null;
      };
      const sqlQuery = findSQLQuery(jsonData);
      if (!sqlQuery) {
        import_core2.elizaLogger.warn("No valid SQL query found in preResponse");
        return null;
      }
      return sqlQuery;
    } catch (error) {
      import_core2.elizaLogger.error("Error in extractSQLQuery:", error);
      return null;
    }
  }
  async sendSqlQuery(sql, mock = false) {
    if (mock) {
      import_core2.elizaLogger.log("Using mock data for SQL query");
      return this.MOCK_RESPONSE;
    }
    try {
      const response = await fetch(this.API_URL, {
        method: "POST",
        headers: {
          "Content-Type": "application/json"
        },
        body: JSON.stringify({
          sql_content: sql
        })
      });
      if (!response.ok) {
        throw new Error(`HTTP error! status: ${response.status}`);
      }
      const data = await response.json();
      return data;
    } catch (error) {
      import_core2.elizaLogger.error("Error sending SQL query to API:", error);
      throw error;
    }
  }
  // Transform API response data
  transformApiResponse(apiResponse) {
    const { column_infos, rows } = apiResponse.data;
    return rows.map((row) => {
      const rowData = {};
      row.items.forEach((value, index) => {
        const columnName = column_infos[index];
        rowData[columnName] = value;
      });
      return rowData;
    });
  }
  // Execute query
  async executeQuery(sql) {
    try {
      if (!sql || sql.length > 5e3) {
        throw new Error("Invalid SQL query length");
      }
      const queryType = sql.toLowerCase().includes("token_transfers") ? "token" : sql.toLowerCase().includes("count") ? "aggregate" : "transaction";
      const apiResponse = await this.sendSqlQuery(sql);
      if (apiResponse.code !== 0) {
        throw new Error(`API Error: ${apiResponse.msg}`);
      }
      const transformedData = this.transformApiResponse(apiResponse);
      const queryResult = {
        success: true,
        data: transformedData,
        metadata: {
          total: transformedData.length,
          queryTime: (/* @__PURE__ */ new Date()).toISOString(),
          queryType,
          executionTime: 0,
          cached: false
        }
      };
      return queryResult;
    } catch (error) {
      import_core2.elizaLogger.error("Query execution failed:", error);
      return {
        success: false,
        data: [],
        metadata: {
          total: 0,
          queryTime: (/* @__PURE__ */ new Date()).toISOString(),
          queryType: "unknown",
          executionTime: 0,
          cached: false
        },
        error: {
          code: error.code || "EXECUTION_ERROR",
          message: error.message || "Unknown error occurred",
          details: error
        }
      };
    }
  }
  async query(sql) {
    return this.executeQuery(sql);
  }
  getDatabaseSchema() {
    return `
        CREATE EXTERNAL TABLE transactions(
            hash string,
            nonce bigint,
            block_hash string,
            block_number bigint,
            block_timestamp timestamp,
            date string,
            transaction_index bigint,
            from_address string,
            to_address string,
            value double,
            gas bigint,
            gas_price bigint,
            input string,
            max_fee_per_gas bigint,
            max_priority_fee_per_gas bigint,
            transaction_type bigint
        ) PARTITIONED BY (date string)
        ROW FORMAT SERDE 'org.apache.hadoop.hive.ql.io.parquet.serde.ParquetHiveSerDe'
        STORED AS INPUTFORMAT 'org.apache.hadoop.hive.ql.io.parquet.MapredParquetInputFormat'
        OUTPUTFORMAT 'org.apache.hadoop.hive.ql.io.parquet.MapredParquetOutputFormat';

        CREATE EXTERNAL TABLE token_transfers(
            token_address string,
            from_address string,
            to_address string,
            value double,
            transaction_hash string,
            log_index bigint,
            block_timestamp timestamp,
            date string,
            block_number bigint,
            block_hash string
        ) PARTITIONED BY (date string)
        ROW FORMAT SERDE 'org.apache.hadoop.hive.ql.io.parquet.serde.ParquetHiveSerDe'
        STORED AS INPUTFORMAT 'org.apache.hadoop.hive.ql.io.parquet.MapredParquetInputFormat'
        OUTPUTFORMAT 'org.apache.hadoop.hive.ql.io.parquet.MapredParquetOutputFormat';
        `;
  }
  getQueryExamples() {
    return `
        Common Query Examples:

        1. Find Most Active Addresses in Last 7 Days:
        WITH address_activity AS (
            SELECT
                from_address AS address,
                COUNT(*) AS tx_count
            FROM
                eth.transactions
            WHERE date_parse(date, '%Y-%m-%d') >= date_add('day', -7, current_date)
            GROUP BY
                from_address
            UNION ALL
            SELECT
                to_address AS address,
                COUNT(*) AS tx_count
            FROM
                eth.transactions
            WHERE
                date_parse(date, '%Y-%m-%d') >= date_add('day', -7, current_date)
            GROUP BY
                to_address
        )
        SELECT
            address,
            SUM(tx_count) AS total_transactions
        FROM
            address_activity
        GROUP BY
            address
        ORDER BY
            total_transactions DESC
        LIMIT 10;

        2. Analyze Address Transaction Statistics (Last 30 Days):
        WITH recent_transactions AS (
            SELECT
                from_address,
                to_address,
                value,
                block_timestamp,
                CASE
                    WHEN from_address = :address THEN 'outgoing'
                    WHEN to_address = :address THEN 'incoming'
                    ELSE 'other'
                END AS transaction_type
            FROM eth.transactions
            WHERE date >= date_format(date_add('day', -30, current_date), '%Y-%m-%d')
                AND (from_address = :address OR to_address = :address)
        )
        SELECT
            transaction_type,
            COUNT(*) AS transaction_count,
            SUM(CASE WHEN transaction_type = 'outgoing' THEN value ELSE 0 END) AS total_outgoing_value,
            SUM(CASE WHEN transaction_type = 'incoming' THEN value ELSE 0 END) AS total_incoming_value
        FROM recent_transactions
        GROUP BY transaction_type;

        3. Token Transfer Analysis:
        WITH filtered_transactions AS (
            SELECT
                token_address,
                from_address,
                to_address,
                value,
                block_timestamp
            FROM eth.token_transfers
            WHERE token_address = :token_address
                AND date >= :start_date
        )
        SELECT
            COUNT(*) AS transaction_count,
            SUM(value) AS total_transaction_value,
            MAX(value) AS max_transaction_value,
            MIN(value) AS min_transaction_value,
            MAX_BY(from_address, value) AS max_value_from_address,
            MAX_BY(to_address, value) AS max_value_to_address,
            MIN_BY(from_address, value) AS min_value_from_address,
            MIN_BY(to_address, value) AS min_value_to_address
        FROM filtered_transactions;

        Note: Replace :address, :token_address, and :start_date with actual values when querying.
        `;
  }
  getQueryTemplate() {
    return `
        # Database Schema
        {{databaseSchema}}

        # Query Examples
        {{queryExamples}}

        # User's Query
        {{userQuery}}

        # Query Guidelines:
        1. Time Range Requirements:
           - ALWAYS include time range limitations in queries
           - Default to last 3 months if no specific time range is mentioned
           - Use date_parse(date, '%Y-%m-%d') >= date_add('month', -3, current_date) for default time range
           - Adjust time range based on user's specific requirements

        2. Query Optimization:
           - Include appropriate LIMIT clauses
           - Use proper indexing columns (date, address, block_number)
           - Consider partitioning by date
           - Add WHERE clauses for efficient filtering

        3. Response Format Requirements:
           You MUST respond in the following JSON format:
           {
             "sql": {
               "query": "your SQL query string",
               "explanation": "brief explanation of the query",
               "timeRange": "specified time range in the query"
             },
             "analysis": {
               "overview": {
                 "totalTransactions": "number",
                 "timeSpan": "time period covered",
                 "keyMetrics": ["list of important metrics"]
               },
               "patterns": {
                 "transactionPatterns": ["identified patterns"],
                 "addressBehavior": ["address analysis"],
                 "temporalTrends": ["time-based trends"]
               },
               "statistics": {
                 "averages": {},
                 "distributions": {},
                 "anomalies": []
               },
               "insights": ["key insights from the data"],
               "recommendations": ["suggested actions or areas for further investigation"]
             }
           }

        4. Analysis Requirements:
           - Focus on recent data patterns
           - Identify trends and anomalies
           - Provide statistical analysis
           - Include risk assessment
           - Suggest further investigations

        Example Response:
        {
          "sql": {
            "query": "WITH recent_txs AS (SELECT * FROM eth.transactions WHERE date_parse(date, '%Y-%m-%d') >= date_add('month', -3, current_date))...",
            "explanation": "Query fetches last 3 months of transactions with aggregated metrics",
            "timeRange": "Last 3 months"
          },
          "analysis": {
            "overview": {
              "totalTransactions": 1000000,
              "timeSpan": "2024-01-01 to 2024-03-12",
              "keyMetrics": ["Average daily transactions: 11000", "Peak day: 2024-02-15"]
            },
            "patterns": {
              "transactionPatterns": ["High volume during Asian trading hours", "Weekend dips in activity"],
              "addressBehavior": ["5 addresses responsible for 30% of volume", "Increasing DEX activity"],
              "temporalTrends": ["Growing transaction volume", "Decreasing gas costs"]
            },
            "statistics": {
              "averages": {
                "dailyTransactions": 11000,
                "gasPrice": "25 gwei"
              },
              "distributions": {
                "valueRanges": ["0-1 ETH: 60%", "1-10 ETH: 30%", ">10 ETH: 10%"]
              },
              "anomalies": ["Unusual spike in gas prices on 2024-02-01"]
            },
            "insights": [
              "Growing DeFi activity indicated by smart contract interactions",
              "Whale addresses showing increased accumulation"
            ],
            "recommendations": [
              "Monitor growing gas usage trend",
              "Track new active addresses for potential market signals"
            ]
          }
        }
        `;
  }
  getAnalysisInstruction() {
    return `
            1. Data Overview:
                - Analyze the overall pattern in the query results
                - Identify key metrics and their significance
                - Note any unusual or interesting patterns

            2. Transaction Analysis:
                - Examine transaction values and their distribution
                - Analyze gas usage patterns
                - Evaluate transaction frequency and timing
                - Identify significant transactions or patterns

            3. Address Behavior:
                - Analyze address interactions
                - Identify frequent participants
                - Evaluate transaction patterns for specific addresses
                - Note any suspicious or interesting behavior

            4. Temporal Patterns:
                - Analyze time-based patterns
                - Identify peak activity periods
                - Note any temporal anomalies
                - Consider seasonal or cyclical patterns

            5. Token Analysis (if applicable):
                - Examine token transfer patterns
                - Analyze token holder behavior
                - Evaluate token concentration
                - Note significant token movements

            6. Statistical Insights:
                - Provide relevant statistical measures
                - Compare with typical blockchain metrics
                - Highlight significant deviations
                - Consider historical context

            7. Risk Assessment:
                - Identify potential suspicious activities
                - Note any unusual patterns
                - Flag potential security concerns
                - Consider regulatory implications

            Please provide a comprehensive analysis of the Ethereum blockchain data based on these ethereum information.
            Focus on significant patterns, anomalies, and insights that would be valuable for understanding the blockchain activity.
            Use technical blockchain terminology and provide specific examples from the data to support your analysis.

            Note: This analysis is based on simulated data for demonstration purposes.
        `;
  }
};
var databaseProvider = (runtime) => {
  const chain = "ethereum-mainnet";
  return new DatabaseProvider(chain);
};
var ethereumDataProvider = {
  get: async (runtime, message, state) => {
    import_core2.elizaLogger.log("%%%% Pis Retrieving from ethereum data provider...");
    try {
      const provider = databaseProvider(runtime);
      const schema = provider.getDatabaseSchema();
      const examples = provider.getQueryExamples();
      const template = provider.getQueryTemplate();
      if (!state) {
        state = await runtime.composeState(message);
      } else {
        state = await runtime.updateRecentMessageState(state);
      }
      import_core2.elizaLogger.log("%%%%&& Pis Context:", message.content.text);
      const buildContext = template.replace("{{databaseSchema}}", schema).replace("{{queryExamples}}", examples).replace("{{userQuery}}", message.content.text || "");
      const context = JSON.stringify({
        user: runtime.agentId,
        content: buildContext,
        action: "fetch_transactions"
      });
      import_core2.elizaLogger.log("%%%% Pis Generated database context");
      const preResponse = await (0, import_core2.generateMessageResponse)({
        runtime,
        context,
        modelClass: import_core2.ModelClass.LARGE
      });
      const userMessage = {
        agentId: runtime.agentId,
        roomId: message.roomId,
        userId: message.userId,
        content: message.content
      };
      const preResponseMessage = {
        id: (0, import_core2.stringToUuid)(message.id + "-" + runtime.agentId),
        ...userMessage,
        userId: runtime.agentId,
        content: preResponse,
        embedding: (0, import_core2.getEmbeddingZeroVector)(),
        createdAt: Date.now()
      };
      await runtime.messageManager.createMemory(preResponseMessage);
      await runtime.updateRecentMessageState(state);
      import_core2.elizaLogger.log("**** Pis preResponse", preResponse);
      const sqlQuery = provider.extractSQLQuery(preResponse);
      if (sqlQuery) {
        import_core2.elizaLogger.log("%%%% Found SQL query:", sqlQuery);
        const analysisInstruction = provider.getAnalysisInstruction();
        try {
          const queryResult = await provider.query(sqlQuery);
          import_core2.elizaLogger.log("%%%% Pis queryResult", queryResult);
          return `
                    # query by user
                    ${message.content.text}

                    # query result
                    ${JSON.stringify(queryResult, null, 2)}

                    # Analysis Instructions
                    ${analysisInstruction}
                    `;
        } catch (error) {
          import_core2.elizaLogger.error("Error executing query:", error);
          return context;
        }
      } else {
        import_core2.elizaLogger.log("%%%% Pis no SQL query found");
      }
      return context;
    } catch (error) {
      import_core2.elizaLogger.error("Error in ethereum data provider:", error);
      return null;
    }
  }
};

// src/index.ts
var onchainDataPlugin = {
  name: "onchain data plugin",
  description: "Enables onchain data fetching",
  actions: [fetchTransactionAction],
  providers: [ethereumDataProvider],
  evaluators: [],
  // separate examples will be added for services and clients
  // services: [new DataService()],
  services: [],
  clients: []
};
// Annotate the CommonJS export names for ESM import in node:
0 && (module.exports = {
  onchainDataPlugin
});

```

`/Users/tuyukai/go/src/github.com/carv-protocol/d.a.t.a/eliza/plugin-d.a.t.a/dist/index.mjs`:

```mjs
var __create = Object.create;
var __defProp = Object.defineProperty;
var __getOwnPropDesc = Object.getOwnPropertyDescriptor;
var __getOwnPropNames = Object.getOwnPropertyNames;
var __getProtoOf = Object.getPrototypeOf;
var __hasOwnProp = Object.prototype.hasOwnProperty;
var __require = /* @__PURE__ */ ((x) => typeof require !== "undefined" ? require : typeof Proxy !== "undefined" ? new Proxy(x, {
  get: (a, b) => (typeof require !== "undefined" ? require : a)[b]
}) : x)(function(x) {
  if (typeof require !== "undefined") return require.apply(this, arguments);
  throw Error('Dynamic require of "' + x + '" is not supported');
});
var __commonJS = (cb, mod) => function __require2() {
  return mod || (0, cb[__getOwnPropNames(cb)[0]])((mod = { exports: {} }).exports, mod), mod.exports;
};
var __copyProps = (to, from, except, desc) => {
  if (from && typeof from === "object" || typeof from === "function") {
    for (let key of __getOwnPropNames(from))
      if (!__hasOwnProp.call(to, key) && key !== except)
        __defProp(to, key, { get: () => from[key], enumerable: !(desc = __getOwnPropDesc(from, key)) || desc.enumerable });
  }
  return to;
};
var __toESM = (mod, isNodeMode, target) => (target = mod != null ? __create(__getProtoOf(mod)) : {}, __copyProps(
  // If the importer is in node compatibility mode or this is not an ESM
  // file that has been converted to a CommonJS file using a Babel-
  // compatible transform (i.e. "__esModule" has not been set), then set
  // "default" to the CommonJS "module.exports" for node compatibility.
  isNodeMode || !mod || !mod.__esModule ? __defProp(target, "default", { value: mod, enumerable: true }) : target,
  mod
));

// ../../node_modules/universalify/index.js
var require_universalify = __commonJS({
  "../../node_modules/universalify/index.js"(exports) {
    "use strict";
    exports.fromCallback = function(fn) {
      return Object.defineProperty(function(...args) {
        if (typeof args[args.length - 1] === "function") fn.apply(this, args);
        else {
          return new Promise((resolve, reject) => {
            args.push((err, res) => err != null ? reject(err) : resolve(res));
            fn.apply(this, args);
          });
        }
      }, "name", { value: fn.name });
    };
    exports.fromPromise = function(fn) {
      return Object.defineProperty(function(...args) {
        const cb = args[args.length - 1];
        if (typeof cb !== "function") return fn.apply(this, args);
        else {
          args.pop();
          fn.apply(this, args).then((r) => cb(null, r), cb);
        }
      }, "name", { value: fn.name });
    };
  }
});

// ../../node_modules/graceful-fs/polyfills.js
var require_polyfills = __commonJS({
  "../../node_modules/graceful-fs/polyfills.js"(exports, module) {
    var constants = __require("constants");
    var origCwd = process.cwd;
    var cwd = null;
    var platform = process.env.GRACEFUL_FS_PLATFORM || process.platform;
    process.cwd = function() {
      if (!cwd)
        cwd = origCwd.call(process);
      return cwd;
    };
    try {
      process.cwd();
    } catch (er) {
    }
    if (typeof process.chdir === "function") {
      chdir = process.chdir;
      process.chdir = function(d) {
        cwd = null;
        chdir.call(process, d);
      };
      if (Object.setPrototypeOf) Object.setPrototypeOf(process.chdir, chdir);
    }
    var chdir;
    module.exports = patch;
    function patch(fs2) {
      if (constants.hasOwnProperty("O_SYMLINK") && process.version.match(/^v0\.6\.[0-2]|^v0\.5\./)) {
        patchLchmod(fs2);
      }
      if (!fs2.lutimes) {
        patchLutimes(fs2);
      }
      fs2.chown = chownFix(fs2.chown);
      fs2.fchown = chownFix(fs2.fchown);
      fs2.lchown = chownFix(fs2.lchown);
      fs2.chmod = chmodFix(fs2.chmod);
      fs2.fchmod = chmodFix(fs2.fchmod);
      fs2.lchmod = chmodFix(fs2.lchmod);
      fs2.chownSync = chownFixSync(fs2.chownSync);
      fs2.fchownSync = chownFixSync(fs2.fchownSync);
      fs2.lchownSync = chownFixSync(fs2.lchownSync);
      fs2.chmodSync = chmodFixSync(fs2.chmodSync);
      fs2.fchmodSync = chmodFixSync(fs2.fchmodSync);
      fs2.lchmodSync = chmodFixSync(fs2.lchmodSync);
      fs2.stat = statFix(fs2.stat);
      fs2.fstat = statFix(fs2.fstat);
      fs2.lstat = statFix(fs2.lstat);
      fs2.statSync = statFixSync(fs2.statSync);
      fs2.fstatSync = statFixSync(fs2.fstatSync);
      fs2.lstatSync = statFixSync(fs2.lstatSync);
      if (fs2.chmod && !fs2.lchmod) {
        fs2.lchmod = function(path2, mode, cb) {
          if (cb) process.nextTick(cb);
        };
        fs2.lchmodSync = function() {
        };
      }
      if (fs2.chown && !fs2.lchown) {
        fs2.lchown = function(path2, uid, gid, cb) {
          if (cb) process.nextTick(cb);
        };
        fs2.lchownSync = function() {
        };
      }
      if (platform === "win32") {
        fs2.rename = typeof fs2.rename !== "function" ? fs2.rename : function(fs$rename) {
          function rename(from, to, cb) {
            var start = Date.now();
            var backoff = 0;
            fs$rename(from, to, function CB(er) {
              if (er && (er.code === "EACCES" || er.code === "EPERM" || er.code === "EBUSY") && Date.now() - start < 6e4) {
                setTimeout(function() {
                  fs2.stat(to, function(stater, st) {
                    if (stater && stater.code === "ENOENT")
                      fs$rename(from, to, CB);
                    else
                      cb(er);
                  });
                }, backoff);
                if (backoff < 100)
                  backoff += 10;
                return;
              }
              if (cb) cb(er);
            });
          }
          if (Object.setPrototypeOf) Object.setPrototypeOf(rename, fs$rename);
          return rename;
        }(fs2.rename);
      }
      fs2.read = typeof fs2.read !== "function" ? fs2.read : function(fs$read) {
        function read(fd, buffer, offset, length, position, callback_) {
          var callback;
          if (callback_ && typeof callback_ === "function") {
            var eagCounter = 0;
            callback = function(er, _, __) {
              if (er && er.code === "EAGAIN" && eagCounter < 10) {
                eagCounter++;
                return fs$read.call(fs2, fd, buffer, offset, length, position, callback);
              }
              callback_.apply(this, arguments);
            };
          }
          return fs$read.call(fs2, fd, buffer, offset, length, position, callback);
        }
        if (Object.setPrototypeOf) Object.setPrototypeOf(read, fs$read);
        return read;
      }(fs2.read);
      fs2.readSync = typeof fs2.readSync !== "function" ? fs2.readSync : /* @__PURE__ */ function(fs$readSync) {
        return function(fd, buffer, offset, length, position) {
          var eagCounter = 0;
          while (true) {
            try {
              return fs$readSync.call(fs2, fd, buffer, offset, length, position);
            } catch (er) {
              if (er.code === "EAGAIN" && eagCounter < 10) {
                eagCounter++;
                continue;
              }
              throw er;
            }
          }
        };
      }(fs2.readSync);
      function patchLchmod(fs3) {
        fs3.lchmod = function(path2, mode, callback) {
          fs3.open(
            path2,
            constants.O_WRONLY | constants.O_SYMLINK,
            mode,
            function(err, fd) {
              if (err) {
                if (callback) callback(err);
                return;
              }
              fs3.fchmod(fd, mode, function(err2) {
                fs3.close(fd, function(err22) {
                  if (callback) callback(err2 || err22);
                });
              });
            }
          );
        };
        fs3.lchmodSync = function(path2, mode) {
          var fd = fs3.openSync(path2, constants.O_WRONLY | constants.O_SYMLINK, mode);
          var threw = true;
          var ret;
          try {
            ret = fs3.fchmodSync(fd, mode);
            threw = false;
          } finally {
            if (threw) {
              try {
                fs3.closeSync(fd);
              } catch (er) {
              }
            } else {
              fs3.closeSync(fd);
            }
          }
          return ret;
        };
      }
      function patchLutimes(fs3) {
        if (constants.hasOwnProperty("O_SYMLINK") && fs3.futimes) {
          fs3.lutimes = function(path2, at, mt, cb) {
            fs3.open(path2, constants.O_SYMLINK, function(er, fd) {
              if (er) {
                if (cb) cb(er);
                return;
              }
              fs3.futimes(fd, at, mt, function(er2) {
                fs3.close(fd, function(er22) {
                  if (cb) cb(er2 || er22);
                });
              });
            });
          };
          fs3.lutimesSync = function(path2, at, mt) {
            var fd = fs3.openSync(path2, constants.O_SYMLINK);
            var ret;
            var threw = true;
            try {
              ret = fs3.futimesSync(fd, at, mt);
              threw = false;
            } finally {
              if (threw) {
                try {
                  fs3.closeSync(fd);
                } catch (er) {
                }
              } else {
                fs3.closeSync(fd);
              }
            }
            return ret;
          };
        } else if (fs3.futimes) {
          fs3.lutimes = function(_a, _b, _c, cb) {
            if (cb) process.nextTick(cb);
          };
          fs3.lutimesSync = function() {
          };
        }
      }
      function chmodFix(orig) {
        if (!orig) return orig;
        return function(target, mode, cb) {
          return orig.call(fs2, target, mode, function(er) {
            if (chownErOk(er)) er = null;
            if (cb) cb.apply(this, arguments);
          });
        };
      }
      function chmodFixSync(orig) {
        if (!orig) return orig;
        return function(target, mode) {
          try {
            return orig.call(fs2, target, mode);
          } catch (er) {
            if (!chownErOk(er)) throw er;
          }
        };
      }
      function chownFix(orig) {
        if (!orig) return orig;
        return function(target, uid, gid, cb) {
          return orig.call(fs2, target, uid, gid, function(er) {
            if (chownErOk(er)) er = null;
            if (cb) cb.apply(this, arguments);
          });
        };
      }
      function chownFixSync(orig) {
        if (!orig) return orig;
        return function(target, uid, gid) {
          try {
            return orig.call(fs2, target, uid, gid);
          } catch (er) {
            if (!chownErOk(er)) throw er;
          }
        };
      }
      function statFix(orig) {
        if (!orig) return orig;
        return function(target, options, cb) {
          if (typeof options === "function") {
            cb = options;
            options = null;
          }
          function callback(er, stats) {
            if (stats) {
              if (stats.uid < 0) stats.uid += 4294967296;
              if (stats.gid < 0) stats.gid += 4294967296;
            }
            if (cb) cb.apply(this, arguments);
          }
          return options ? orig.call(fs2, target, options, callback) : orig.call(fs2, target, callback);
        };
      }
      function statFixSync(orig) {
        if (!orig) return orig;
        return function(target, options) {
          var stats = options ? orig.call(fs2, target, options) : orig.call(fs2, target);
          if (stats) {
            if (stats.uid < 0) stats.uid += 4294967296;
            if (stats.gid < 0) stats.gid += 4294967296;
          }
          return stats;
        };
      }
      function chownErOk(er) {
        if (!er)
          return true;
        if (er.code === "ENOSYS")
          return true;
        var nonroot = !process.getuid || process.getuid() !== 0;
        if (nonroot) {
          if (er.code === "EINVAL" || er.code === "EPERM")
            return true;
        }
        return false;
      }
    }
  }
});

// ../../node_modules/graceful-fs/legacy-streams.js
var require_legacy_streams = __commonJS({
  "../../node_modules/graceful-fs/legacy-streams.js"(exports, module) {
    var Stream = __require("stream").Stream;
    module.exports = legacy;
    function legacy(fs2) {
      return {
        ReadStream,
        WriteStream
      };
      function ReadStream(path2, options) {
        if (!(this instanceof ReadStream)) return new ReadStream(path2, options);
        Stream.call(this);
        var self = this;
        this.path = path2;
        this.fd = null;
        this.readable = true;
        this.paused = false;
        this.flags = "r";
        this.mode = 438;
        this.bufferSize = 64 * 1024;
        options = options || {};
        var keys = Object.keys(options);
        for (var index = 0, length = keys.length; index < length; index++) {
          var key = keys[index];
          this[key] = options[key];
        }
        if (this.encoding) this.setEncoding(this.encoding);
        if (this.start !== void 0) {
          if ("number" !== typeof this.start) {
            throw TypeError("start must be a Number");
          }
          if (this.end === void 0) {
            this.end = Infinity;
          } else if ("number" !== typeof this.end) {
            throw TypeError("end must be a Number");
          }
          if (this.start > this.end) {
            throw new Error("start must be <= end");
          }
          this.pos = this.start;
        }
        if (this.fd !== null) {
          process.nextTick(function() {
            self._read();
          });
          return;
        }
        fs2.open(this.path, this.flags, this.mode, function(err, fd) {
          if (err) {
            self.emit("error", err);
            self.readable = false;
            return;
          }
          self.fd = fd;
          self.emit("open", fd);
          self._read();
        });
      }
      function WriteStream(path2, options) {
        if (!(this instanceof WriteStream)) return new WriteStream(path2, options);
        Stream.call(this);
        this.path = path2;
        this.fd = null;
        this.writable = true;
        this.flags = "w";
        this.encoding = "binary";
        this.mode = 438;
        this.bytesWritten = 0;
        options = options || {};
        var keys = Object.keys(options);
        for (var index = 0, length = keys.length; index < length; index++) {
          var key = keys[index];
          this[key] = options[key];
        }
        if (this.start !== void 0) {
          if ("number" !== typeof this.start) {
            throw TypeError("start must be a Number");
          }
          if (this.start < 0) {
            throw new Error("start must be >= zero");
          }
          this.pos = this.start;
        }
        this.busy = false;
        this._queue = [];
        if (this.fd === null) {
          this._open = fs2.open;
          this._queue.push([this._open, this.path, this.flags, this.mode, void 0]);
          this.flush();
        }
      }
    }
  }
});

// ../../node_modules/graceful-fs/clone.js
var require_clone = __commonJS({
  "../../node_modules/graceful-fs/clone.js"(exports, module) {
    "use strict";
    module.exports = clone;
    var getPrototypeOf = Object.getPrototypeOf || function(obj) {
      return obj.__proto__;
    };
    function clone(obj) {
      if (obj === null || typeof obj !== "object")
        return obj;
      if (obj instanceof Object)
        var copy = { __proto__: getPrototypeOf(obj) };
      else
        var copy = /* @__PURE__ */ Object.create(null);
      Object.getOwnPropertyNames(obj).forEach(function(key) {
        Object.defineProperty(copy, key, Object.getOwnPropertyDescriptor(obj, key));
      });
      return copy;
    }
  }
});

// ../../node_modules/graceful-fs/graceful-fs.js
var require_graceful_fs = __commonJS({
  "../../node_modules/graceful-fs/graceful-fs.js"(exports, module) {
    var fs2 = __require("fs");
    var polyfills = require_polyfills();
    var legacy = require_legacy_streams();
    var clone = require_clone();
    var util = __require("util");
    var gracefulQueue;
    var previousSymbol;
    if (typeof Symbol === "function" && typeof Symbol.for === "function") {
      gracefulQueue = Symbol.for("graceful-fs.queue");
      previousSymbol = Symbol.for("graceful-fs.previous");
    } else {
      gracefulQueue = "___graceful-fs.queue";
      previousSymbol = "___graceful-fs.previous";
    }
    function noop() {
    }
    function publishQueue(context, queue2) {
      Object.defineProperty(context, gracefulQueue, {
        get: function() {
          return queue2;
        }
      });
    }
    var debug = noop;
    if (util.debuglog)
      debug = util.debuglog("gfs4");
    else if (/\bgfs4\b/i.test(process.env.NODE_DEBUG || ""))
      debug = function() {
        var m = util.format.apply(util, arguments);
        m = "GFS4: " + m.split(/\n/).join("\nGFS4: ");
        console.error(m);
      };
    if (!fs2[gracefulQueue]) {
      queue = global[gracefulQueue] || [];
      publishQueue(fs2, queue);
      fs2.close = function(fs$close) {
        function close(fd, cb) {
          return fs$close.call(fs2, fd, function(err) {
            if (!err) {
              resetQueue();
            }
            if (typeof cb === "function")
              cb.apply(this, arguments);
          });
        }
        Object.defineProperty(close, previousSymbol, {
          value: fs$close
        });
        return close;
      }(fs2.close);
      fs2.closeSync = function(fs$closeSync) {
        function closeSync(fd) {
          fs$closeSync.apply(fs2, arguments);
          resetQueue();
        }
        Object.defineProperty(closeSync, previousSymbol, {
          value: fs$closeSync
        });
        return closeSync;
      }(fs2.closeSync);
      if (/\bgfs4\b/i.test(process.env.NODE_DEBUG || "")) {
        process.on("exit", function() {
          debug(fs2[gracefulQueue]);
          __require("assert").equal(fs2[gracefulQueue].length, 0);
        });
      }
    }
    var queue;
    if (!global[gracefulQueue]) {
      publishQueue(global, fs2[gracefulQueue]);
    }
    module.exports = patch(clone(fs2));
    if (process.env.TEST_GRACEFUL_FS_GLOBAL_PATCH && !fs2.__patched) {
      module.exports = patch(fs2);
      fs2.__patched = true;
    }
    function patch(fs3) {
      polyfills(fs3);
      fs3.gracefulify = patch;
      fs3.createReadStream = createReadStream;
      fs3.createWriteStream = createWriteStream;
      var fs$readFile = fs3.readFile;
      fs3.readFile = readFile2;
      function readFile2(path2, options, cb) {
        if (typeof options === "function")
          cb = options, options = null;
        return go$readFile(path2, options, cb);
        function go$readFile(path3, options2, cb2, startTime) {
          return fs$readFile(path3, options2, function(err) {
            if (err && (err.code === "EMFILE" || err.code === "ENFILE"))
              enqueue([go$readFile, [path3, options2, cb2], err, startTime || Date.now(), Date.now()]);
            else {
              if (typeof cb2 === "function")
                cb2.apply(this, arguments);
            }
          });
        }
      }
      var fs$writeFile = fs3.writeFile;
      fs3.writeFile = writeFile;
      function writeFile(path2, data, options, cb) {
        if (typeof options === "function")
          cb = options, options = null;
        return go$writeFile(path2, data, options, cb);
        function go$writeFile(path3, data2, options2, cb2, startTime) {
          return fs$writeFile(path3, data2, options2, function(err) {
            if (err && (err.code === "EMFILE" || err.code === "ENFILE"))
              enqueue([go$writeFile, [path3, data2, options2, cb2], err, startTime || Date.now(), Date.now()]);
            else {
              if (typeof cb2 === "function")
                cb2.apply(this, arguments);
            }
          });
        }
      }
      var fs$appendFile = fs3.appendFile;
      if (fs$appendFile)
        fs3.appendFile = appendFile;
      function appendFile(path2, data, options, cb) {
        if (typeof options === "function")
          cb = options, options = null;
        return go$appendFile(path2, data, options, cb);
        function go$appendFile(path3, data2, options2, cb2, startTime) {
          return fs$appendFile(path3, data2, options2, function(err) {
            if (err && (err.code === "EMFILE" || err.code === "ENFILE"))
              enqueue([go$appendFile, [path3, data2, options2, cb2], err, startTime || Date.now(), Date.now()]);
            else {
              if (typeof cb2 === "function")
                cb2.apply(this, arguments);
            }
          });
        }
      }
      var fs$copyFile = fs3.copyFile;
      if (fs$copyFile)
        fs3.copyFile = copyFile;
      function copyFile(src, dest, flags, cb) {
        if (typeof flags === "function") {
          cb = flags;
          flags = 0;
        }
        return go$copyFile(src, dest, flags, cb);
        function go$copyFile(src2, dest2, flags2, cb2, startTime) {
          return fs$copyFile(src2, dest2, flags2, function(err) {
            if (err && (err.code === "EMFILE" || err.code === "ENFILE"))
              enqueue([go$copyFile, [src2, dest2, flags2, cb2], err, startTime || Date.now(), Date.now()]);
            else {
              if (typeof cb2 === "function")
                cb2.apply(this, arguments);
            }
          });
        }
      }
      var fs$readdir = fs3.readdir;
      fs3.readdir = readdir;
      var noReaddirOptionVersions = /^v[0-5]\./;
      function readdir(path2, options, cb) {
        if (typeof options === "function")
          cb = options, options = null;
        var go$readdir = noReaddirOptionVersions.test(process.version) ? function go$readdir2(path3, options2, cb2, startTime) {
          return fs$readdir(path3, fs$readdirCallback(
            path3,
            options2,
            cb2,
            startTime
          ));
        } : function go$readdir2(path3, options2, cb2, startTime) {
          return fs$readdir(path3, options2, fs$readdirCallback(
            path3,
            options2,
            cb2,
            startTime
          ));
        };
        return go$readdir(path2, options, cb);
        function fs$readdirCallback(path3, options2, cb2, startTime) {
          return function(err, files) {
            if (err && (err.code === "EMFILE" || err.code === "ENFILE"))
              enqueue([
                go$readdir,
                [path3, options2, cb2],
                err,
                startTime || Date.now(),
                Date.now()
              ]);
            else {
              if (files && files.sort)
                files.sort();
              if (typeof cb2 === "function")
                cb2.call(this, err, files);
            }
          };
        }
      }
      if (process.version.substr(0, 4) === "v0.8") {
        var legStreams = legacy(fs3);
        ReadStream = legStreams.ReadStream;
        WriteStream = legStreams.WriteStream;
      }
      var fs$ReadStream = fs3.ReadStream;
      if (fs$ReadStream) {
        ReadStream.prototype = Object.create(fs$ReadStream.prototype);
        ReadStream.prototype.open = ReadStream$open;
      }
      var fs$WriteStream = fs3.WriteStream;
      if (fs$WriteStream) {
        WriteStream.prototype = Object.create(fs$WriteStream.prototype);
        WriteStream.prototype.open = WriteStream$open;
      }
      Object.defineProperty(fs3, "ReadStream", {
        get: function() {
          return ReadStream;
        },
        set: function(val) {
          ReadStream = val;
        },
        enumerable: true,
        configurable: true
      });
      Object.defineProperty(fs3, "WriteStream", {
        get: function() {
          return WriteStream;
        },
        set: function(val) {
          WriteStream = val;
        },
        enumerable: true,
        configurable: true
      });
      var FileReadStream = ReadStream;
      Object.defineProperty(fs3, "FileReadStream", {
        get: function() {
          return FileReadStream;
        },
        set: function(val) {
          FileReadStream = val;
        },
        enumerable: true,
        configurable: true
      });
      var FileWriteStream = WriteStream;
      Object.defineProperty(fs3, "FileWriteStream", {
        get: function() {
          return FileWriteStream;
        },
        set: function(val) {
          FileWriteStream = val;
        },
        enumerable: true,
        configurable: true
      });
      function ReadStream(path2, options) {
        if (this instanceof ReadStream)
          return fs$ReadStream.apply(this, arguments), this;
        else
          return ReadStream.apply(Object.create(ReadStream.prototype), arguments);
      }
      function ReadStream$open() {
        var that = this;
        open(that.path, that.flags, that.mode, function(err, fd) {
          if (err) {
            if (that.autoClose)
              that.destroy();
            that.emit("error", err);
          } else {
            that.fd = fd;
            that.emit("open", fd);
            that.read();
          }
        });
      }
      function WriteStream(path2, options) {
        if (this instanceof WriteStream)
          return fs$WriteStream.apply(this, arguments), this;
        else
          return WriteStream.apply(Object.create(WriteStream.prototype), arguments);
      }
      function WriteStream$open() {
        var that = this;
        open(that.path, that.flags, that.mode, function(err, fd) {
          if (err) {
            that.destroy();
            that.emit("error", err);
          } else {
            that.fd = fd;
            that.emit("open", fd);
          }
        });
      }
      function createReadStream(path2, options) {
        return new fs3.ReadStream(path2, options);
      }
      function createWriteStream(path2, options) {
        return new fs3.WriteStream(path2, options);
      }
      var fs$open = fs3.open;
      fs3.open = open;
      function open(path2, flags, mode, cb) {
        if (typeof mode === "function")
          cb = mode, mode = null;
        return go$open(path2, flags, mode, cb);
        function go$open(path3, flags2, mode2, cb2, startTime) {
          return fs$open(path3, flags2, mode2, function(err, fd) {
            if (err && (err.code === "EMFILE" || err.code === "ENFILE"))
              enqueue([go$open, [path3, flags2, mode2, cb2], err, startTime || Date.now(), Date.now()]);
            else {
              if (typeof cb2 === "function")
                cb2.apply(this, arguments);
            }
          });
        }
      }
      return fs3;
    }
    function enqueue(elem) {
      debug("ENQUEUE", elem[0].name, elem[1]);
      fs2[gracefulQueue].push(elem);
      retry();
    }
    var retryTimer;
    function resetQueue() {
      var now = Date.now();
      for (var i = 0; i < fs2[gracefulQueue].length; ++i) {
        if (fs2[gracefulQueue][i].length > 2) {
          fs2[gracefulQueue][i][3] = now;
          fs2[gracefulQueue][i][4] = now;
        }
      }
      retry();
    }
    function retry() {
      clearTimeout(retryTimer);
      retryTimer = void 0;
      if (fs2[gracefulQueue].length === 0)
        return;
      var elem = fs2[gracefulQueue].shift();
      var fn = elem[0];
      var args = elem[1];
      var err = elem[2];
      var startTime = elem[3];
      var lastTime = elem[4];
      if (startTime === void 0) {
        debug("RETRY", fn.name, args);
        fn.apply(null, args);
      } else if (Date.now() - startTime >= 6e4) {
        debug("TIMEOUT", fn.name, args);
        var cb = args.pop();
        if (typeof cb === "function")
          cb.call(null, err);
      } else {
        var sinceAttempt = Date.now() - lastTime;
        var sinceStart = Math.max(lastTime - startTime, 1);
        var desiredDelay = Math.min(sinceStart * 1.2, 100);
        if (sinceAttempt >= desiredDelay) {
          debug("RETRY", fn.name, args);
          fn.apply(null, args.concat([startTime]));
        } else {
          fs2[gracefulQueue].push(elem);
        }
      }
      if (retryTimer === void 0) {
        retryTimer = setTimeout(retry, 0);
      }
    }
  }
});

// ../../node_modules/fs-extra/lib/fs/index.js
var require_fs = __commonJS({
  "../../node_modules/fs-extra/lib/fs/index.js"(exports) {
    "use strict";
    var u = require_universalify().fromCallback;
    var fs2 = require_graceful_fs();
    var api = [
      "access",
      "appendFile",
      "chmod",
      "chown",
      "close",
      "copyFile",
      "fchmod",
      "fchown",
      "fdatasync",
      "fstat",
      "fsync",
      "ftruncate",
      "futimes",
      "lchmod",
      "lchown",
      "link",
      "lstat",
      "mkdir",
      "mkdtemp",
      "open",
      "opendir",
      "readdir",
      "readFile",
      "readlink",
      "realpath",
      "rename",
      "rm",
      "rmdir",
      "stat",
      "symlink",
      "truncate",
      "unlink",
      "utimes",
      "writeFile"
    ].filter((key) => {
      return typeof fs2[key] === "function";
    });
    Object.assign(exports, fs2);
    api.forEach((method) => {
      exports[method] = u(fs2[method]);
    });
    exports.exists = function(filename, callback) {
      if (typeof callback === "function") {
        return fs2.exists(filename, callback);
      }
      return new Promise((resolve) => {
        return fs2.exists(filename, resolve);
      });
    };
    exports.read = function(fd, buffer, offset, length, position, callback) {
      if (typeof callback === "function") {
        return fs2.read(fd, buffer, offset, length, position, callback);
      }
      return new Promise((resolve, reject) => {
        fs2.read(fd, buffer, offset, length, position, (err, bytesRead, buffer2) => {
          if (err) return reject(err);
          resolve({ bytesRead, buffer: buffer2 });
        });
      });
    };
    exports.write = function(fd, buffer, ...args) {
      if (typeof args[args.length - 1] === "function") {
        return fs2.write(fd, buffer, ...args);
      }
      return new Promise((resolve, reject) => {
        fs2.write(fd, buffer, ...args, (err, bytesWritten, buffer2) => {
          if (err) return reject(err);
          resolve({ bytesWritten, buffer: buffer2 });
        });
      });
    };
    exports.readv = function(fd, buffers, ...args) {
      if (typeof args[args.length - 1] === "function") {
        return fs2.readv(fd, buffers, ...args);
      }
      return new Promise((resolve, reject) => {
        fs2.readv(fd, buffers, ...args, (err, bytesRead, buffers2) => {
          if (err) return reject(err);
          resolve({ bytesRead, buffers: buffers2 });
        });
      });
    };
    exports.writev = function(fd, buffers, ...args) {
      if (typeof args[args.length - 1] === "function") {
        return fs2.writev(fd, buffers, ...args);
      }
      return new Promise((resolve, reject) => {
        fs2.writev(fd, buffers, ...args, (err, bytesWritten, buffers2) => {
          if (err) return reject(err);
          resolve({ bytesWritten, buffers: buffers2 });
        });
      });
    };
    if (typeof fs2.realpath.native === "function") {
      exports.realpath.native = u(fs2.realpath.native);
    } else {
      process.emitWarning(
        "fs.realpath.native is not a function. Is fs being monkey-patched?",
        "Warning",
        "fs-extra-WARN0003"
      );
    }
  }
});

// ../../node_modules/fs-extra/lib/mkdirs/utils.js
var require_utils = __commonJS({
  "../../node_modules/fs-extra/lib/mkdirs/utils.js"(exports, module) {
    "use strict";
    var path2 = __require("path");
    module.exports.checkPath = function checkPath(pth) {
      if (process.platform === "win32") {
        const pathHasInvalidWinCharacters = /[<>:"|?*]/.test(pth.replace(path2.parse(pth).root, ""));
        if (pathHasInvalidWinCharacters) {
          const error = new Error(`Path contains invalid characters: ${pth}`);
          error.code = "EINVAL";
          throw error;
        }
      }
    };
  }
});

// ../../node_modules/fs-extra/lib/mkdirs/make-dir.js
var require_make_dir = __commonJS({
  "../../node_modules/fs-extra/lib/mkdirs/make-dir.js"(exports, module) {
    "use strict";
    var fs2 = require_fs();
    var { checkPath } = require_utils();
    var getMode = (options) => {
      const defaults = { mode: 511 };
      if (typeof options === "number") return options;
      return { ...defaults, ...options }.mode;
    };
    module.exports.makeDir = async (dir, options) => {
      checkPath(dir);
      return fs2.mkdir(dir, {
        mode: getMode(options),
        recursive: true
      });
    };
    module.exports.makeDirSync = (dir, options) => {
      checkPath(dir);
      return fs2.mkdirSync(dir, {
        mode: getMode(options),
        recursive: true
      });
    };
  }
});

// ../../node_modules/fs-extra/lib/mkdirs/index.js
var require_mkdirs = __commonJS({
  "../../node_modules/fs-extra/lib/mkdirs/index.js"(exports, module) {
    "use strict";
    var u = require_universalify().fromPromise;
    var { makeDir: _makeDir, makeDirSync } = require_make_dir();
    var makeDir = u(_makeDir);
    module.exports = {
      mkdirs: makeDir,
      mkdirsSync: makeDirSync,
      // alias
      mkdirp: makeDir,
      mkdirpSync: makeDirSync,
      ensureDir: makeDir,
      ensureDirSync: makeDirSync
    };
  }
});

// ../../node_modules/fs-extra/lib/path-exists/index.js
var require_path_exists = __commonJS({
  "../../node_modules/fs-extra/lib/path-exists/index.js"(exports, module) {
    "use strict";
    var u = require_universalify().fromPromise;
    var fs2 = require_fs();
    function pathExists(path2) {
      return fs2.access(path2).then(() => true).catch(() => false);
    }
    module.exports = {
      pathExists: u(pathExists),
      pathExistsSync: fs2.existsSync
    };
  }
});

// ../../node_modules/fs-extra/lib/util/utimes.js
var require_utimes = __commonJS({
  "../../node_modules/fs-extra/lib/util/utimes.js"(exports, module) {
    "use strict";
    var fs2 = require_fs();
    var u = require_universalify().fromPromise;
    async function utimesMillis(path2, atime, mtime) {
      const fd = await fs2.open(path2, "r+");
      let closeErr = null;
      try {
        await fs2.futimes(fd, atime, mtime);
      } finally {
        try {
          await fs2.close(fd);
        } catch (e) {
          closeErr = e;
        }
      }
      if (closeErr) {
        throw closeErr;
      }
    }
    function utimesMillisSync(path2, atime, mtime) {
      const fd = fs2.openSync(path2, "r+");
      fs2.futimesSync(fd, atime, mtime);
      return fs2.closeSync(fd);
    }
    module.exports = {
      utimesMillis: u(utimesMillis),
      utimesMillisSync
    };
  }
});

// ../../node_modules/fs-extra/lib/util/stat.js
var require_stat = __commonJS({
  "../../node_modules/fs-extra/lib/util/stat.js"(exports, module) {
    "use strict";
    var fs2 = require_fs();
    var path2 = __require("path");
    var u = require_universalify().fromPromise;
    function getStats(src, dest, opts) {
      const statFunc = opts.dereference ? (file) => fs2.stat(file, { bigint: true }) : (file) => fs2.lstat(file, { bigint: true });
      return Promise.all([
        statFunc(src),
        statFunc(dest).catch((err) => {
          if (err.code === "ENOENT") return null;
          throw err;
        })
      ]).then(([srcStat, destStat]) => ({ srcStat, destStat }));
    }
    function getStatsSync(src, dest, opts) {
      let destStat;
      const statFunc = opts.dereference ? (file) => fs2.statSync(file, { bigint: true }) : (file) => fs2.lstatSync(file, { bigint: true });
      const srcStat = statFunc(src);
      try {
        destStat = statFunc(dest);
      } catch (err) {
        if (err.code === "ENOENT") return { srcStat, destStat: null };
        throw err;
      }
      return { srcStat, destStat };
    }
    async function checkPaths(src, dest, funcName, opts) {
      const { srcStat, destStat } = await getStats(src, dest, opts);
      if (destStat) {
        if (areIdentical(srcStat, destStat)) {
          const srcBaseName = path2.basename(src);
          const destBaseName = path2.basename(dest);
          if (funcName === "move" && srcBaseName !== destBaseName && srcBaseName.toLowerCase() === destBaseName.toLowerCase()) {
            return { srcStat, destStat, isChangingCase: true };
          }
          throw new Error("Source and destination must not be the same.");
        }
        if (srcStat.isDirectory() && !destStat.isDirectory()) {
          throw new Error(`Cannot overwrite non-directory '${dest}' with directory '${src}'.`);
        }
        if (!srcStat.isDirectory() && destStat.isDirectory()) {
          throw new Error(`Cannot overwrite directory '${dest}' with non-directory '${src}'.`);
        }
      }
      if (srcStat.isDirectory() && isSrcSubdir(src, dest)) {
        throw new Error(errMsg(src, dest, funcName));
      }
      return { srcStat, destStat };
    }
    function checkPathsSync(src, dest, funcName, opts) {
      const { srcStat, destStat } = getStatsSync(src, dest, opts);
      if (destStat) {
        if (areIdentical(srcStat, destStat)) {
          const srcBaseName = path2.basename(src);
          const destBaseName = path2.basename(dest);
          if (funcName === "move" && srcBaseName !== destBaseName && srcBaseName.toLowerCase() === destBaseName.toLowerCase()) {
            return { srcStat, destStat, isChangingCase: true };
          }
          throw new Error("Source and destination must not be the same.");
        }
        if (srcStat.isDirectory() && !destStat.isDirectory()) {
          throw new Error(`Cannot overwrite non-directory '${dest}' with directory '${src}'.`);
        }
        if (!srcStat.isDirectory() && destStat.isDirectory()) {
          throw new Error(`Cannot overwrite directory '${dest}' with non-directory '${src}'.`);
        }
      }
      if (srcStat.isDirectory() && isSrcSubdir(src, dest)) {
        throw new Error(errMsg(src, dest, funcName));
      }
      return { srcStat, destStat };
    }
    async function checkParentPaths(src, srcStat, dest, funcName) {
      const srcParent = path2.resolve(path2.dirname(src));
      const destParent = path2.resolve(path2.dirname(dest));
      if (destParent === srcParent || destParent === path2.parse(destParent).root) return;
      let destStat;
      try {
        destStat = await fs2.stat(destParent, { bigint: true });
      } catch (err) {
        if (err.code === "ENOENT") return;
        throw err;
      }
      if (areIdentical(srcStat, destStat)) {
        throw new Error(errMsg(src, dest, funcName));
      }
      return checkParentPaths(src, srcStat, destParent, funcName);
    }
    function checkParentPathsSync(src, srcStat, dest, funcName) {
      const srcParent = path2.resolve(path2.dirname(src));
      const destParent = path2.resolve(path2.dirname(dest));
      if (destParent === srcParent || destParent === path2.parse(destParent).root) return;
      let destStat;
      try {
        destStat = fs2.statSync(destParent, { bigint: true });
      } catch (err) {
        if (err.code === "ENOENT") return;
        throw err;
      }
      if (areIdentical(srcStat, destStat)) {
        throw new Error(errMsg(src, dest, funcName));
      }
      return checkParentPathsSync(src, srcStat, destParent, funcName);
    }
    function areIdentical(srcStat, destStat) {
      return destStat.ino && destStat.dev && destStat.ino === srcStat.ino && destStat.dev === srcStat.dev;
    }
    function isSrcSubdir(src, dest) {
      const srcArr = path2.resolve(src).split(path2.sep).filter((i) => i);
      const destArr = path2.resolve(dest).split(path2.sep).filter((i) => i);
      return srcArr.every((cur, i) => destArr[i] === cur);
    }
    function errMsg(src, dest, funcName) {
      return `Cannot ${funcName} '${src}' to a subdirectory of itself, '${dest}'.`;
    }
    module.exports = {
      // checkPaths
      checkPaths: u(checkPaths),
      checkPathsSync,
      // checkParent
      checkParentPaths: u(checkParentPaths),
      checkParentPathsSync,
      // Misc
      isSrcSubdir,
      areIdentical
    };
  }
});

// ../../node_modules/fs-extra/lib/copy/copy.js
var require_copy = __commonJS({
  "../../node_modules/fs-extra/lib/copy/copy.js"(exports, module) {
    "use strict";
    var fs2 = require_fs();
    var path2 = __require("path");
    var { mkdirs } = require_mkdirs();
    var { pathExists } = require_path_exists();
    var { utimesMillis } = require_utimes();
    var stat = require_stat();
    async function copy(src, dest, opts = {}) {
      if (typeof opts === "function") {
        opts = { filter: opts };
      }
      opts.clobber = "clobber" in opts ? !!opts.clobber : true;
      opts.overwrite = "overwrite" in opts ? !!opts.overwrite : opts.clobber;
      if (opts.preserveTimestamps && process.arch === "ia32") {
        process.emitWarning(
          "Using the preserveTimestamps option in 32-bit node is not recommended;\n\n	see https://github.com/jprichardson/node-fs-extra/issues/269",
          "Warning",
          "fs-extra-WARN0001"
        );
      }
      const { srcStat, destStat } = await stat.checkPaths(src, dest, "copy", opts);
      await stat.checkParentPaths(src, srcStat, dest, "copy");
      const include = await runFilter(src, dest, opts);
      if (!include) return;
      const destParent = path2.dirname(dest);
      const dirExists = await pathExists(destParent);
      if (!dirExists) {
        await mkdirs(destParent);
      }
      await getStatsAndPerformCopy(destStat, src, dest, opts);
    }
    async function runFilter(src, dest, opts) {
      if (!opts.filter) return true;
      return opts.filter(src, dest);
    }
    async function getStatsAndPerformCopy(destStat, src, dest, opts) {
      const statFn = opts.dereference ? fs2.stat : fs2.lstat;
      const srcStat = await statFn(src);
      if (srcStat.isDirectory()) return onDir(srcStat, destStat, src, dest, opts);
      if (srcStat.isFile() || srcStat.isCharacterDevice() || srcStat.isBlockDevice()) return onFile(srcStat, destStat, src, dest, opts);
      if (srcStat.isSymbolicLink()) return onLink(destStat, src, dest, opts);
      if (srcStat.isSocket()) throw new Error(`Cannot copy a socket file: ${src}`);
      if (srcStat.isFIFO()) throw new Error(`Cannot copy a FIFO pipe: ${src}`);
      throw new Error(`Unknown file: ${src}`);
    }
    async function onFile(srcStat, destStat, src, dest, opts) {
      if (!destStat) return copyFile(srcStat, src, dest, opts);
      if (opts.overwrite) {
        await fs2.unlink(dest);
        return copyFile(srcStat, src, dest, opts);
      }
      if (opts.errorOnExist) {
        throw new Error(`'${dest}' already exists`);
      }
    }
    async function copyFile(srcStat, src, dest, opts) {
      await fs2.copyFile(src, dest);
      if (opts.preserveTimestamps) {
        if (fileIsNotWritable(srcStat.mode)) {
          await makeFileWritable(dest, srcStat.mode);
        }
        const updatedSrcStat = await fs2.stat(src);
        await utimesMillis(dest, updatedSrcStat.atime, updatedSrcStat.mtime);
      }
      return fs2.chmod(dest, srcStat.mode);
    }
    function fileIsNotWritable(srcMode) {
      return (srcMode & 128) === 0;
    }
    function makeFileWritable(dest, srcMode) {
      return fs2.chmod(dest, srcMode | 128);
    }
    async function onDir(srcStat, destStat, src, dest, opts) {
      if (!destStat) {
        await fs2.mkdir(dest);
      }
      const items = await fs2.readdir(src);
      await Promise.all(items.map(async (item) => {
        const srcItem = path2.join(src, item);
        const destItem = path2.join(dest, item);
        const include = await runFilter(srcItem, destItem, opts);
        if (!include) return;
        const { destStat: destStat2 } = await stat.checkPaths(srcItem, destItem, "copy", opts);
        return getStatsAndPerformCopy(destStat2, srcItem, destItem, opts);
      }));
      if (!destStat) {
        await fs2.chmod(dest, srcStat.mode);
      }
    }
    async function onLink(destStat, src, dest, opts) {
      let resolvedSrc = await fs2.readlink(src);
      if (opts.dereference) {
        resolvedSrc = path2.resolve(process.cwd(), resolvedSrc);
      }
      if (!destStat) {
        return fs2.symlink(resolvedSrc, dest);
      }
      let resolvedDest = null;
      try {
        resolvedDest = await fs2.readlink(dest);
      } catch (e) {
        if (e.code === "EINVAL" || e.code === "UNKNOWN") return fs2.symlink(resolvedSrc, dest);
        throw e;
      }
      if (opts.dereference) {
        resolvedDest = path2.resolve(process.cwd(), resolvedDest);
      }
      if (stat.isSrcSubdir(resolvedSrc, resolvedDest)) {
        throw new Error(`Cannot copy '${resolvedSrc}' to a subdirectory of itself, '${resolvedDest}'.`);
      }
      if (stat.isSrcSubdir(resolvedDest, resolvedSrc)) {
        throw new Error(`Cannot overwrite '${resolvedDest}' with '${resolvedSrc}'.`);
      }
      await fs2.unlink(dest);
      return fs2.symlink(resolvedSrc, dest);
    }
    module.exports = copy;
  }
});

// ../../node_modules/fs-extra/lib/copy/copy-sync.js
var require_copy_sync = __commonJS({
  "../../node_modules/fs-extra/lib/copy/copy-sync.js"(exports, module) {
    "use strict";
    var fs2 = require_graceful_fs();
    var path2 = __require("path");
    var mkdirsSync = require_mkdirs().mkdirsSync;
    var utimesMillisSync = require_utimes().utimesMillisSync;
    var stat = require_stat();
    function copySync(src, dest, opts) {
      if (typeof opts === "function") {
        opts = { filter: opts };
      }
      opts = opts || {};
      opts.clobber = "clobber" in opts ? !!opts.clobber : true;
      opts.overwrite = "overwrite" in opts ? !!opts.overwrite : opts.clobber;
      if (opts.preserveTimestamps && process.arch === "ia32") {
        process.emitWarning(
          "Using the preserveTimestamps option in 32-bit node is not recommended;\n\n	see https://github.com/jprichardson/node-fs-extra/issues/269",
          "Warning",
          "fs-extra-WARN0002"
        );
      }
      const { srcStat, destStat } = stat.checkPathsSync(src, dest, "copy", opts);
      stat.checkParentPathsSync(src, srcStat, dest, "copy");
      if (opts.filter && !opts.filter(src, dest)) return;
      const destParent = path2.dirname(dest);
      if (!fs2.existsSync(destParent)) mkdirsSync(destParent);
      return getStats(destStat, src, dest, opts);
    }
    function getStats(destStat, src, dest, opts) {
      const statSync = opts.dereference ? fs2.statSync : fs2.lstatSync;
      const srcStat = statSync(src);
      if (srcStat.isDirectory()) return onDir(srcStat, destStat, src, dest, opts);
      else if (srcStat.isFile() || srcStat.isCharacterDevice() || srcStat.isBlockDevice()) return onFile(srcStat, destStat, src, dest, opts);
      else if (srcStat.isSymbolicLink()) return onLink(destStat, src, dest, opts);
      else if (srcStat.isSocket()) throw new Error(`Cannot copy a socket file: ${src}`);
      else if (srcStat.isFIFO()) throw new Error(`Cannot copy a FIFO pipe: ${src}`);
      throw new Error(`Unknown file: ${src}`);
    }
    function onFile(srcStat, destStat, src, dest, opts) {
      if (!destStat) return copyFile(srcStat, src, dest, opts);
      return mayCopyFile(srcStat, src, dest, opts);
    }
    function mayCopyFile(srcStat, src, dest, opts) {
      if (opts.overwrite) {
        fs2.unlinkSync(dest);
        return copyFile(srcStat, src, dest, opts);
      } else if (opts.errorOnExist) {
        throw new Error(`'${dest}' already exists`);
      }
    }
    function copyFile(srcStat, src, dest, opts) {
      fs2.copyFileSync(src, dest);
      if (opts.preserveTimestamps) handleTimestamps(srcStat.mode, src, dest);
      return setDestMode(dest, srcStat.mode);
    }
    function handleTimestamps(srcMode, src, dest) {
      if (fileIsNotWritable(srcMode)) makeFileWritable(dest, srcMode);
      return setDestTimestamps(src, dest);
    }
    function fileIsNotWritable(srcMode) {
      return (srcMode & 128) === 0;
    }
    function makeFileWritable(dest, srcMode) {
      return setDestMode(dest, srcMode | 128);
    }
    function setDestMode(dest, srcMode) {
      return fs2.chmodSync(dest, srcMode);
    }
    function setDestTimestamps(src, dest) {
      const updatedSrcStat = fs2.statSync(src);
      return utimesMillisSync(dest, updatedSrcStat.atime, updatedSrcStat.mtime);
    }
    function onDir(srcStat, destStat, src, dest, opts) {
      if (!destStat) return mkDirAndCopy(srcStat.mode, src, dest, opts);
      return copyDir(src, dest, opts);
    }
    function mkDirAndCopy(srcMode, src, dest, opts) {
      fs2.mkdirSync(dest);
      copyDir(src, dest, opts);
      return setDestMode(dest, srcMode);
    }
    function copyDir(src, dest, opts) {
      fs2.readdirSync(src).forEach((item) => copyDirItem(item, src, dest, opts));
    }
    function copyDirItem(item, src, dest, opts) {
      const srcItem = path2.join(src, item);
      const destItem = path2.join(dest, item);
      if (opts.filter && !opts.filter(srcItem, destItem)) return;
      const { destStat } = stat.checkPathsSync(srcItem, destItem, "copy", opts);
      return getStats(destStat, srcItem, destItem, opts);
    }
    function onLink(destStat, src, dest, opts) {
      let resolvedSrc = fs2.readlinkSync(src);
      if (opts.dereference) {
        resolvedSrc = path2.resolve(process.cwd(), resolvedSrc);
      }
      if (!destStat) {
        return fs2.symlinkSync(resolvedSrc, dest);
      } else {
        let resolvedDest;
        try {
          resolvedDest = fs2.readlinkSync(dest);
        } catch (err) {
          if (err.code === "EINVAL" || err.code === "UNKNOWN") return fs2.symlinkSync(resolvedSrc, dest);
          throw err;
        }
        if (opts.dereference) {
          resolvedDest = path2.resolve(process.cwd(), resolvedDest);
        }
        if (stat.isSrcSubdir(resolvedSrc, resolvedDest)) {
          throw new Error(`Cannot copy '${resolvedSrc}' to a subdirectory of itself, '${resolvedDest}'.`);
        }
        if (stat.isSrcSubdir(resolvedDest, resolvedSrc)) {
          throw new Error(`Cannot overwrite '${resolvedDest}' with '${resolvedSrc}'.`);
        }
        return copyLink(resolvedSrc, dest);
      }
    }
    function copyLink(resolvedSrc, dest) {
      fs2.unlinkSync(dest);
      return fs2.symlinkSync(resolvedSrc, dest);
    }
    module.exports = copySync;
  }
});

// ../../node_modules/fs-extra/lib/copy/index.js
var require_copy2 = __commonJS({
  "../../node_modules/fs-extra/lib/copy/index.js"(exports, module) {
    "use strict";
    var u = require_universalify().fromPromise;
    module.exports = {
      copy: u(require_copy()),
      copySync: require_copy_sync()
    };
  }
});

// ../../node_modules/fs-extra/lib/remove/index.js
var require_remove = __commonJS({
  "../../node_modules/fs-extra/lib/remove/index.js"(exports, module) {
    "use strict";
    var fs2 = require_graceful_fs();
    var u = require_universalify().fromCallback;
    function remove(path2, callback) {
      fs2.rm(path2, { recursive: true, force: true }, callback);
    }
    function removeSync(path2) {
      fs2.rmSync(path2, { recursive: true, force: true });
    }
    module.exports = {
      remove: u(remove),
      removeSync
    };
  }
});

// ../../node_modules/fs-extra/lib/empty/index.js
var require_empty = __commonJS({
  "../../node_modules/fs-extra/lib/empty/index.js"(exports, module) {
    "use strict";
    var u = require_universalify().fromPromise;
    var fs2 = require_fs();
    var path2 = __require("path");
    var mkdir = require_mkdirs();
    var remove = require_remove();
    var emptyDir = u(async function emptyDir2(dir) {
      let items;
      try {
        items = await fs2.readdir(dir);
      } catch {
        return mkdir.mkdirs(dir);
      }
      return Promise.all(items.map((item) => remove.remove(path2.join(dir, item))));
    });
    function emptyDirSync(dir) {
      let items;
      try {
        items = fs2.readdirSync(dir);
      } catch {
        return mkdir.mkdirsSync(dir);
      }
      items.forEach((item) => {
        item = path2.join(dir, item);
        remove.removeSync(item);
      });
    }
    module.exports = {
      emptyDirSync,
      emptydirSync: emptyDirSync,
      emptyDir,
      emptydir: emptyDir
    };
  }
});

// ../../node_modules/fs-extra/lib/ensure/file.js
var require_file = __commonJS({
  "../../node_modules/fs-extra/lib/ensure/file.js"(exports, module) {
    "use strict";
    var u = require_universalify().fromPromise;
    var path2 = __require("path");
    var fs2 = require_fs();
    var mkdir = require_mkdirs();
    async function createFile(file) {
      let stats;
      try {
        stats = await fs2.stat(file);
      } catch {
      }
      if (stats && stats.isFile()) return;
      const dir = path2.dirname(file);
      let dirStats = null;
      try {
        dirStats = await fs2.stat(dir);
      } catch (err) {
        if (err.code === "ENOENT") {
          await mkdir.mkdirs(dir);
          await fs2.writeFile(file, "");
          return;
        } else {
          throw err;
        }
      }
      if (dirStats.isDirectory()) {
        await fs2.writeFile(file, "");
      } else {
        await fs2.readdir(dir);
      }
    }
    function createFileSync(file) {
      let stats;
      try {
        stats = fs2.statSync(file);
      } catch {
      }
      if (stats && stats.isFile()) return;
      const dir = path2.dirname(file);
      try {
        if (!fs2.statSync(dir).isDirectory()) {
          fs2.readdirSync(dir);
        }
      } catch (err) {
        if (err && err.code === "ENOENT") mkdir.mkdirsSync(dir);
        else throw err;
      }
      fs2.writeFileSync(file, "");
    }
    module.exports = {
      createFile: u(createFile),
      createFileSync
    };
  }
});

// ../../node_modules/fs-extra/lib/ensure/link.js
var require_link = __commonJS({
  "../../node_modules/fs-extra/lib/ensure/link.js"(exports, module) {
    "use strict";
    var u = require_universalify().fromPromise;
    var path2 = __require("path");
    var fs2 = require_fs();
    var mkdir = require_mkdirs();
    var { pathExists } = require_path_exists();
    var { areIdentical } = require_stat();
    async function createLink(srcpath, dstpath) {
      let dstStat;
      try {
        dstStat = await fs2.lstat(dstpath);
      } catch {
      }
      let srcStat;
      try {
        srcStat = await fs2.lstat(srcpath);
      } catch (err) {
        err.message = err.message.replace("lstat", "ensureLink");
        throw err;
      }
      if (dstStat && areIdentical(srcStat, dstStat)) return;
      const dir = path2.dirname(dstpath);
      const dirExists = await pathExists(dir);
      if (!dirExists) {
        await mkdir.mkdirs(dir);
      }
      await fs2.link(srcpath, dstpath);
    }
    function createLinkSync(srcpath, dstpath) {
      let dstStat;
      try {
        dstStat = fs2.lstatSync(dstpath);
      } catch {
      }
      try {
        const srcStat = fs2.lstatSync(srcpath);
        if (dstStat && areIdentical(srcStat, dstStat)) return;
      } catch (err) {
        err.message = err.message.replace("lstat", "ensureLink");
        throw err;
      }
      const dir = path2.dirname(dstpath);
      const dirExists = fs2.existsSync(dir);
      if (dirExists) return fs2.linkSync(srcpath, dstpath);
      mkdir.mkdirsSync(dir);
      return fs2.linkSync(srcpath, dstpath);
    }
    module.exports = {
      createLink: u(createLink),
      createLinkSync
    };
  }
});

// ../../node_modules/fs-extra/lib/ensure/symlink-paths.js
var require_symlink_paths = __commonJS({
  "../../node_modules/fs-extra/lib/ensure/symlink-paths.js"(exports, module) {
    "use strict";
    var path2 = __require("path");
    var fs2 = require_fs();
    var { pathExists } = require_path_exists();
    var u = require_universalify().fromPromise;
    async function symlinkPaths(srcpath, dstpath) {
      if (path2.isAbsolute(srcpath)) {
        try {
          await fs2.lstat(srcpath);
        } catch (err) {
          err.message = err.message.replace("lstat", "ensureSymlink");
          throw err;
        }
        return {
          toCwd: srcpath,
          toDst: srcpath
        };
      }
      const dstdir = path2.dirname(dstpath);
      const relativeToDst = path2.join(dstdir, srcpath);
      const exists = await pathExists(relativeToDst);
      if (exists) {
        return {
          toCwd: relativeToDst,
          toDst: srcpath
        };
      }
      try {
        await fs2.lstat(srcpath);
      } catch (err) {
        err.message = err.message.replace("lstat", "ensureSymlink");
        throw err;
      }
      return {
        toCwd: srcpath,
        toDst: path2.relative(dstdir, srcpath)
      };
    }
    function symlinkPathsSync(srcpath, dstpath) {
      if (path2.isAbsolute(srcpath)) {
        const exists2 = fs2.existsSync(srcpath);
        if (!exists2) throw new Error("absolute srcpath does not exist");
        return {
          toCwd: srcpath,
          toDst: srcpath
        };
      }
      const dstdir = path2.dirname(dstpath);
      const relativeToDst = path2.join(dstdir, srcpath);
      const exists = fs2.existsSync(relativeToDst);
      if (exists) {
        return {
          toCwd: relativeToDst,
          toDst: srcpath
        };
      }
      const srcExists = fs2.existsSync(srcpath);
      if (!srcExists) throw new Error("relative srcpath does not exist");
      return {
        toCwd: srcpath,
        toDst: path2.relative(dstdir, srcpath)
      };
    }
    module.exports = {
      symlinkPaths: u(symlinkPaths),
      symlinkPathsSync
    };
  }
});

// ../../node_modules/fs-extra/lib/ensure/symlink-type.js
var require_symlink_type = __commonJS({
  "../../node_modules/fs-extra/lib/ensure/symlink-type.js"(exports, module) {
    "use strict";
    var fs2 = require_fs();
    var u = require_universalify().fromPromise;
    async function symlinkType(srcpath, type) {
      if (type) return type;
      let stats;
      try {
        stats = await fs2.lstat(srcpath);
      } catch {
        return "file";
      }
      return stats && stats.isDirectory() ? "dir" : "file";
    }
    function symlinkTypeSync(srcpath, type) {
      if (type) return type;
      let stats;
      try {
        stats = fs2.lstatSync(srcpath);
      } catch {
        return "file";
      }
      return stats && stats.isDirectory() ? "dir" : "file";
    }
    module.exports = {
      symlinkType: u(symlinkType),
      symlinkTypeSync
    };
  }
});

// ../../node_modules/fs-extra/lib/ensure/symlink.js
var require_symlink = __commonJS({
  "../../node_modules/fs-extra/lib/ensure/symlink.js"(exports, module) {
    "use strict";
    var u = require_universalify().fromPromise;
    var path2 = __require("path");
    var fs2 = require_fs();
    var { mkdirs, mkdirsSync } = require_mkdirs();
    var { symlinkPaths, symlinkPathsSync } = require_symlink_paths();
    var { symlinkType, symlinkTypeSync } = require_symlink_type();
    var { pathExists } = require_path_exists();
    var { areIdentical } = require_stat();
    async function createSymlink(srcpath, dstpath, type) {
      let stats;
      try {
        stats = await fs2.lstat(dstpath);
      } catch {
      }
      if (stats && stats.isSymbolicLink()) {
        const [srcStat, dstStat] = await Promise.all([
          fs2.stat(srcpath),
          fs2.stat(dstpath)
        ]);
        if (areIdentical(srcStat, dstStat)) return;
      }
      const relative = await symlinkPaths(srcpath, dstpath);
      srcpath = relative.toDst;
      const toType = await symlinkType(relative.toCwd, type);
      const dir = path2.dirname(dstpath);
      if (!await pathExists(dir)) {
        await mkdirs(dir);
      }
      return fs2.symlink(srcpath, dstpath, toType);
    }
    function createSymlinkSync(srcpath, dstpath, type) {
      let stats;
      try {
        stats = fs2.lstatSync(dstpath);
      } catch {
      }
      if (stats && stats.isSymbolicLink()) {
        const srcStat = fs2.statSync(srcpath);
        const dstStat = fs2.statSync(dstpath);
        if (areIdentical(srcStat, dstStat)) return;
      }
      const relative = symlinkPathsSync(srcpath, dstpath);
      srcpath = relative.toDst;
      type = symlinkTypeSync(relative.toCwd, type);
      const dir = path2.dirname(dstpath);
      const exists = fs2.existsSync(dir);
      if (exists) return fs2.symlinkSync(srcpath, dstpath, type);
      mkdirsSync(dir);
      return fs2.symlinkSync(srcpath, dstpath, type);
    }
    module.exports = {
      createSymlink: u(createSymlink),
      createSymlinkSync
    };
  }
});

// ../../node_modules/fs-extra/lib/ensure/index.js
var require_ensure = __commonJS({
  "../../node_modules/fs-extra/lib/ensure/index.js"(exports, module) {
    "use strict";
    var { createFile, createFileSync } = require_file();
    var { createLink, createLinkSync } = require_link();
    var { createSymlink, createSymlinkSync } = require_symlink();
    module.exports = {
      // file
      createFile,
      createFileSync,
      ensureFile: createFile,
      ensureFileSync: createFileSync,
      // link
      createLink,
      createLinkSync,
      ensureLink: createLink,
      ensureLinkSync: createLinkSync,
      // symlink
      createSymlink,
      createSymlinkSync,
      ensureSymlink: createSymlink,
      ensureSymlinkSync: createSymlinkSync
    };
  }
});

// ../../node_modules/jsonfile/utils.js
var require_utils2 = __commonJS({
  "../../node_modules/jsonfile/utils.js"(exports, module) {
    function stringify(obj, { EOL = "\n", finalEOL = true, replacer = null, spaces } = {}) {
      const EOF = finalEOL ? EOL : "";
      const str = JSON.stringify(obj, replacer, spaces);
      return str.replace(/\n/g, EOL) + EOF;
    }
    function stripBom(content) {
      if (Buffer.isBuffer(content)) content = content.toString("utf8");
      return content.replace(/^\uFEFF/, "");
    }
    module.exports = { stringify, stripBom };
  }
});

// ../../node_modules/jsonfile/index.js
var require_jsonfile = __commonJS({
  "../../node_modules/jsonfile/index.js"(exports, module) {
    var _fs;
    try {
      _fs = require_graceful_fs();
    } catch (_) {
      _fs = __require("fs");
    }
    var universalify = require_universalify();
    var { stringify, stripBom } = require_utils2();
    async function _readFile(file, options = {}) {
      if (typeof options === "string") {
        options = { encoding: options };
      }
      const fs2 = options.fs || _fs;
      const shouldThrow = "throws" in options ? options.throws : true;
      let data = await universalify.fromCallback(fs2.readFile)(file, options);
      data = stripBom(data);
      let obj;
      try {
        obj = JSON.parse(data, options ? options.reviver : null);
      } catch (err) {
        if (shouldThrow) {
          err.message = `${file}: ${err.message}`;
          throw err;
        } else {
          return null;
        }
      }
      return obj;
    }
    var readFile2 = universalify.fromPromise(_readFile);
    function readFileSync(file, options = {}) {
      if (typeof options === "string") {
        options = { encoding: options };
      }
      const fs2 = options.fs || _fs;
      const shouldThrow = "throws" in options ? options.throws : true;
      try {
        let content = fs2.readFileSync(file, options);
        content = stripBom(content);
        return JSON.parse(content, options.reviver);
      } catch (err) {
        if (shouldThrow) {
          err.message = `${file}: ${err.message}`;
          throw err;
        } else {
          return null;
        }
      }
    }
    async function _writeFile(file, obj, options = {}) {
      const fs2 = options.fs || _fs;
      const str = stringify(obj, options);
      await universalify.fromCallback(fs2.writeFile)(file, str, options);
    }
    var writeFile = universalify.fromPromise(_writeFile);
    function writeFileSync(file, obj, options = {}) {
      const fs2 = options.fs || _fs;
      const str = stringify(obj, options);
      return fs2.writeFileSync(file, str, options);
    }
    var jsonfile = {
      readFile: readFile2,
      readFileSync,
      writeFile,
      writeFileSync
    };
    module.exports = jsonfile;
  }
});

// ../../node_modules/fs-extra/lib/json/jsonfile.js
var require_jsonfile2 = __commonJS({
  "../../node_modules/fs-extra/lib/json/jsonfile.js"(exports, module) {
    "use strict";
    var jsonFile = require_jsonfile();
    module.exports = {
      // jsonfile exports
      readJson: jsonFile.readFile,
      readJsonSync: jsonFile.readFileSync,
      writeJson: jsonFile.writeFile,
      writeJsonSync: jsonFile.writeFileSync
    };
  }
});

// ../../node_modules/fs-extra/lib/output-file/index.js
var require_output_file = __commonJS({
  "../../node_modules/fs-extra/lib/output-file/index.js"(exports, module) {
    "use strict";
    var u = require_universalify().fromPromise;
    var fs2 = require_fs();
    var path2 = __require("path");
    var mkdir = require_mkdirs();
    var pathExists = require_path_exists().pathExists;
    async function outputFile(file, data, encoding = "utf-8") {
      const dir = path2.dirname(file);
      if (!await pathExists(dir)) {
        await mkdir.mkdirs(dir);
      }
      return fs2.writeFile(file, data, encoding);
    }
    function outputFileSync(file, ...args) {
      const dir = path2.dirname(file);
      if (!fs2.existsSync(dir)) {
        mkdir.mkdirsSync(dir);
      }
      fs2.writeFileSync(file, ...args);
    }
    module.exports = {
      outputFile: u(outputFile),
      outputFileSync
    };
  }
});

// ../../node_modules/fs-extra/lib/json/output-json.js
var require_output_json = __commonJS({
  "../../node_modules/fs-extra/lib/json/output-json.js"(exports, module) {
    "use strict";
    var { stringify } = require_utils2();
    var { outputFile } = require_output_file();
    async function outputJson(file, data, options = {}) {
      const str = stringify(data, options);
      await outputFile(file, str, options);
    }
    module.exports = outputJson;
  }
});

// ../../node_modules/fs-extra/lib/json/output-json-sync.js
var require_output_json_sync = __commonJS({
  "../../node_modules/fs-extra/lib/json/output-json-sync.js"(exports, module) {
    "use strict";
    var { stringify } = require_utils2();
    var { outputFileSync } = require_output_file();
    function outputJsonSync(file, data, options) {
      const str = stringify(data, options);
      outputFileSync(file, str, options);
    }
    module.exports = outputJsonSync;
  }
});

// ../../node_modules/fs-extra/lib/json/index.js
var require_json = __commonJS({
  "../../node_modules/fs-extra/lib/json/index.js"(exports, module) {
    "use strict";
    var u = require_universalify().fromPromise;
    var jsonFile = require_jsonfile2();
    jsonFile.outputJson = u(require_output_json());
    jsonFile.outputJsonSync = require_output_json_sync();
    jsonFile.outputJSON = jsonFile.outputJson;
    jsonFile.outputJSONSync = jsonFile.outputJsonSync;
    jsonFile.writeJSON = jsonFile.writeJson;
    jsonFile.writeJSONSync = jsonFile.writeJsonSync;
    jsonFile.readJSON = jsonFile.readJson;
    jsonFile.readJSONSync = jsonFile.readJsonSync;
    module.exports = jsonFile;
  }
});

// ../../node_modules/fs-extra/lib/move/move.js
var require_move = __commonJS({
  "../../node_modules/fs-extra/lib/move/move.js"(exports, module) {
    "use strict";
    var fs2 = require_fs();
    var path2 = __require("path");
    var { copy } = require_copy2();
    var { remove } = require_remove();
    var { mkdirp } = require_mkdirs();
    var { pathExists } = require_path_exists();
    var stat = require_stat();
    async function move(src, dest, opts = {}) {
      const overwrite = opts.overwrite || opts.clobber || false;
      const { srcStat, isChangingCase = false } = await stat.checkPaths(src, dest, "move", opts);
      await stat.checkParentPaths(src, srcStat, dest, "move");
      const destParent = path2.dirname(dest);
      const parsedParentPath = path2.parse(destParent);
      if (parsedParentPath.root !== destParent) {
        await mkdirp(destParent);
      }
      return doRename(src, dest, overwrite, isChangingCase);
    }
    async function doRename(src, dest, overwrite, isChangingCase) {
      if (!isChangingCase) {
        if (overwrite) {
          await remove(dest);
        } else if (await pathExists(dest)) {
          throw new Error("dest already exists.");
        }
      }
      try {
        await fs2.rename(src, dest);
      } catch (err) {
        if (err.code !== "EXDEV") {
          throw err;
        }
        await moveAcrossDevice(src, dest, overwrite);
      }
    }
    async function moveAcrossDevice(src, dest, overwrite) {
      const opts = {
        overwrite,
        errorOnExist: true,
        preserveTimestamps: true
      };
      await copy(src, dest, opts);
      return remove(src);
    }
    module.exports = move;
  }
});

// ../../node_modules/fs-extra/lib/move/move-sync.js
var require_move_sync = __commonJS({
  "../../node_modules/fs-extra/lib/move/move-sync.js"(exports, module) {
    "use strict";
    var fs2 = require_graceful_fs();
    var path2 = __require("path");
    var copySync = require_copy2().copySync;
    var removeSync = require_remove().removeSync;
    var mkdirpSync = require_mkdirs().mkdirpSync;
    var stat = require_stat();
    function moveSync(src, dest, opts) {
      opts = opts || {};
      const overwrite = opts.overwrite || opts.clobber || false;
      const { srcStat, isChangingCase = false } = stat.checkPathsSync(src, dest, "move", opts);
      stat.checkParentPathsSync(src, srcStat, dest, "move");
      if (!isParentRoot(dest)) mkdirpSync(path2.dirname(dest));
      return doRename(src, dest, overwrite, isChangingCase);
    }
    function isParentRoot(dest) {
      const parent = path2.dirname(dest);
      const parsedPath = path2.parse(parent);
      return parsedPath.root === parent;
    }
    function doRename(src, dest, overwrite, isChangingCase) {
      if (isChangingCase) return rename(src, dest, overwrite);
      if (overwrite) {
        removeSync(dest);
        return rename(src, dest, overwrite);
      }
      if (fs2.existsSync(dest)) throw new Error("dest already exists.");
      return rename(src, dest, overwrite);
    }
    function rename(src, dest, overwrite) {
      try {
        fs2.renameSync(src, dest);
      } catch (err) {
        if (err.code !== "EXDEV") throw err;
        return moveAcrossDevice(src, dest, overwrite);
      }
    }
    function moveAcrossDevice(src, dest, overwrite) {
      const opts = {
        overwrite,
        errorOnExist: true,
        preserveTimestamps: true
      };
      copySync(src, dest, opts);
      return removeSync(src);
    }
    module.exports = moveSync;
  }
});

// ../../node_modules/fs-extra/lib/move/index.js
var require_move2 = __commonJS({
  "../../node_modules/fs-extra/lib/move/index.js"(exports, module) {
    "use strict";
    var u = require_universalify().fromPromise;
    module.exports = {
      move: u(require_move()),
      moveSync: require_move_sync()
    };
  }
});

// ../../node_modules/fs-extra/lib/index.js
var require_lib = __commonJS({
  "../../node_modules/fs-extra/lib/index.js"(exports, module) {
    "use strict";
    module.exports = {
      // Export promiseified graceful-fs:
      ...require_fs(),
      // Export extra methods:
      ...require_copy2(),
      ...require_empty(),
      ...require_ensure(),
      ...require_json(),
      ...require_mkdirs(),
      ...require_move2(),
      ...require_output_file(),
      ...require_path_exists(),
      ...require_remove()
    };
  }
});

// src/providers/ethereum/txs.ts
import {
  elizaLogger,
  embed
} from "@elizaos/core";
var relativeTxsCount = 100;
var EthTxsProvider = class {
  constructor(blockchainDataTableName) {
    this.blockchainDataTableName = blockchainDataTableName;
  }
  async get(runtime, message, state) {
    const blockchainDataRuntimeManager = runtime.getMemoryManager(this.blockchainDataTableName);
    elizaLogger.log("Retrieving onchain eth txs from blockchain data runtime manager...");
    const embedding = await embed(runtime, message.content.text);
    const memories = await blockchainDataRuntimeManager.searchMemoriesByEmbedding(
      embedding,
      {
        roomId: message.agentId,
        count: relativeTxsCount,
        match_threshold: 0.1
      }
    );
    return concatenateMemories(memories);
  }
};
function concatenateMemories(memories) {
  const prefix = "A list of relevant on-chain ethereum txs content: ";
  const concatenatedContent = memories.map((memory) => memory.content.text).join(" ");
  return prefix + concatenatedContent;
}

// src/evaluators/data_evaluator.ts
import {
  elizaLogger as elizaLogger2
} from "@elizaos/core";
var dataEvaluator = {
  alwaysRun: false,
  name: "GET_ONCHAIN_DATA",
  similes: ["GET_ONCHAIN_INFO"],
  description: "Evaluates for onchain data",
  validate: async (runtime, message) => true,
  handler: async (runtime, message) => {
    elizaLogger2.log("GET_ONCHAIN_DATA evaluator handler called");
    return true;
  },
  examples: []
};

// src/data_service.ts
var fs = __toESM(require_lib());
import { Service, MemoryManager } from "@elizaos/core";
import { stringToUuid } from "@elizaos/core";
import * as path from "path";
var BLOCKCHAIN_DATA_TABLE_NAME = "d.a.t.a-blockchain-data";
var DataService = class extends Service {
  static serviceType = ServiceType.INTIFACE;
  runtime = null;
  blockchainDataManager = null;
  async initialize(runtime) {
    console.log("Initializing carv data service");
    this.runtime = runtime;
    this.blockchainDataManager = new MemoryManager({
      runtime,
      tableName: BLOCKCHAIN_DATA_TABLE_NAME
    });
    this.runtime.registerMemoryManager(this.blockchainDataManager);
    console.log("Reading transactions from file...", path.join(__dirname, "txs.json"));
    const txs = await readJsonFile(path.join(__dirname, "txs.json"));
    for (const tx of txs) {
      console.log("Adding transaction to memory:", tx);
      const txInfo = formatTransaction(tx);
      const memory = {
        id: stringToUuid(txInfo),
        content: {
          text: txInfo
        },
        roomId: runtime.agentId,
        agentId: runtime.agentId,
        userId: runtime.agentId,
        createdAt: Date.now()
      };
      await this.blockchainDataManager.addEmbeddingToMemory(memory);
      console.log("got memory:!!!!!");
      await this.blockchainDataManager.createMemory(memory);
    }
  }
  getBlockchainDataTableName() {
    return BLOCKCHAIN_DATA_TABLE_NAME;
  }
};
async function readJsonFile(filePath) {
  try {
    const fileContent = await fs.readFile(filePath, "utf-8");
    const jsonArray = JSON.parse(fileContent);
    if (!Array.isArray(jsonArray)) {
      throw new Error("The file content is not a list of JSON objects.");
    }
    return jsonArray;
  } catch (error) {
    console.error("Error reading or parsing the file:", error);
    throw error;
  }
}
function formatTransaction(transaction) {
  return `Transaction of type ${transaction.type} from ${transaction.from} to ${transaction.to} with value ${transaction.value}. Transaction hash: ${transaction.txHash}, nonce: ${transaction.nonce}, block hash: ${transaction.blockHash}, block number: ${transaction.blockNum}.`;
}

// src/index.ts
var onchainDataPlugin = {
  name: "onchain data plugin",
  description: "Enables onchain data fetching",
  actions: [],
  providers: [new EthTxsProvider(BLOCKCHAIN_DATA_TABLE_NAME)],
  evaluators: [dataEvaluator],
  // separate examples will be added for services and clients
  services: [new DataService()],
  clients: []
};

```

`/Users/tuyukai/go/src/github.com/carv-protocol/d.a.t.a/eliza/plugin-d.a.t.a/dist/index.d.ts`:

```ts
import { Plugin } from '@elizaos/core';

declare const onchainDataPlugin: Plugin;

export { onchainDataPlugin };

```

`/Users/tuyukai/go/src/github.com/carv-protocol/d.a.t.a/eliza/plugin-d.a.t.a/package.json`:

```json
{
    "name": "@elizaos/plugin-d.a.t.a",
    "version": "0.0.1",
    "types": "dist/index.d.ts",
    "description": "D.A.T.A framework, provide on-chain/off-chain knowledge to AGI",
    "main": "dist/index.js",
    "scripts": {
        "build": "tsup src/index.ts --format cjs --dts",
        "dev": "tsup src/index.ts --format cjs --dts --watch",
        "lint": "eslint --fix  --cache .",
        "test": "vitest run",
        "test:watch": "vitest",
        "test:coverage": "vitest run --coverage"
    },
    "dependencies": {
        "@elizaos/core": "workspace:*"
    },
    "repository": {
        "type": "git",
        "url": "git+https://github.com/carv-protocol/eliza-d.a.t.a.git"
    },
    "keywords": [
        "data",
        "AI",
        "Agent",
        "AGI",
        "plugin",
        "CARV"
    ],
    "author": "CARV",
    "license": "ISC",
    "bugs": {
        "url": "https://github.com/carv-protocol/eliza-d.a.t.a/issues"
    },
    "homepage": "https://github.com/carv-protocol/eliza-d.a.t.a#readme"
}

```

`/Users/tuyukai/go/src/github.com/carv-protocol/d.a.t.a/eliza/plugin-d.a.t.a/tsconfig.json`:

```json
{
    "extends": "../core/tsconfig.json",
    "compilerOptions": {
        "outDir": "dist",
        "rootDir": "src",
        "types": [
            "node"
        ]
    },
    "include": [
        "src"
    ]
}
```

`/Users/tuyukai/go/src/github.com/carv-protocol/d.a.t.a/eliza/plugin-d.a.t.a/vitest.config.ts`:

```ts
import { defineConfig } from "vitest/config";
import path from "path";

export default defineConfig({
    test: {
        environment: "node",
        testTimeout: 120000,
        coverage: {
            reporter: ["text", "json", "html"],
            exclude: ["node_modules/", "dist/"],
        },
    },
    resolve: {
        alias: {
            "@": path.resolve(__dirname, "./src"),
        },
    },
});

```

`/Users/tuyukai/go/src/github.com/carv-protocol/d.a.t.a/eliza/plugin-d.a.t.a/src/providers/ethereum/sequelize.ts`:

```ts
import {
    Provider,
    IAgentRuntime,
    Memory,
    State,
    elizaLogger,
} from "@elizaos/core";

export class TxProvider {
    chain: string;
    constructor(chain: string) {
        this.chain = chain;
    }

    getSequelizeUrl(): string {
        return `https://${this.chain}.sequelize.com`;
    }

    getDataSchema(): string {
        return `
        Ethereum Transaction Data Schema:
        - hash: string (unique transaction hash)
        - blockNumber: number (block containing the transaction)
        - timestamp: datetime (transaction timestamp)
        - from: string (sender address)
        - to: string (recipient address)
        - value: string (transaction amount in wei)
        - gasUsed: string (gas consumed)
        - status: boolean (transaction success)

        Common Query Patterns:
        1. Get transactions by address:
           - Filter by from/to address
           - Time range optional
           - Pagination support

        2. Get transactions by block:
           - Filter by block number/range
           - Support for latest blocks

        3. Get transaction details:
           - Lookup by transaction hash
           - Returns full transaction data

        Query Parameters:
        - address: Ethereum address (0x...)
        - startBlock: number
        - endBlock: number
        - startTime: ISO datetime
        - endTime: ISO datetime
        - limit: number (default 100)
        - offset: number (default 0)

        Response Format:
        {
          transactions: [{
            hash: string,
            blockNumber: number,
            timestamp: string,
            from: string,
            to: string,
            value: string,
            gasUsed: string,
            status: boolean
          }],
          pagination: {
            total: number,
            limit: number,
            offset: number
          }
        }

        Error Handling:
        - Invalid address format
        - Block number out of range
        - Invalid time range
        - Rate limiting considerations
        `;
    }
}

export const txsSequelizeProvider = (runtime: IAgentRuntime) => {
    const chain = "ethereum-mainnet";
    return new TxProvider(chain);
};

export const sequelizeProvider: Provider = {
    get: async (
        runtime: IAgentRuntime,
        message: Memory,
        state: State
    ): Promise<string | null> => {
        elizaLogger.log("==== pis Retrieving from d.a.t.a provider...");
        try {
            const txsProvider = txsSequelizeProvider(runtime);
            const url = txsProvider.getSequelizeUrl();
            const schema = txsProvider.getDataSchema();

            elizaLogger.log("==== pis url: ", url);

            // Return context information for AI to understand the data structure and query patterns
            return `
            Ethereum Mainnet Transaction Database Context:

            Database URL: ${url}

            ${schema}

            Usage Notes:
            1. Always validate input parameters before querying
            2. Consider using pagination for large result sets
            3. Include error handling for failed queries
            4. Cache frequently accessed data when possible
            5. Monitor rate limits and query performance

            This database provides access to all Ethereum mainnet transactions with standardized query patterns and response formats.
            `;
        } catch (error) {
            elizaLogger.error("Error in d.a.t.a provider:", error);
            return null;
        }
    },
};

```

`/Users/tuyukai/go/src/github.com/carv-protocol/d.a.t.a/eliza/plugin-d.a.t.a/src/providers/ethereum/block.ts`:

```ts
import {
  Provider,
  IAgentRuntime,
  Memory,
  State,
  elizaLogger,
  embed,
} from "@elizaos/core";

const relativeTxsCount = 100;

export class EthTxsProvider implements Provider {
constructor(private blockchainDataTableName: string) {}
async get(runtime: IAgentRuntime, message: Memory, state: State): Promise<string | null> {
      // Data retrieval logic for the provider
      const blockchainDataRuntimeManager = runtime.getMemoryManager(this.blockchainDataTableName);
      elizaLogger.log("Retrieving onchain eth txs from blockchain data runtime manager...");

      const embedding = await embed(runtime, message.content.text);
      const memories = await blockchainDataRuntimeManager.searchMemoriesByEmbedding(
        embedding,
        {
            roomId: message.agentId,
            count: relativeTxsCount,
            match_threshold: 0.1,
        }
      );
      return concatenateMemories(memories);
  }
};

function concatenateMemories(memories: Memory[]): string {
  const prefix = "A list of relevant on-chain ethereum txs content: ";
  const concatenatedContent = memories.map(memory => memory.content.text).join(' ');
  return prefix + concatenatedContent;
}
```

`/Users/tuyukai/go/src/github.com/carv-protocol/d.a.t.a/eliza/plugin-d.a.t.a/src/providers/ethereum/txs.ts`:

```ts
import {
  Provider,
  IAgentRuntime,
  Memory,
  State,
  elizaLogger,
  embed,
} from "@elizaos/core";

const relativeTxsCount = 100;

export class EthTxsProvider implements Provider {
constructor(private blockchainDataTableName: string) {}
async get(runtime: IAgentRuntime, message: Memory, state: State): Promise<string | null> {
      // Data retrieval logic for the provider
      const blockchainDataRuntimeManager = runtime.getMemoryManager(this.blockchainDataTableName);
      elizaLogger.log("Retrieving onchain eth txs from blockchain data runtime manager...");

      const embedding = await embed(runtime, message.content.text);
      const memories = await blockchainDataRuntimeManager.searchMemoriesByEmbedding(
        embedding,
        {
            roomId: message.agentId,
            count: relativeTxsCount,
            match_threshold: 0.1,
        }
      );
      return concatenateMemories(memories);
  }
};

function concatenateMemories(memories: Memory[]): string {
  const prefix = "A list of relevant on-chain ethereum txs content: ";
  const concatenatedContent = memories.map(memory => memory.content.text).join(' ');
  return prefix + concatenatedContent;
}
```

`/Users/tuyukai/go/src/github.com/carv-protocol/d.a.t.a/eliza/plugin-d.a.t.a/src/providers/ethereum/database.ts`:

```ts
import {
    Provider,
    IAgentRuntime,
    Memory,
    State,
    elizaLogger,
    generateMessageResponse,
    ModelClass,
    stringToUuid,
    getEmbeddingZeroVector,
} from "@elizaos/core";

// API response interface for query results
interface IQueryResult {
    success: boolean;
    data: any[];
    metadata: {
        total: number;
        queryTime: string;
        queryType: "transaction" | "token" | "aggregate" | "unknown";
        executionTime: number;
        cached: boolean;
        pagination?: {
            currentPage: number;
            totalPages: number;
            hasMore: boolean;
        };
    };
    error?: {
        code: string;
        message: string;
        details?: any;
    };
}
// API response interface
interface IApiResponse {
    code: number;
    msg: string;
    data: {
        column_infos: string[];
        rows: {
            items: (string | number)[];
        }[];
    };
}

export class DatabaseProvider {
    private chain: string;
    private readonly API_URL: string;
    private readonly AUTH_TOKEN: string;

    constructor(chain: string, runtime: IAgentRuntime) {
        this.chain = chain;
        this.API_URL = runtime.getSetting("DATA_API_KEY");
        this.AUTH_TOKEN = runtime.getSetting("DATA_AUTH_TOKEN");
    }

    public extractSQLQuery(preResponse: any): string | null {
        try {
            // Try to parse if input is string
            let jsonData = preResponse;
            if (typeof preResponse === "string") {
                try {
                    jsonData = JSON.parse(preResponse);
                } catch (e) {
                    elizaLogger.error(
                        "Failed to parse preResponse as JSON:",
                        e
                    );
                    return null;
                }
            }

            // Function to recursively search for SQL query in object
            const findSQLQuery = (obj: any): string | null => {
                // Base cases
                if (!obj) return null;

                // If string, check if it's a SQL query
                if (typeof obj === "string") {
                    const sqlPattern = /^\s*(SELECT|WITH)\s+[\s\S]+?(?:;|$)/i;
                    const commentPattern = /--.*$|\/\*[\s\S]*?\*\//gm;

                    // Clean and check the string
                    const cleanStr = obj.replace(commentPattern, "").trim();
                    if (sqlPattern.test(cleanStr)) {
                        // Validate SQL safety
                        const unsafeKeywords = [
                            "drop",
                            "delete",
                            "update",
                            "insert",
                            "alter",
                            "create",
                        ];
                        const isUnsafe = unsafeKeywords.some((keyword) =>
                            cleanStr.toLowerCase().includes(keyword)
                        );

                        if (!isUnsafe) {
                            return cleanStr;
                        }
                    }
                    return null;
                }

                // If array, search each element
                if (Array.isArray(obj)) {
                    for (const item of obj) {
                        const result = findSQLQuery(item);
                        if (result) return result;
                    }
                    return null;
                }

                // If object, search each value
                if (typeof obj === "object") {
                    for (const key of Object.keys(obj)) {
                        // Prioritize 'query' field in sql object
                        if (key.toLowerCase() === "query" && obj.sql) {
                            const result = findSQLQuery(obj[key]);
                            if (result) return result;
                        }
                    }

                    // Search other fields
                    for (const key of Object.keys(obj)) {
                        const result = findSQLQuery(obj[key]);
                        if (result) return result;
                    }
                }

                return null;
            };

            // Start the search
            const sqlQuery = findSQLQuery(jsonData);

            if (!sqlQuery) {
                elizaLogger.warn("No valid SQL query found in preResponse");
                return null;
            }
            return sqlQuery;
        } catch (error) {
            elizaLogger.error("Error in extractSQLQuery:", error);
            return null;
        }
    }

    private async sendSqlQuery(sql: string): Promise<IApiResponse> {
        try {
            const response = await fetch(this.API_URL, {
                method: "POST",
                headers: {
                    "Content-Type": "application/json",
                    Authorization: this.AUTH_TOKEN,
                },
                body: JSON.stringify({
                    sql_content: sql,
                }),
            });

            if (!response.ok) {
                throw new Error(`HTTP error! status: ${response.status}`);
            }

            const data = await response.json();
            return data as IApiResponse;
        } catch (error) {
            elizaLogger.error("Error sending SQL query to API:", error);
            throw error;
        }
    }

    // Transform API response data
    private transformApiResponse(apiResponse: IApiResponse): any[] {
        const { column_infos, rows } = apiResponse.data;

        return rows.map((row) => {
            const rowData: Record<string, any> = {};
            row.items.forEach((value, index) => {
                const columnName = column_infos[index];
                rowData[columnName] = value;
            });
            return rowData;
        });
    }

    // Execute query
    private async executeQuery(sql: string): Promise<IQueryResult> {
        try {
            // Validate query
            if (!sql || sql.length > 5000) {
                throw new Error("Invalid SQL query length");
            }

            const queryType = sql.toLowerCase().includes("token_transfers")
                ? "token"
                : sql.toLowerCase().includes("count")
                    ? "aggregate"
                    : "transaction";

            // Send query to API
            const apiResponse = await this.sendSqlQuery(sql);

            // Check API response status
            if (apiResponse.code !== 0) {
                throw new Error(`API Error: ${apiResponse.msg}`);
            }

            // Transform data
            const transformedData = this.transformApiResponse(apiResponse);

            const queryResult: IQueryResult = {
                success: true,
                data: transformedData,
                metadata: {
                    total: transformedData.length,
                    queryTime: new Date().toISOString(),
                    queryType: queryType as
                        | "token"
                        | "aggregate"
                        | "transaction",
                    executionTime: 0,
                    cached: false,
                },
            };

            return queryResult;
        } catch (error) {
            elizaLogger.error("Query execution failed:", error);
            return {
                success: false,
                data: [],
                metadata: {
                    total: 0,
                    queryTime: new Date().toISOString(),
                    queryType: "unknown",
                    executionTime: 0,
                    cached: false,
                },
                error: {
                    code: error.code || "EXECUTION_ERROR",
                    message: error.message || "Unknown error occurred",
                    details: error,
                },
            };
        }
    }

    public async query(sql: string): Promise<IQueryResult> {
        return this.executeQuery(sql);
    }

    getDatabaseSchema(): string {
        return `
        CREATE EXTERNAL TABLE transactions(
            hash string,
            nonce bigint,
            block_hash string,
            block_number bigint,
            block_timestamp timestamp,
            date string,
            transaction_index bigint,
            from_address string,
            to_address string,
            value double,
            gas bigint,
            gas_price bigint,
            input string,
            max_fee_per_gas bigint,
            max_priority_fee_per_gas bigint,
            transaction_type bigint
        ) PARTITIONED BY (date string)
        ROW FORMAT SERDE 'org.apache.hadoop.hive.ql.io.parquet.serde.ParquetHiveSerDe'
        STORED AS INPUTFORMAT 'org.apache.hadoop.hive.ql.io.parquet.MapredParquetInputFormat'
        OUTPUTFORMAT 'org.apache.hadoop.hive.ql.io.parquet.MapredParquetOutputFormat';

        CREATE EXTERNAL TABLE token_transfers(
            token_address string,
            from_address string,
            to_address string,
            value double,
            transaction_hash string,
            log_index bigint,
            block_timestamp timestamp,
            date string,
            block_number bigint,
            block_hash string
        ) PARTITIONED BY (date string)
        ROW FORMAT SERDE 'org.apache.hadoop.hive.ql.io.parquet.serde.ParquetHiveSerDe'
        STORED AS INPUTFORMAT 'org.apache.hadoop.hive.ql.io.parquet.MapredParquetInputFormat'
        OUTPUTFORMAT 'org.apache.hadoop.hive.ql.io.parquet.MapredParquetOutputFormat';
        `;
    }

    getQueryExamples(): string {
        return `
        Common Query Examples:

        1. Find Most Active Addresses in Last 7 Days:
        WITH address_activity AS (
            SELECT
                from_address AS address,
                COUNT(*) AS tx_count
            FROM
                eth.transactions
            WHERE date_parse(date, '%Y-%m-%d') >= date_add('day', -7, current_date)
            GROUP BY
                from_address
            UNION ALL
            SELECT
                to_address AS address,
                COUNT(*) AS tx_count
            FROM
                eth.transactions
            WHERE
                date_parse(date, '%Y-%m-%d') >= date_add('day', -7, current_date)
            GROUP BY
                to_address
        )
        SELECT
            address,
            SUM(tx_count) AS total_transactions
        FROM
            address_activity
        GROUP BY
            address
        ORDER BY
            total_transactions DESC
        LIMIT 10;

        2. Analyze Address Transaction Statistics (Last 30 Days):
        WITH recent_transactions AS (
            SELECT
                from_address,
                to_address,
                value,
                block_timestamp,
                CASE
                    WHEN from_address = :address THEN 'outgoing'
                    WHEN to_address = :address THEN 'incoming'
                    ELSE 'other'
                END AS transaction_type
            FROM eth.transactions
            WHERE date >= date_format(date_add('day', -30, current_date), '%Y-%m-%d')
                AND (from_address = :address OR to_address = :address)
        )
        SELECT
            transaction_type,
            COUNT(*) AS transaction_count,
            SUM(CASE WHEN transaction_type = 'outgoing' THEN value ELSE 0 END) AS total_outgoing_value,
            SUM(CASE WHEN transaction_type = 'incoming' THEN value ELSE 0 END) AS total_incoming_value
        FROM recent_transactions
        GROUP BY transaction_type;

        3. Token Transfer Analysis:
        WITH filtered_transactions AS (
            SELECT
                token_address,
                from_address,
                to_address,
                value,
                block_timestamp
            FROM eth.token_transfers
            WHERE token_address = :token_address
                AND date >= :start_date
        )
        SELECT
            COUNT(*) AS transaction_count,
            SUM(value) AS total_transaction_value,
            MAX(value) AS max_transaction_value,
            MIN(value) AS min_transaction_value,
            MAX_BY(from_address, value) AS max_value_from_address,
            MAX_BY(to_address, value) AS max_value_to_address,
            MIN_BY(from_address, value) AS min_value_from_address,
            MIN_BY(to_address, value) AS min_value_to_address
        FROM filtered_transactions;

        Note: Replace :address, :token_address, and :start_date with actual values when querying.
        `;
    }

    getQueryTemplate(): string {
        return `
        # Database Schema
        {{databaseSchema}}

        # Query Examples
        {{queryExamples}}

        # User's Query
        {{userQuery}}

        # Query Guidelines:
        1. Time Range Requirements:
           - ALWAYS include time range limitations in queries
           - Default to last 3 months if no specific time range is mentioned
           - Use date_parse(date, '%Y-%m-%d') >= date_add('month', -3, current_date) for default time range
           - Adjust time range based on user's specific requirements

        2. Query Optimization:
           - Include appropriate LIMIT clauses
           - Use proper indexing columns (date, address, block_number)
           - Consider partitioning by date
           - Add WHERE clauses for efficient filtering

        3. Response Format Requirements:
           You MUST respond in the following JSON format:
           {
             "sql": {
               "query": "your SQL query string",
               "explanation": "brief explanation of the query",
               "timeRange": "specified time range in the query"
             },
             "analysis": {
               "overview": {
                 "totalTransactions": "number",
                 "timeSpan": "time period covered",
                 "keyMetrics": ["list of important metrics"]
               },
               "patterns": {
                 "transactionPatterns": ["identified patterns"],
                 "addressBehavior": ["address analysis"],
                 "temporalTrends": ["time-based trends"]
               },
               "statistics": {
                 "averages": {},
                 "distributions": {},
                 "anomalies": []
               },
               "insights": ["key insights from the data"],
               "recommendations": ["suggested actions or areas for further investigation"]
             }
           }

        4. Analysis Requirements:
           - Focus on recent data patterns
           - Identify trends and anomalies
           - Provide statistical analysis
           - Include risk assessment
           - Suggest further investigations

        Example Response:
        {
          "sql": {
            "query": "WITH recent_txs AS (SELECT * FROM eth.transactions WHERE date_parse(date, '%Y-%m-%d') >= date_add('month', -3, current_date))...",
            "explanation": "Query fetches last 3 months of transactions with aggregated metrics",
            "timeRange": "Last 3 months"
          },
          "analysis": {
            "overview": {
              "totalTransactions": 1000000,
              "timeSpan": "2024-01-01 to 2024-03-12",
              "keyMetrics": ["Average daily transactions: 11000", "Peak day: 2024-02-15"]
            },
            "patterns": {
              "transactionPatterns": ["High volume during Asian trading hours", "Weekend dips in activity"],
              "addressBehavior": ["5 addresses responsible for 30% of volume", "Increasing DEX activity"],
              "temporalTrends": ["Growing transaction volume", "Decreasing gas costs"]
            },
            "statistics": {
              "averages": {
                "dailyTransactions": 11000,
                "gasPrice": "25 gwei"
              },
              "distributions": {
                "valueRanges": ["0-1 ETH: 60%", "1-10 ETH: 30%", ">10 ETH: 10%"]
              },
              "anomalies": ["Unusual spike in gas prices on 2024-02-01"]
            },
            "insights": [
              "Growing DeFi activity indicated by smart contract interactions",
              "Whale addresses showing increased accumulation"
            ],
            "recommendations": [
              "Monitor growing gas usage trend",
              "Track new active addresses for potential market signals"
            ]
          }
        }
        `;
    }

    getAnalysisInstruction(): string {
        return `
            1. Data Overview:
                - Analyze the overall pattern in the query results
                - Identify key metrics and their significance
                - Note any unusual or interesting patterns

            2. Transaction Analysis:
                - Examine transaction values and their distribution
                - Analyze gas usage patterns
                - Evaluate transaction frequency and timing
                - Identify significant transactions or patterns

            3. Address Behavior:
                - Analyze address interactions
                - Identify frequent participants
                - Evaluate transaction patterns for specific addresses
                - Note any suspicious or interesting behavior

            4. Temporal Patterns:
                - Analyze time-based patterns
                - Identify peak activity periods
                - Note any temporal anomalies
                - Consider seasonal or cyclical patterns

            5. Token Analysis (if applicable):
                - Examine token transfer patterns
                - Analyze token holder behavior
                - Evaluate token concentration
                - Note significant token movements

            6. Statistical Insights:
                - Provide relevant statistical measures
                - Compare with typical blockchain metrics
                - Highlight significant deviations
                - Consider historical context

            7. Risk Assessment:
                - Identify potential suspicious activities
                - Note any unusual patterns
                - Flag potential security concerns
                - Consider regulatory implications

            Please provide a comprehensive analysis of the Ethereum blockchain data based on these ethereum information.
            Focus on significant patterns, anomalies, and insights that would be valuable for understanding the blockchain activity.
            Use technical blockchain terminology and provide specific examples from the data to support your analysis.

            Note: This analysis is based on simulated data for demonstration purposes.
        `;
    }
}

export const databaseProvider = (runtime: IAgentRuntime) => {
    const chain = "ethereum-mainnet";
    return new DatabaseProvider(chain, runtime);
};

export const ethereumDataProvider: Provider = {
    get: async (
        runtime: IAgentRuntime,
        message: Memory,
        state: State
    ): Promise<string | null> => {
        try {
            const provider = databaseProvider(runtime);
            const schema = provider.getDatabaseSchema();
            const examples = provider.getQueryExamples();
            const template = provider.getQueryTemplate();

            if (!state) {
                state = (await runtime.composeState(message)) as State;
            } else {
                state = await runtime.updateRecentMessageState(state);
            }

            const buildContext = template
                .replace("{{databaseSchema}}", schema)
                .replace("{{queryExamples}}", examples)
                .replace("{{userQuery}}", message.content.text || "");

            const context = JSON.stringify({
                user: runtime.agentId,
                content: buildContext,
                action: "NONE",
            });

            const preResponse = await generateMessageResponse({
                runtime: runtime,
                context: context,
                modelClass: ModelClass.LARGE,
            });

            const userMessage = {
                agentId: runtime.agentId,
                roomId: message.roomId,
                userId: message.userId,
                content: message.content,
            };

            // Save response to memory
            const preResponseMessage: Memory = {
                id: stringToUuid(message.id + "-" + runtime.agentId),
                ...userMessage,
                userId: runtime.agentId,
                content: preResponse,
                embedding: getEmbeddingZeroVector(),
                createdAt: Date.now(),
            };

            await runtime.messageManager.createMemory(preResponseMessage);
            await runtime.updateRecentMessageState(state);

            // Check for SQL query in the response using class method
            const sqlQuery = provider.extractSQLQuery(preResponse);
            if (sqlQuery) {
                elizaLogger.log("%%%% D.A.T.A. Generated SQL query:", sqlQuery);
                const analysisInstruction = provider.getAnalysisInstruction();
                try {
                    // Call query method on provider
                    const queryResult = await provider.query(sqlQuery);

                    elizaLogger.log("%%%% D.A.T.A. queryResult", queryResult);
                    // Return combined context with query results and analysis instructions
                    return `
                    # query by user
                    ${message.content.text}

                    # query result
                    ${JSON.stringify(queryResult, null, 2)}

                    # Analysis Instructions
                    ${analysisInstruction}
                    `;
                } catch (error) {
                    elizaLogger.error("Error executing query:", error);
                    return context;
                }
            } else {
                elizaLogger.log("no sql query found in user message");
            }
            return context;
        } catch (error) {
            elizaLogger.error("Error in ethereum data provider:", error);
            return null;
        }
    },
};

```

`/Users/tuyukai/go/src/github.com/carv-protocol/d.a.t.a/eliza/plugin-d.a.t.a/src/evaluators/data_evaluator.ts`:

```ts
import {
  Evaluator,
  IAgentRuntime,
  Memory,
  elizaLogger,
} from "@elizaos/core";

export const dataEvaluator: Evaluator = {
  alwaysRun: false,
  name: "GET_ONCHAIN_DATA",
  similes: ["GET_ONCHAIN_INFO"],
  description: "Evaluates for onchain data",
  validate: async (runtime: IAgentRuntime, message: Memory) => true,
  handler: async (runtime: IAgentRuntime, message: Memory) => {
      // Evaluation logic here
      elizaLogger.log("GET_ONCHAIN_DATA evaluator handler called");
      return true;
  },
  examples: [],
};
```

`/Users/tuyukai/go/src/github.com/carv-protocol/d.a.t.a/eliza/plugin-d.a.t.a/src/actions/fetchTransaction.ts`:

```ts
import {
    Action,
    HandlerCallback,
    IAgentRuntime,
    Memory,
    State,
    elizaLogger,
    composeContext,
    generateObject,
    ModelClass,
} from "@elizaos/core";
import {
    DatabaseProvider,
    databaseProvider,
} from "../providers/ethereum/database";
import { fetchTransactionTemplate } from "../templates";

// Query parameter interface with stricter types
interface FetchTransactionParams {
    address?: string;
    startDate?: string;
    endDate?: string;
    minValue?: string;
    maxValue?: string;
    limit?: number;
    orderBy?: "block_timestamp" | "value" | "gas_price";
    orderDirection?: "ASC" | "DESC";
}

// Response interface with enhanced metadata
interface TransactionQueryResult {
    success: boolean;
    data: any[];
    metadata: {
        total: number;
        queryTime: string;
        queryType: "transaction" | "token" | "aggregate" | "unknown";
        executionTime: number;
        cached: boolean;
        queryDetails?: {
            params: FetchTransactionParams;
            query: string;
            paramValidation?: string[];
        };
    };
    error?: {
        code: string;
        message: string;
        details?: any;
    };
}

export class FetchTransactionAction {
    constructor(private dbProvider: DatabaseProvider) { }

    private validateParams(params: FetchTransactionParams): string[] {
        const validationMessages: string[] = [];

        // Date format validation
        const dateRegex = /^\d{4}-\d{2}-\d{2}$/;
        if (params.startDate && !dateRegex.test(params.startDate)) {
            validationMessages.push(
                `Invalid start date format: ${params.startDate}`
            );
        }
        if (params.endDate && !dateRegex.test(params.endDate)) {
            validationMessages.push(
                `Invalid end date format: ${params.endDate}`
            );
        }

        // Address format validation
        if (params.address && !/^0x[a-fA-F0-9]{40}$/.test(params.address)) {
            validationMessages.push(
                `Invalid address format: ${params.address}`
            );
        }

        // Value validation
        if (params.minValue && isNaN(parseFloat(params.minValue))) {
            validationMessages.push(
                `Invalid minimum value: ${params.minValue}`
            );
        }
        if (params.maxValue && isNaN(parseFloat(params.maxValue))) {
            validationMessages.push(
                `Invalid maximum value: ${params.maxValue}`
            );
        }

        // Limit validation
        if (params.limit) {
            if (isNaN(params.limit)) {
                validationMessages.push(`Invalid limit: must be a number`);
            } else if (params.limit < 1 || params.limit > 100) {
                validationMessages.push(
                    `Invalid limit: ${params.limit}. Must be between 1 and 100`
                );
            }
        }

        // Order validation
        const validOrderBy = ["block_timestamp", "value", "gas_price"];
        if (params.orderBy && !validOrderBy.includes(params.orderBy)) {
            validationMessages.push(
                `Invalid orderBy: ${params.orderBy}. Must be one of: ${validOrderBy.join(
                    ", "
                )}`
            );
        }

        const validOrderDirection = ["ASC", "DESC"];
        if (
            params.orderDirection &&
            !validOrderDirection.includes(params.orderDirection)
        ) {
            validationMessages.push(
                `Invalid orderDirection: ${params.orderDirection
                }. Must be one of: ${validOrderDirection.join(", ")}`
            );
        }

        return validationMessages;
    }

    private buildSqlQuery(params: FetchTransactionParams): string {
        const conditions: string[] = [];

        // Add time range condition
        if (!params.startDate) {
            conditions.push(
                "date_parse(date, '%Y-%m-%d') >= date_add('month', -3, current_date)"
            );
        } else {
            conditions.push(`date >= '${params.startDate}'`);
            if (params.endDate) {
                conditions.push(`date <= '${params.endDate}'`);
            }
        }

        // Add address condition
        if (params.address) {
            conditions.push(
                `(from_address = '${params.address}' OR to_address = '${params.address}')`
            );
        }

        // Add value conditions
        if (params.minValue) {
            // Convert ETH to Wei for comparison
            const minValueWei = (parseFloat(params.minValue) * 1e18).toString();
            conditions.push(`value >= ${minValueWei}`);
        }
        if (params.maxValue) {
            const maxValueWei = (parseFloat(params.maxValue) * 1e18).toString();
            conditions.push(`value <= ${maxValueWei}`);
        }

        // Build the final query
        const query = `
            SELECT
                hash,
                block_number,
                block_timestamp,
                from_address,
                to_address,
                value / 1e18 as value_eth,
                gas,
                gas_price
            FROM eth.transactions
            WHERE ${conditions.join(" AND ")}
            ORDER BY ${params.orderBy || "block_timestamp"} ${params.orderDirection || "DESC"
            }
            LIMIT ${params.limit || 10}
        `;

        return query.trim();
    }

    public async fetchTransactions(
        message: string,
        runtime: IAgentRuntime,
        state: State
    ): Promise<TransactionQueryResult> {
        try {
            // Parse parameters using LLM
            const context = composeContext({
                state,
                template: fetchTransactionTemplate,
            });

            const paramsJson = (await generateObject({
                runtime,
                context,
                modelClass: ModelClass.SMALL,
            })) as FetchTransactionParams;

            // Validate parameters
            const validationMessages = this.validateParams(paramsJson);
            if (validationMessages.length > 0) {
                throw new Error(validationMessages.join("; "));
            }

            // Build and execute query
            const sqlQuery = this.buildSqlQuery(paramsJson);
            elizaLogger.log("Generated SQL query:", sqlQuery);

            const result = (await this.dbProvider.query(
                sqlQuery
            )) as TransactionQueryResult;

            // Enhance result with query details
            if (result.success) {
                result.metadata.queryDetails = {
                    params: paramsJson,
                    query: sqlQuery,
                    paramValidation: validationMessages,
                };
            }

            return result;
        } catch (error) {
            elizaLogger.error("Error fetching transactions:", error);
            return {
                success: false,
                data: [],
                metadata: {
                    total: 0,
                    queryTime: new Date().toISOString(),
                    queryType: "transaction",
                    executionTime: 0,
                    cached: false,
                },
                error: {
                    code: "FETCH_ERROR",
                    message: error.message,
                    details: error,
                },
            };
        }
    }
}

export const fetchTransactionAction: Action = {
    name: "fetch_transactions",
    description: "Fetch Ethereum transactions based on various criteria",
    similes: [
        "get transactions",
        "show transfers",
        "display eth transactions",
        "find transactions",
        "search transfers",
        "check transactions",
        "view transfers",
        "list transactions",
        "recent transactions",
        "transaction history",
    ],
    examples: [
        [
            {
                user: "user",
                content: {
                    text: "Show me the latest 10 Ethereum transactions",
                    action: "FETCH_TRANSACTIONS",
                },
            },
        ],
        [
            {
                user: "user",
                content: {
                    text: "Get transactions for address 0x742d35Cc6634C0532925a3b844Bc454e4438f44e",
                    action: "FETCH_TRANSACTIONS",
                },
            },
        ],
        [
            {
                user: "user",
                content: {
                    text: "Find transactions above 1 ETH from last month",
                    action: "FETCH_TRANSACTIONS",
                },
            },
        ],
    ],
    validate: async (runtime: IAgentRuntime) => {
        const apiKey = runtime.getSetting("DATA_API_KEY");
        const authToken = runtime.getSetting("DATA_AUTH_TOKEN");
        return !!(apiKey && authToken);
    },
    handler: async (
        runtime: IAgentRuntime,
        message: Memory,
        state: State,
        _options: any,
        callback?: HandlerCallback
    ) => {
        try {
            const provider = databaseProvider(runtime);
            const action = new FetchTransactionAction(provider);

            const result = await action.fetchTransactions(
                message.content.text,
                runtime,
                state
            );

            if (callback) {
                if (result.success) {
                    const params = result.metadata.queryDetails?.params;
                    let details = "";
                    if (params) {
                        details = `
- Address: ${params.address || "any"}
- Date Range: ${params.startDate || "last 3 months"} to ${params.endDate || "now"
                            }
- Value Range: ${params.minValue ? `>${params.minValue} ETH` : "any"} ${params.maxValue ? `to <${params.maxValue} ETH` : ""
                            }
- Showing: ${params.limit || 10} transactions
- Ordered by: ${params.orderBy || "timestamp"} ${params.orderDirection || "DESC"
                            }`;
                    }

                    callback({
                        text: `Found ${result.metadata.total
                            } transactions with the following criteria:${details}\n\nHere are the details:`,
                        content: {
                            success: true,
                            data: result.data,
                            metadata: result.metadata,
                        },
                    });
                } else {
                    callback({
                        text: `Error fetching transactions: ${result.error?.message}`,
                        content: { error: result.error },
                    });
                }
            }

            return result.success;
        } catch (error) {
            elizaLogger.error("Error in fetch transaction action:", error);
            if (callback) {
                callback({
                    text: `Error fetching transactions: ${error.message}`,
                    content: { error: error.message },
                });
            }
            return false;
        }
    },
};

export default fetchTransactionAction;

```

`/Users/tuyukai/go/src/github.com/carv-protocol/d.a.t.a/eliza/plugin-d.a.t.a/src/templates/index.ts`:

```ts
export const fetchTransactionTemplate = `Respond with a JSON markdown block containing only the extracted values. Use null for any values that cannot be determined.

Example response:
\`\`\`json
{
    "address": "0x742d35Cc6634C0532925a3b844Bc454e4438f44e",
    "startDate": "2024-01-01",
    "endDate": "2024-03-01",
    "minValue": "1.5",
    "maxValue": null,
    "limit": 10,
    "orderBy": "block_timestamp",
    "orderDirection": "DESC"
}
\`\`\`

{{recentMessages}}

Given the recent messages, extract the following information about the transaction query:
- Wallet address to query (if any)
- Start date (YYYY-MM-DD format)
- End date (YYYY-MM-DD format)
- Minimum value in ETH (if any)
- Maximum value in ETH (if any)
- Number of transactions to return (default 10, max 100)
- Order by field (block_timestamp, value, or gas_price)
- Order direction (ASC or DESC)

Respond with a JSON markdown block containing only the extracted values.`;

```

`/Users/tuyukai/go/src/github.com/carv-protocol/d.a.t.a/eliza/plugin-d.a.t.a/src/index.ts`:

```ts
import { Plugin } from "@elizaos/core";
import { ethereumDataProvider } from "./providers/ethereum/database";

export const onchainDataPlugin: Plugin = {
    name: "onchain data plugin",
    description: "Enables onchain data fetching",
    actions: [],
    providers: [ethereumDataProvider],
    evaluators: [],
    services: [],
    clients: [],
};

```

`/Users/tuyukai/go/src/github.com/carv-protocol/d.a.t.a/eliza/plugin-d.a.t.a/src/data_service.ts`:

```ts
import {
    IAgentRuntime,
    IMemoryManager,
    Service,
    Memory,
    MemoryManager,
} from "@elizaos/core";
import { stringToUuid, ServiceType } from "@elizaos/core";
import * as fs from "fs";
import * as path from "path";
import { txsArray } from "./txs.json.ts";
// const DATA_FILE_PATH = path.join(__dirname, "txs.json");

export const BLOCKCHAIN_DATA_TABLE_NAME = "d.a.t.a-blockchain-data";

type Transaction = {
    type: string;
    from: string;
    to: string;
    value: string;
    txHash: string;
    nonce: string;
    blockHash: string;
    blockNum: string;
};

export class DataService extends Service {
    static serviceType: ServiceType = ServiceType.INTIFACE;
    private runtime: IAgentRuntime | null = null;
    private blockchainDataManager: IMemoryManager | null = null;

    async initialize(runtime: IAgentRuntime): Promise<void> {
        console.log("Initializing carv data service");
        this.runtime = runtime;

        this.blockchainDataManager = new MemoryManager({
            runtime: runtime,
            tableName: BLOCKCHAIN_DATA_TABLE_NAME,
        });

        this.runtime.registerMemoryManager(this.blockchainDataManager);

        // console.log("Reading transactions from file...", DATA_FILE_PATH);
        // const txs = await readJsonFile(DATA_FILE_PATH);
        const txs = txsArray;
        for (const tx of txs) {
            console.log("Adding transaction to memory:", tx);

            const txInfo = formatTransaction(tx);
            const memory: Memory = {
                id: stringToUuid(txInfo),
                content: {
                    text: txInfo,
                },
                roomId: runtime.agentId,
                agentId: runtime.agentId,
                userId: runtime.agentId,
                createdAt: Date.now(),
            };
            await this.blockchainDataManager.addEmbeddingToMemory(memory);
            console.log("got memory:!!!!!");
            await this.blockchainDataManager.createMemory(memory);
        }
    }

    getBlockchainDataTableName(): string {
        return BLOCKCHAIN_DATA_TABLE_NAME;
    }
}

async function readJsonFile(filePath: string): Promise<Transaction[]> {
    try {
        // Read the file content
        const fileContent = await fs.readFileSync(filePath, "utf-8");

        // Parse the JSON content
        const jsonArray = JSON.parse(fileContent);

        // Check if the content is an array
        if (!Array.isArray(jsonArray)) {
            throw new Error("The file content is not a list of JSON objects.");
        }

        // Return the JSON objects
        return jsonArray as Transaction[];
    } catch (error) {
        console.error("Error reading or parsing the file:", error);
        throw error;
    }
}

function formatTransaction(transaction: Transaction): string {
    return `Transaction of type ${transaction.type} from ${transaction.from} to ${transaction.to} with value ${transaction.value}. Transaction hash: ${transaction.txHash}, nonce: ${transaction.nonce}, block hash: ${transaction.blockHash}, block number: ${transaction.blockNum}.`;
}

```

`/Users/tuyukai/go/src/github.com/carv-protocol/d.a.t.a/README.md`:

```md

# D.A.T.A
The D.A.T.A Framework (Data Authentication, Trust, and Attestation) is a cutting-edge solution developed by CARV to empower AI agents with unparalleled access to high-quality on-chain and off-chain data. Designed as a plugin for AI frameworks like Eliza, D.A.T.A enables AI agents to fetch, process, and act on data autonomously, fostering a new era of intelligent, data-driven decision-making.

## Documents

[D.A.T.A Framework](https://docs.carv.io/d.a.t.a.-ai-framework/introduction)

At its core, D.A.T.A bridges the gap between raw data and actionable insights by providing a complete lifecycle for data interaction. With robust integration into blockchain networks like Ethereum and Solana, as well as access to off-chain information through advanced tools such as vector databases, D.A.T.A equips AI agents to understand, interact with, and respond to the world more effectively.

Key highlights of the D.A.T.A Framework:

- On-Chain Data Access: Fetch blockchain data, such as balances, transaction histories, and activity metrics, through scalable backend architectures using tools like AWS Lambda, Google Cloud Functions, and Amazon Athena.
- Off-Chain Data Integration: Enrich on-chain insights with contextual data, including user profiles, token metadata, and market information.
- Autonomous Decision-Making: Allow AI agents to determine and execute actions based on data, from sending alerts to performing on-chain transactions.
- Cross-Chain Insights: Aggregate and unify data across multiple blockchains, enabling comprehensive understanding and decision-making.
- Memory Sharing: Enable collaborative intelligence among AI agents through shared on-chain memory and a centralized knowledge repository.

The D.A.T.A Framework is a game-changer for developers, providing the tools needed to build powerful, intelligent AI agents that can autonomously interact with decentralized ecosystems, make data-driven decisions, and evolve over time.
```

`/Users/tuyukai/go/src/github.com/carv-protocol/d.a.t.a/d.a.t.a.json`:

```json
Project Path: d.a.t.a

Source Tree:

```
d.a.t.a
├── eliza
│   ├── plugin-d.a.t.a
│   │   ├── dist
│   │   │   ├── index.d.mts
│   │   │   ├── index.js
│   │   │   ├── index.mjs
│   │   │   └── index.d.ts
│   │   ├── node_modules
│   │   │   └── @elizaos
│   │   ├── package.json
│   │   ├── tsconfig.json
│   │   ├── vitest.config.ts
│   │   └── src
│   │       ├── providers
│   │       │   └── ethereum
│   │       │       ├── sequelize.ts
│   │       │       ├── block.ts
│   │       │       ├── txs.ts
│   │       │       └── database.ts
│   │       ├── txs.json.ts
│   │       ├── evaluators
│   │       │   └── data_evaluator.ts
│   │       ├── actions
│   │       │   └── fetchTransaction.ts
│   │       ├── templates
│   │       │   └── index.ts
│   │       ├── index.ts
│   │       └── data_service.ts
│   └── README.md
├── script
├── README.md
└── zerepy
    └── d.a.t.a_connection.py

```

`/Users/tuyukai/go/src/github.com/carv-protocol/d.a.t.a/eliza/plugin-d.a.t.a/dist/index.d.mts`:

```mts

export {  }

```

`/Users/tuyukai/go/src/github.com/carv-protocol/d.a.t.a/eliza/plugin-d.a.t.a/dist/index.js`:

```js
var __defProp = Object.defineProperty;
var __getOwnPropDesc = Object.getOwnPropertyDescriptor;
var __getOwnPropNames = Object.getOwnPropertyNames;
var __hasOwnProp = Object.prototype.hasOwnProperty;
var __export = (target, all) => {
  for (var name in all)
    __defProp(target, name, { get: all[name], enumerable: true });
};
var __copyProps = (to, from, except, desc) => {
  if (from && typeof from === "object" || typeof from === "function") {
    for (let key of __getOwnPropNames(from))
      if (!__hasOwnProp.call(to, key) && key !== except)
        __defProp(to, key, { get: () => from[key], enumerable: !(desc = __getOwnPropDesc(from, key)) || desc.enumerable });
  }
  return to;
};
var __toCommonJS = (mod) => __copyProps(__defProp({}, "__esModule", { value: true }), mod);

// src/index.ts
var index_exports = {};
__export(index_exports, {
  onchainDataPlugin: () => onchainDataPlugin
});
module.exports = __toCommonJS(index_exports);

// src/actions/fetchTransaction.ts
var import_core = require("@elizaos/core");
var fetchTransactionAction = {
  name: "fetch_transactions",
  description: "Fetch Ethereum transactions based on various criteria",
  similes: [
    "get transactions",
    "show transfers",
    "display eth transactions",
    "find transactions",
    "search transfers",
    "check transactions",
    "view transfers",
    "list transactions",
    "recent transactions",
    "transaction history",
    "transfer records",
    "eth movements",
    "wallet activity",
    "transaction lookup",
    "transfer search"
  ],
  examples: [
    [
      {
        user: "user",
        content: {
          text: "Show me the latest 10 Ethereum transactions",
          action: "FETCH_TRANSACTIONS"
        }
      }
    ],
    [
      {
        user: "user",
        content: {
          text: "Get transactions for address 0x742d35Cc6634C0532925a3b844Bc454e4438f44e",
          action: "FETCH_TRANSACTIONS"
        }
      }
    ],
    [
      {
        user: "user",
        content: {
          text: "Show me transactions above 10 ETH from last week",
          action: "FETCH_TRANSACTIONS"
        }
      }
    ],
    [
      {
        user: "user",
        content: {
          text: "Find failed transactions with highest gas fees",
          action: "FETCH_TRANSACTIONS"
        }
      }
    ],
    [
      {
        user: "user",
        content: {
          text: "Show me transactions between blocks 1000000 and 1000100",
          action: "FETCH_TRANSACTIONS"
        }
      }
    ]
  ],
  validate: async (runtime) => {
    return true;
  },
  handler: async (runtime, message, state, _options, callback) => {
    try {
      import_core.elizaLogger.log("$$$$ Fetching Ethereum transactions...");
      import_core.elizaLogger.log("$$$$message", message);
      if (callback) {
        const mockText = "This is the transaction details of ethereum";
        const mockQuery = "SELECT * FROM ethereum_transactions";
        const mockParams = {
          limit: 10,
          orderBy: "timestamp",
          orderDirection: "DESC"
        };
        callback({
          // text: `Here's the SQL query to retrieve Ethereum transactions:\n${sqlQuery}\nThis query will return the specified transactions, including details like transaction hash, block number, sender/receiver addresses, value, and gas used. Let me know if you'd like further analysis or specific details about any of these transactions!`,
          // content: {
          //     success: true,
          //     query: sqlQuery,
          //     params: queryParams,
          // },
          text: mockText,
          content: {
            success: true,
            query: mockQuery,
            params: mockParams
          }
        });
      }
      return true;
    } catch (error) {
      import_core.elizaLogger.error("Error in fetch transaction action:", error);
      if (callback) {
        callback({
          text: `Error fetching transactions: ${error.message}`,
          content: { error: error.message }
        });
      }
      return false;
    }
  }
};

// src/providers/ethereum/database.ts
var import_core2 = require("@elizaos/core");
var DatabaseProvider = class {
  chain;
  API_URL = "https://dev-interface.carv.io/ai-agent-backend/sql_query";
  // fake data
  MOCK_RESPONSE = {
    code: 0,
    msg: "Success",
    data: {
      column_infos: [
        "hash",
        "nonce",
        "transaction_index",
        "from_address",
        "to_address",
        "value",
        "gas",
        "gas_price",
        "input",
        "receipt_cumulative_gas_used",
        "receipt_gas_used",
        "receipt_contract_address",
        "receipt_root",
        "receipt_status",
        "block_timestamp",
        "block_number",
        "block_hash",
        "max_fee_per_gas",
        "max_priority_fee_per_gas",
        "transaction_type",
        "receipt_effective_gas_price",
        "date"
      ],
      rows: [
        {
          items: [
            "0xb9f2c4dd816305a29471f7e843f33b8a4f52c24bfac58dba5f6dd703bdcc347d",
            "131",
            "0",
            "0xb7b3690efa6b3f08d4ec289ff655c4b7bb15ee39",
            "0x32be343b94f860124dc4fee278fdcbd38c102d88",
            "5.06132703E18",
            "21000",
            "58587049895",
            "0x",
            "21000",
            "21000",
            "",
            "",
            "1",
            "2015-10-18 09:01:42.000",
            "401609",
            "0xab8cf7f52769cb62a8a970347a116368c2ae08581d412573dd09a8a4af99b4cd",
            "0",
            "0",
            "0",
            "58587049895",
            "2015-10-18"
          ]
        },
        {
          items: [
            "0xbca5c575aa36f6164dbd98bf7c1008791d615cb0f255c19db7c6b45b06367ba0",
            "9573",
            "0",
            "0x2a65aca4d5fc5b5c859090a6c34d164135398226",
            "0xff3a70d8d5692dd05d71175fa29e8565f7450f57",
            "2.7443251E18",
            "90000",
            "50000000000",
            "0x",
            "21000",
            "21000",
            "",
            "",
            "1",
            "2015-10-18 12:21:43.000",
            "402253",
            "0x290dfc39bec9918fca28c8cebe2beaa19f9303f3b432d62d1e3409b1195556d6",
            "0",
            "0",
            "0",
            "50000000000",
            "2015-10-18"
          ]
        }
      ]
    }
  };
  constructor(chain) {
    this.chain = chain;
  }
  extractSQLQuery(preResponse) {
    try {
      let jsonData = preResponse;
      if (typeof preResponse === "string") {
        try {
          jsonData = JSON.parse(preResponse);
        } catch (e) {
          import_core2.elizaLogger.error(
            "Failed to parse preResponse as JSON:",
            e
          );
          return null;
        }
      }
      const findSQLQuery = (obj) => {
        if (!obj) return null;
        if (typeof obj === "string") {
          const sqlPattern = /^\s*(SELECT|WITH)\s+[\s\S]+?(?:;|$)/i;
          const commentPattern = /--.*$|\/\*[\s\S]*?\*\//gm;
          const cleanStr = obj.replace(commentPattern, "").trim();
          if (sqlPattern.test(cleanStr)) {
            const unsafeKeywords = [
              "drop",
              "delete",
              "update",
              "insert",
              "alter",
              "create"
            ];
            const isUnsafe = unsafeKeywords.some(
              (keyword) => cleanStr.toLowerCase().includes(keyword)
            );
            if (!isUnsafe) {
              return cleanStr;
            }
          }
          return null;
        }
        if (Array.isArray(obj)) {
          for (const item of obj) {
            const result = findSQLQuery(item);
            if (result) return result;
          }
          return null;
        }
        if (typeof obj === "object") {
          for (const key of Object.keys(obj)) {
            if (key.toLowerCase() === "query" && obj.sql) {
              const result = findSQLQuery(obj[key]);
              if (result) return result;
            }
          }
          for (const key of Object.keys(obj)) {
            const result = findSQLQuery(obj[key]);
            if (result) return result;
          }
        }
        return null;
      };
      const sqlQuery = findSQLQuery(jsonData);
      if (!sqlQuery) {
        import_core2.elizaLogger.warn("No valid SQL query found in preResponse");
        return null;
      }
      return sqlQuery;
    } catch (error) {
      import_core2.elizaLogger.error("Error in extractSQLQuery:", error);
      return null;
    }
  }
  async sendSqlQuery(sql, mock = false) {
    if (mock) {
      import_core2.elizaLogger.log("Using mock data for SQL query");
      return this.MOCK_RESPONSE;
    }
    try {
      const response = await fetch(this.API_URL, {
        method: "POST",
        headers: {
          "Content-Type": "application/json"
        },
        body: JSON.stringify({
          sql_content: sql
        })
      });
      if (!response.ok) {
        throw new Error(`HTTP error! status: ${response.status}`);
      }
      const data = await response.json();
      return data;
    } catch (error) {
      import_core2.elizaLogger.error("Error sending SQL query to API:", error);
      throw error;
    }
  }
  // Transform API response data
  transformApiResponse(apiResponse) {
    const { column_infos, rows } = apiResponse.data;
    return rows.map((row) => {
      const rowData = {};
      row.items.forEach((value, index) => {
        const columnName = column_infos[index];
        rowData[columnName] = value;
      });
      return rowData;
    });
  }
  // Execute query
  async executeQuery(sql) {
    try {
      if (!sql || sql.length > 5e3) {
        throw new Error("Invalid SQL query length");
      }
      const queryType = sql.toLowerCase().includes("token_transfers") ? "token" : sql.toLowerCase().includes("count") ? "aggregate" : "transaction";
      const apiResponse = await this.sendSqlQuery(sql);
      if (apiResponse.code !== 0) {
        throw new Error(`API Error: ${apiResponse.msg}`);
      }
      const transformedData = this.transformApiResponse(apiResponse);
      const queryResult = {
        success: true,
        data: transformedData,
        metadata: {
          total: transformedData.length,
          queryTime: (/* @__PURE__ */ new Date()).toISOString(),
          queryType,
          executionTime: 0,
          cached: false
        }
      };
      return queryResult;
    } catch (error) {
      import_core2.elizaLogger.error("Query execution failed:", error);
      return {
        success: false,
        data: [],
        metadata: {
          total: 0,
          queryTime: (/* @__PURE__ */ new Date()).toISOString(),
          queryType: "unknown",
          executionTime: 0,
          cached: false
        },
        error: {
          code: error.code || "EXECUTION_ERROR",
          message: error.message || "Unknown error occurred",
          details: error
        }
      };
    }
  }
  async query(sql) {
    return this.executeQuery(sql);
  }
  getDatabaseSchema() {
    return `
        CREATE EXTERNAL TABLE transactions(
            hash string,
            nonce bigint,
            block_hash string,
            block_number bigint,
            block_timestamp timestamp,
            date string,
            transaction_index bigint,
            from_address string,
            to_address string,
            value double,
            gas bigint,
            gas_price bigint,
            input string,
            max_fee_per_gas bigint,
            max_priority_fee_per_gas bigint,
            transaction_type bigint
        ) PARTITIONED BY (date string)
        ROW FORMAT SERDE 'org.apache.hadoop.hive.ql.io.parquet.serde.ParquetHiveSerDe'
        STORED AS INPUTFORMAT 'org.apache.hadoop.hive.ql.io.parquet.MapredParquetInputFormat'
        OUTPUTFORMAT 'org.apache.hadoop.hive.ql.io.parquet.MapredParquetOutputFormat';

        CREATE EXTERNAL TABLE token_transfers(
            token_address string,
            from_address string,
            to_address string,
            value double,
            transaction_hash string,
            log_index bigint,
            block_timestamp timestamp,
            date string,
            block_number bigint,
            block_hash string
        ) PARTITIONED BY (date string)
        ROW FORMAT SERDE 'org.apache.hadoop.hive.ql.io.parquet.serde.ParquetHiveSerDe'
        STORED AS INPUTFORMAT 'org.apache.hadoop.hive.ql.io.parquet.MapredParquetInputFormat'
        OUTPUTFORMAT 'org.apache.hadoop.hive.ql.io.parquet.MapredParquetOutputFormat';
        `;
  }
  getQueryExamples() {
    return `
        Common Query Examples:

        1. Find Most Active Addresses in Last 7 Days:
        WITH address_activity AS (
            SELECT
                from_address AS address,
                COUNT(*) AS tx_count
            FROM
                eth.transactions
            WHERE date_parse(date, '%Y-%m-%d') >= date_add('day', -7, current_date)
            GROUP BY
                from_address
            UNION ALL
            SELECT
                to_address AS address,
                COUNT(*) AS tx_count
            FROM
                eth.transactions
            WHERE
                date_parse(date, '%Y-%m-%d') >= date_add('day', -7, current_date)
            GROUP BY
                to_address
        )
        SELECT
            address,
            SUM(tx_count) AS total_transactions
        FROM
            address_activity
        GROUP BY
            address
        ORDER BY
            total_transactions DESC
        LIMIT 10;

        2. Analyze Address Transaction Statistics (Last 30 Days):
        WITH recent_transactions AS (
            SELECT
                from_address,
                to_address,
                value,
                block_timestamp,
                CASE
                    WHEN from_address = :address THEN 'outgoing'
                    WHEN to_address = :address THEN 'incoming'
                    ELSE 'other'
                END AS transaction_type
            FROM eth.transactions
            WHERE date >= date_format(date_add('day', -30, current_date), '%Y-%m-%d')
                AND (from_address = :address OR to_address = :address)
        )
        SELECT
            transaction_type,
            COUNT(*) AS transaction_count,
            SUM(CASE WHEN transaction_type = 'outgoing' THEN value ELSE 0 END) AS total_outgoing_value,
            SUM(CASE WHEN transaction_type = 'incoming' THEN value ELSE 0 END) AS total_incoming_value
        FROM recent_transactions
        GROUP BY transaction_type;

        3. Token Transfer Analysis:
        WITH filtered_transactions AS (
            SELECT
                token_address,
                from_address,
                to_address,
                value,
                block_timestamp
            FROM eth.token_transfers
            WHERE token_address = :token_address
                AND date >= :start_date
        )
        SELECT
            COUNT(*) AS transaction_count,
            SUM(value) AS total_transaction_value,
            MAX(value) AS max_transaction_value,
            MIN(value) AS min_transaction_value,
            MAX_BY(from_address, value) AS max_value_from_address,
            MAX_BY(to_address, value) AS max_value_to_address,
            MIN_BY(from_address, value) AS min_value_from_address,
            MIN_BY(to_address, value) AS min_value_to_address
        FROM filtered_transactions;

        Note: Replace :address, :token_address, and :start_date with actual values when querying.
        `;
  }
  getQueryTemplate() {
    return `
        # Database Schema
        {{databaseSchema}}

        # Query Examples
        {{queryExamples}}

        # User's Query
        {{userQuery}}

        # Query Guidelines:
        1. Time Range Requirements:
           - ALWAYS include time range limitations in queries
           - Default to last 3 months if no specific time range is mentioned
           - Use date_parse(date, '%Y-%m-%d') >= date_add('month', -3, current_date) for default time range
           - Adjust time range based on user's specific requirements

        2. Query Optimization:
           - Include appropriate LIMIT clauses
           - Use proper indexing columns (date, address, block_number)
           - Consider partitioning by date
           - Add WHERE clauses for efficient filtering

        3. Response Format Requirements:
           You MUST respond in the following JSON format:
           {
             "sql": {
               "query": "your SQL query string",
               "explanation": "brief explanation of the query",
               "timeRange": "specified time range in the query"
             },
             "analysis": {
               "overview": {
                 "totalTransactions": "number",
                 "timeSpan": "time period covered",
                 "keyMetrics": ["list of important metrics"]
               },
               "patterns": {
                 "transactionPatterns": ["identified patterns"],
                 "addressBehavior": ["address analysis"],
                 "temporalTrends": ["time-based trends"]
               },
               "statistics": {
                 "averages": {},
                 "distributions": {},
                 "anomalies": []
               },
               "insights": ["key insights from the data"],
               "recommendations": ["suggested actions or areas for further investigation"]
             }
           }

        4. Analysis Requirements:
           - Focus on recent data patterns
           - Identify trends and anomalies
           - Provide statistical analysis
           - Include risk assessment
           - Suggest further investigations

        Example Response:
        {
          "sql": {
            "query": "WITH recent_txs AS (SELECT * FROM eth.transactions WHERE date_parse(date, '%Y-%m-%d') >= date_add('month', -3, current_date))...",
            "explanation": "Query fetches last 3 months of transactions with aggregated metrics",
            "timeRange": "Last 3 months"
          },
          "analysis": {
            "overview": {
              "totalTransactions": 1000000,
              "timeSpan": "2024-01-01 to 2024-03-12",
              "keyMetrics": ["Average daily transactions: 11000", "Peak day: 2024-02-15"]
            },
            "patterns": {
              "transactionPatterns": ["High volume during Asian trading hours", "Weekend dips in activity"],
              "addressBehavior": ["5 addresses responsible for 30% of volume", "Increasing DEX activity"],
              "temporalTrends": ["Growing transaction volume", "Decreasing gas costs"]
            },
            "statistics": {
              "averages": {
                "dailyTransactions": 11000,
                "gasPrice": "25 gwei"
              },
              "distributions": {
                "valueRanges": ["0-1 ETH: 60%", "1-10 ETH: 30%", ">10 ETH: 10%"]
              },
              "anomalies": ["Unusual spike in gas prices on 2024-02-01"]
            },
            "insights": [
              "Growing DeFi activity indicated by smart contract interactions",
              "Whale addresses showing increased accumulation"
            ],
            "recommendations": [
              "Monitor growing gas usage trend",
              "Track new active addresses for potential market signals"
            ]
          }
        }
        `;
  }
  getAnalysisInstruction() {
    return `
            1. Data Overview:
                - Analyze the overall pattern in the query results
                - Identify key metrics and their significance
                - Note any unusual or interesting patterns

            2. Transaction Analysis:
                - Examine transaction values and their distribution
                - Analyze gas usage patterns
                - Evaluate transaction frequency and timing
                - Identify significant transactions or patterns

            3. Address Behavior:
                - Analyze address interactions
                - Identify frequent participants
                - Evaluate transaction patterns for specific addresses
                - Note any suspicious or interesting behavior

            4. Temporal Patterns:
                - Analyze time-based patterns
                - Identify peak activity periods
                - Note any temporal anomalies
                - Consider seasonal or cyclical patterns

            5. Token Analysis (if applicable):
                - Examine token transfer patterns
                - Analyze token holder behavior
                - Evaluate token concentration
                - Note significant token movements

            6. Statistical Insights:
                - Provide relevant statistical measures
                - Compare with typical blockchain metrics
                - Highlight significant deviations
                - Consider historical context

            7. Risk Assessment:
                - Identify potential suspicious activities
                - Note any unusual patterns
                - Flag potential security concerns
                - Consider regulatory implications

            Please provide a comprehensive analysis of the Ethereum blockchain data based on these ethereum information.
            Focus on significant patterns, anomalies, and insights that would be valuable for understanding the blockchain activity.
            Use technical blockchain terminology and provide specific examples from the data to support your analysis.

            Note: This analysis is based on simulated data for demonstration purposes.
        `;
  }
};
var databaseProvider = (runtime) => {
  const chain = "ethereum-mainnet";
  return new DatabaseProvider(chain);
};
var ethereumDataProvider = {
  get: async (runtime, message, state) => {
    import_core2.elizaLogger.log("%%%% Pis Retrieving from ethereum data provider...");
    try {
      const provider = databaseProvider(runtime);
      const schema = provider.getDatabaseSchema();
      const examples = provider.getQueryExamples();
      const template = provider.getQueryTemplate();
      if (!state) {
        state = await runtime.composeState(message);
      } else {
        state = await runtime.updateRecentMessageState(state);
      }
      import_core2.elizaLogger.log("%%%%&& Pis Context:", message.content.text);
      const buildContext = template.replace("{{databaseSchema}}", schema).replace("{{queryExamples}}", examples).replace("{{userQuery}}", message.content.text || "");
      const context = JSON.stringify({
        user: runtime.agentId,
        content: buildContext,
        action: "fetch_transactions"
      });
      import_core2.elizaLogger.log("%%%% Pis Generated database context");
      const preResponse = await (0, import_core2.generateMessageResponse)({
        runtime,
        context,
        modelClass: import_core2.ModelClass.LARGE
      });
      const userMessage = {
        agentId: runtime.agentId,
        roomId: message.roomId,
        userId: message.userId,
        content: message.content
      };
      const preResponseMessage = {
        id: (0, import_core2.stringToUuid)(message.id + "-" + runtime.agentId),
        ...userMessage,
        userId: runtime.agentId,
        content: preResponse,
        embedding: (0, import_core2.getEmbeddingZeroVector)(),
        createdAt: Date.now()
      };
      await runtime.messageManager.createMemory(preResponseMessage);
      await runtime.updateRecentMessageState(state);
      import_core2.elizaLogger.log("**** Pis preResponse", preResponse);
      const sqlQuery = provider.extractSQLQuery(preResponse);
      if (sqlQuery) {
        import_core2.elizaLogger.log("%%%% Found SQL query:", sqlQuery);
        const analysisInstruction = provider.getAnalysisInstruction();
        try {
          const queryResult = await provider.query(sqlQuery);
          import_core2.elizaLogger.log("%%%% Pis queryResult", queryResult);
          return `
                    # query by user
                    ${message.content.text}

                    # query result
                    ${JSON.stringify(queryResult, null, 2)}

                    # Analysis Instructions
                    ${analysisInstruction}
                    `;
        } catch (error) {
          import_core2.elizaLogger.error("Error executing query:", error);
          return context;
        }
      } else {
        import_core2.elizaLogger.log("%%%% Pis no SQL query found");
      }
      return context;
    } catch (error) {
      import_core2.elizaLogger.error("Error in ethereum data provider:", error);
      return null;
    }
  }
};

// src/index.ts
var onchainDataPlugin = {
  name: "onchain data plugin",
  description: "Enables onchain data fetching",
  actions: [fetchTransactionAction],
  providers: [ethereumDataProvider],
  evaluators: [],
  // separate examples will be added for services and clients
  // services: [new DataService()],
  services: [],
  clients: []
};
// Annotate the CommonJS export names for ESM import in node:
0 && (module.exports = {
  onchainDataPlugin
});

```

`/Users/tuyukai/go/src/github.com/carv-protocol/d.a.t.a/eliza/plugin-d.a.t.a/dist/index.mjs`:

```mjs
var __create = Object.create;
var __defProp = Object.defineProperty;
var __getOwnPropDesc = Object.getOwnPropertyDescriptor;
var __getOwnPropNames = Object.getOwnPropertyNames;
var __getProtoOf = Object.getPrototypeOf;
var __hasOwnProp = Object.prototype.hasOwnProperty;
var __require = /* @__PURE__ */ ((x) => typeof require !== "undefined" ? require : typeof Proxy !== "undefined" ? new Proxy(x, {
  get: (a, b) => (typeof require !== "undefined" ? require : a)[b]
}) : x)(function(x) {
  if (typeof require !== "undefined") return require.apply(this, arguments);
  throw Error('Dynamic require of "' + x + '" is not supported');
});
var __commonJS = (cb, mod) => function __require2() {
  return mod || (0, cb[__getOwnPropNames(cb)[0]])((mod = { exports: {} }).exports, mod), mod.exports;
};
var __copyProps = (to, from, except, desc) => {
  if (from && typeof from === "object" || typeof from === "function") {
    for (let key of __getOwnPropNames(from))
      if (!__hasOwnProp.call(to, key) && key !== except)
        __defProp(to, key, { get: () => from[key], enumerable: !(desc = __getOwnPropDesc(from, key)) || desc.enumerable });
  }
  return to;
};
var __toESM = (mod, isNodeMode, target) => (target = mod != null ? __create(__getProtoOf(mod)) : {}, __copyProps(
  // If the importer is in node compatibility mode or this is not an ESM
  // file that has been converted to a CommonJS file using a Babel-
  // compatible transform (i.e. "__esModule" has not been set), then set
  // "default" to the CommonJS "module.exports" for node compatibility.
  isNodeMode || !mod || !mod.__esModule ? __defProp(target, "default", { value: mod, enumerable: true }) : target,
  mod
));

// ../../node_modules/universalify/index.js
var require_universalify = __commonJS({
  "../../node_modules/universalify/index.js"(exports) {
    "use strict";
    exports.fromCallback = function(fn) {
      return Object.defineProperty(function(...args) {
        if (typeof args[args.length - 1] === "function") fn.apply(this, args);
        else {
          return new Promise((resolve, reject) => {
            args.push((err, res) => err != null ? reject(err) : resolve(res));
            fn.apply(this, args);
          });
        }
      }, "name", { value: fn.name });
    };
    exports.fromPromise = function(fn) {
      return Object.defineProperty(function(...args) {
        const cb = args[args.length - 1];
        if (typeof cb !== "function") return fn.apply(this, args);
        else {
          args.pop();
          fn.apply(this, args).then((r) => cb(null, r), cb);
        }
      }, "name", { value: fn.name });
    };
  }
});

// ../../node_modules/graceful-fs/polyfills.js
var require_polyfills = __commonJS({
  "../../node_modules/graceful-fs/polyfills.js"(exports, module) {
    var constants = __require("constants");
    var origCwd = process.cwd;
    var cwd = null;
    var platform = process.env.GRACEFUL_FS_PLATFORM || process.platform;
    process.cwd = function() {
      if (!cwd)
        cwd = origCwd.call(process);
      return cwd;
    };
    try {
      process.cwd();
    } catch (er) {
    }
    if (typeof process.chdir === "function") {
      chdir = process.chdir;
      process.chdir = function(d) {
        cwd = null;
        chdir.call(process, d);
      };
      if (Object.setPrototypeOf) Object.setPrototypeOf(process.chdir, chdir);
    }
    var chdir;
    module.exports = patch;
    function patch(fs2) {
      if (constants.hasOwnProperty("O_SYMLINK") && process.version.match(/^v0\.6\.[0-2]|^v0\.5\./)) {
        patchLchmod(fs2);
      }
      if (!fs2.lutimes) {
        patchLutimes(fs2);
      }
      fs2.chown = chownFix(fs2.chown);
      fs2.fchown = chownFix(fs2.fchown);
      fs2.lchown = chownFix(fs2.lchown);
      fs2.chmod = chmodFix(fs2.chmod);
      fs2.fchmod = chmodFix(fs2.fchmod);
      fs2.lchmod = chmodFix(fs2.lchmod);
      fs2.chownSync = chownFixSync(fs2.chownSync);
      fs2.fchownSync = chownFixSync(fs2.fchownSync);
      fs2.lchownSync = chownFixSync(fs2.lchownSync);
      fs2.chmodSync = chmodFixSync(fs2.chmodSync);
      fs2.fchmodSync = chmodFixSync(fs2.fchmodSync);
      fs2.lchmodSync = chmodFixSync(fs2.lchmodSync);
      fs2.stat = statFix(fs2.stat);
      fs2.fstat = statFix(fs2.fstat);
      fs2.lstat = statFix(fs2.lstat);
      fs2.statSync = statFixSync(fs2.statSync);
      fs2.fstatSync = statFixSync(fs2.fstatSync);
      fs2.lstatSync = statFixSync(fs2.lstatSync);
      if (fs2.chmod && !fs2.lchmod) {
        fs2.lchmod = function(path2, mode, cb) {
          if (cb) process.nextTick(cb);
        };
        fs2.lchmodSync = function() {
        };
      }
      if (fs2.chown && !fs2.lchown) {
        fs2.lchown = function(path2, uid, gid, cb) {
          if (cb) process.nextTick(cb);
        };
        fs2.lchownSync = function() {
        };
      }
      if (platform === "win32") {
        fs2.rename = typeof fs2.rename !== "function" ? fs2.rename : function(fs$rename) {
          function rename(from, to, cb) {
            var start = Date.now();
            var backoff = 0;
            fs$rename(from, to, function CB(er) {
              if (er && (er.code === "EACCES" || er.code === "EPERM" || er.code === "EBUSY") && Date.now() - start < 6e4) {
                setTimeout(function() {
                  fs2.stat(to, function(stater, st) {
                    if (stater && stater.code === "ENOENT")
                      fs$rename(from, to, CB);
                    else
                      cb(er);
                  });
                }, backoff);
                if (backoff < 100)
                  backoff += 10;
                return;
              }
              if (cb) cb(er);
            });
          }
          if (Object.setPrototypeOf) Object.setPrototypeOf(rename, fs$rename);
          return rename;
        }(fs2.rename);
      }
      fs2.read = typeof fs2.read !== "function" ? fs2.read : function(fs$read) {
        function read(fd, buffer, offset, length, position, callback_) {
          var callback;
          if (callback_ && typeof callback_ === "function") {
            var eagCounter = 0;
            callback = function(er, _, __) {
              if (er && er.code === "EAGAIN" && eagCounter < 10) {
                eagCounter++;
                return fs$read.call(fs2, fd, buffer, offset, length, position, callback);
              }
              callback_.apply(this, arguments);
            };
          }
          return fs$read.call(fs2, fd, buffer, offset, length, position, callback);
        }
        if (Object.setPrototypeOf) Object.setPrototypeOf(read, fs$read);
        return read;
      }(fs2.read);
      fs2.readSync = typeof fs2.readSync !== "function" ? fs2.readSync : /* @__PURE__ */ function(fs$readSync) {
        return function(fd, buffer, offset, length, position) {
          var eagCounter = 0;
          while (true) {
            try {
              return fs$readSync.call(fs2, fd, buffer, offset, length, position);
            } catch (er) {
              if (er.code === "EAGAIN" && eagCounter < 10) {
                eagCounter++;
                continue;
              }
              throw er;
            }
          }
        };
      }(fs2.readSync);
      function patchLchmod(fs3) {
        fs3.lchmod = function(path2, mode, callback) {
          fs3.open(
            path2,
            constants.O_WRONLY | constants.O_SYMLINK,
            mode,
            function(err, fd) {
              if (err) {
                if (callback) callback(err);
                return;
              }
              fs3.fchmod(fd, mode, function(err2) {
                fs3.close(fd, function(err22) {
                  if (callback) callback(err2 || err22);
                });
              });
            }
          );
        };
        fs3.lchmodSync = function(path2, mode) {
          var fd = fs3.openSync(path2, constants.O_WRONLY | constants.O_SYMLINK, mode);
          var threw = true;
          var ret;
          try {
            ret = fs3.fchmodSync(fd, mode);
            threw = false;
          } finally {
            if (threw) {
              try {
                fs3.closeSync(fd);
              } catch (er) {
              }
            } else {
              fs3.closeSync(fd);
            }
          }
          return ret;
        };
      }
      function patchLutimes(fs3) {
        if (constants.hasOwnProperty("O_SYMLINK") && fs3.futimes) {
          fs3.lutimes = function(path2, at, mt, cb) {
            fs3.open(path2, constants.O_SYMLINK, function(er, fd) {
              if (er) {
                if (cb) cb(er);
                return;
              }
              fs3.futimes(fd, at, mt, function(er2) {
                fs3.close(fd, function(er22) {
                  if (cb) cb(er2 || er22);
                });
              });
            });
          };
          fs3.lutimesSync = function(path2, at, mt) {
            var fd = fs3.openSync(path2, constants.O_SYMLINK);
            var ret;
            var threw = true;
            try {
              ret = fs3.futimesSync(fd, at, mt);
              threw = false;
            } finally {
              if (threw) {
                try {
                  fs3.closeSync(fd);
                } catch (er) {
                }
              } else {
                fs3.closeSync(fd);
              }
            }
            return ret;
          };
        } else if (fs3.futimes) {
          fs3.lutimes = function(_a, _b, _c, cb) {
            if (cb) process.nextTick(cb);
          };
          fs3.lutimesSync = function() {
          };
        }
      }
      function chmodFix(orig) {
        if (!orig) return orig;
        return function(target, mode, cb) {
          return orig.call(fs2, target, mode, function(er) {
            if (chownErOk(er)) er = null;
            if (cb) cb.apply(this, arguments);
          });
        };
      }
      function chmodFixSync(orig) {
        if (!orig) return orig;
        return function(target, mode) {
          try {
            return orig.call(fs2, target, mode);
          } catch (er) {
            if (!chownErOk(er)) throw er;
          }
        };
      }
      function chownFix(orig) {
        if (!orig) return orig;
        return function(target, uid, gid, cb) {
          return orig.call(fs2, target, uid, gid, function(er) {
            if (chownErOk(er)) er = null;
            if (cb) cb.apply(this, arguments);
          });
        };
      }
      function chownFixSync(orig) {
        if (!orig) return orig;
        return function(target, uid, gid) {
          try {
            return orig.call(fs2, target, uid, gid);
          } catch (er) {
            if (!chownErOk(er)) throw er;
          }
        };
      }
      function statFix(orig) {
        if (!orig) return orig;
        return function(target, options, cb) {
          if (typeof options === "function") {
            cb = options;
            options = null;
          }
          function callback(er, stats) {
            if (stats) {
              if (stats.uid < 0) stats.uid += 4294967296;
              if (stats.gid < 0) stats.gid += 4294967296;
            }
            if (cb) cb.apply(this, arguments);
          }
          return options ? orig.call(fs2, target, options, callback) : orig.call(fs2, target, callback);
        };
      }
      function statFixSync(orig) {
        if (!orig) return orig;
        return function(target, options) {
          var stats = options ? orig.call(fs2, target, options) : orig.call(fs2, target);
          if (stats) {
            if (stats.uid < 0) stats.uid += 4294967296;
            if (stats.gid < 0) stats.gid += 4294967296;
          }
          return stats;
        };
      }
      function chownErOk(er) {
        if (!er)
          return true;
        if (er.code === "ENOSYS")
          return true;
        var nonroot = !process.getuid || process.getuid() !== 0;
        if (nonroot) {
          if (er.code === "EINVAL" || er.code === "EPERM")
            return true;
        }
        return false;
      }
    }
  }
});

// ../../node_modules/graceful-fs/legacy-streams.js
var require_legacy_streams = __commonJS({
  "../../node_modules/graceful-fs/legacy-streams.js"(exports, module) {
    var Stream = __require("stream").Stream;
    module.exports = legacy;
    function legacy(fs2) {
      return {
        ReadStream,
        WriteStream
      };
      function ReadStream(path2, options) {
        if (!(this instanceof ReadStream)) return new ReadStream(path2, options);
        Stream.call(this);
        var self = this;
        this.path = path2;
        this.fd = null;
        this.readable = true;
        this.paused = false;
        this.flags = "r";
        this.mode = 438;
        this.bufferSize = 64 * 1024;
        options = options || {};
        var keys = Object.keys(options);
        for (var index = 0, length = keys.length; index < length; index++) {
          var key = keys[index];
          this[key] = options[key];
        }
        if (this.encoding) this.setEncoding(this.encoding);
        if (this.start !== void 0) {
          if ("number" !== typeof this.start) {
            throw TypeError("start must be a Number");
          }
          if (this.end === void 0) {
            this.end = Infinity;
          } else if ("number" !== typeof this.end) {
            throw TypeError("end must be a Number");
          }
          if (this.start > this.end) {
            throw new Error("start must be <= end");
          }
          this.pos = this.start;
        }
        if (this.fd !== null) {
          process.nextTick(function() {
            self._read();
          });
          return;
        }
        fs2.open(this.path, this.flags, this.mode, function(err, fd) {
          if (err) {
            self.emit("error", err);
            self.readable = false;
            return;
          }
          self.fd = fd;
          self.emit("open", fd);
          self._read();
        });
      }
      function WriteStream(path2, options) {
        if (!(this instanceof WriteStream)) return new WriteStream(path2, options);
        Stream.call(this);
        this.path = path2;
        this.fd = null;
        this.writable = true;
        this.flags = "w";
        this.encoding = "binary";
        this.mode = 438;
        this.bytesWritten = 0;
        options = options || {};
        var keys = Object.keys(options);
        for (var index = 0, length = keys.length; index < length; index++) {
          var key = keys[index];
          this[key] = options[key];
        }
        if (this.start !== void 0) {
          if ("number" !== typeof this.start) {
            throw TypeError("start must be a Number");
          }
          if (this.start < 0) {
            throw new Error("start must be >= zero");
          }
          this.pos = this.start;
        }
        this.busy = false;
        this._queue = [];
        if (this.fd === null) {
          this._open = fs2.open;
          this._queue.push([this._open, this.path, this.flags, this.mode, void 0]);
          this.flush();
        }
      }
    }
  }
});

// ../../node_modules/graceful-fs/clone.js
var require_clone = __commonJS({
  "../../node_modules/graceful-fs/clone.js"(exports, module) {
    "use strict";
    module.exports = clone;
    var getPrototypeOf = Object.getPrototypeOf || function(obj) {
      return obj.__proto__;
    };
    function clone(obj) {
      if (obj === null || typeof obj !== "object")
        return obj;
      if (obj instanceof Object)
        var copy = { __proto__: getPrototypeOf(obj) };
      else
        var copy = /* @__PURE__ */ Object.create(null);
      Object.getOwnPropertyNames(obj).forEach(function(key) {
        Object.defineProperty(copy, key, Object.getOwnPropertyDescriptor(obj, key));
      });
      return copy;
    }
  }
});

// ../../node_modules/graceful-fs/graceful-fs.js
var require_graceful_fs = __commonJS({
  "../../node_modules/graceful-fs/graceful-fs.js"(exports, module) {
    var fs2 = __require("fs");
    var polyfills = require_polyfills();
    var legacy = require_legacy_streams();
    var clone = require_clone();
    var util = __require("util");
    var gracefulQueue;
    var previousSymbol;
    if (typeof Symbol === "function" && typeof Symbol.for === "function") {
      gracefulQueue = Symbol.for("graceful-fs.queue");
      previousSymbol = Symbol.for("graceful-fs.previous");
    } else {
      gracefulQueue = "___graceful-fs.queue";
      previousSymbol = "___graceful-fs.previous";
    }
    function noop() {
    }
    function publishQueue(context, queue2) {
      Object.defineProperty(context, gracefulQueue, {
        get: function() {
          return queue2;
        }
      });
    }
    var debug = noop;
    if (util.debuglog)
      debug = util.debuglog("gfs4");
    else if (/\bgfs4\b/i.test(process.env.NODE_DEBUG || ""))
      debug = function() {
        var m = util.format.apply(util, arguments);
        m = "GFS4: " + m.split(/\n/).join("\nGFS4: ");
        console.error(m);
      };
    if (!fs2[gracefulQueue]) {
      queue = global[gracefulQueue] || [];
      publishQueue(fs2, queue);
      fs2.close = function(fs$close) {
        function close(fd, cb) {
          return fs$close.call(fs2, fd, function(err) {
            if (!err) {
              resetQueue();
            }
            if (typeof cb === "function")
              cb.apply(this, arguments);
          });
        }
        Object.defineProperty(close, previousSymbol, {
          value: fs$close
        });
        return close;
      }(fs2.close);
      fs2.closeSync = function(fs$closeSync) {
        function closeSync(fd) {
          fs$closeSync.apply(fs2, arguments);
          resetQueue();
        }
        Object.defineProperty(closeSync, previousSymbol, {
          value: fs$closeSync
        });
        return closeSync;
      }(fs2.closeSync);
      if (/\bgfs4\b/i.test(process.env.NODE_DEBUG || "")) {
        process.on("exit", function() {
          debug(fs2[gracefulQueue]);
          __require("assert").equal(fs2[gracefulQueue].length, 0);
        });
      }
    }
    var queue;
    if (!global[gracefulQueue]) {
      publishQueue(global, fs2[gracefulQueue]);
    }
    module.exports = patch(clone(fs2));
    if (process.env.TEST_GRACEFUL_FS_GLOBAL_PATCH && !fs2.__patched) {
      module.exports = patch(fs2);
      fs2.__patched = true;
    }
    function patch(fs3) {
      polyfills(fs3);
      fs3.gracefulify = patch;
      fs3.createReadStream = createReadStream;
      fs3.createWriteStream = createWriteStream;
      var fs$readFile = fs3.readFile;
      fs3.readFile = readFile2;
      function readFile2(path2, options, cb) {
        if (typeof options === "function")
          cb = options, options = null;
        return go$readFile(path2, options, cb);
        function go$readFile(path3, options2, cb2, startTime) {
          return fs$readFile(path3, options2, function(err) {
            if (err && (err.code === "EMFILE" || err.code === "ENFILE"))
              enqueue([go$readFile, [path3, options2, cb2], err, startTime || Date.now(), Date.now()]);
            else {
              if (typeof cb2 === "function")
                cb2.apply(this, arguments);
            }
          });
        }
      }
      var fs$writeFile = fs3.writeFile;
      fs3.writeFile = writeFile;
      function writeFile(path2, data, options, cb) {
        if (typeof options === "function")
          cb = options, options = null;
        return go$writeFile(path2, data, options, cb);
        function go$writeFile(path3, data2, options2, cb2, startTime) {
          return fs$writeFile(path3, data2, options2, function(err) {
            if (err && (err.code === "EMFILE" || err.code === "ENFILE"))
              enqueue([go$writeFile, [path3, data2, options2, cb2], err, startTime || Date.now(), Date.now()]);
            else {
              if (typeof cb2 === "function")
                cb2.apply(this, arguments);
            }
          });
        }
      }
      var fs$appendFile = fs3.appendFile;
      if (fs$appendFile)
        fs3.appendFile = appendFile;
      function appendFile(path2, data, options, cb) {
        if (typeof options === "function")
          cb = options, options = null;
        return go$appendFile(path2, data, options, cb);
        function go$appendFile(path3, data2, options2, cb2, startTime) {
          return fs$appendFile(path3, data2, options2, function(err) {
            if (err && (err.code === "EMFILE" || err.code === "ENFILE"))
              enqueue([go$appendFile, [path3, data2, options2, cb2], err, startTime || Date.now(), Date.now()]);
            else {
              if (typeof cb2 === "function")
                cb2.apply(this, arguments);
            }
          });
        }
      }
      var fs$copyFile = fs3.copyFile;
      if (fs$copyFile)
        fs3.copyFile = copyFile;
      function copyFile(src, dest, flags, cb) {
        if (typeof flags === "function") {
          cb = flags;
          flags = 0;
        }
        return go$copyFile(src, dest, flags, cb);
        function go$copyFile(src2, dest2, flags2, cb2, startTime) {
          return fs$copyFile(src2, dest2, flags2, function(err) {
            if (err && (err.code === "EMFILE" || err.code === "ENFILE"))
              enqueue([go$copyFile, [src2, dest2, flags2, cb2], err, startTime || Date.now(), Date.now()]);
            else {
              if (typeof cb2 === "function")
                cb2.apply(this, arguments);
            }
          });
        }
      }
      var fs$readdir = fs3.readdir;
      fs3.readdir = readdir;
      var noReaddirOptionVersions = /^v[0-5]\./;
      function readdir(path2, options, cb) {
        if (typeof options === "function")
          cb = options, options = null;
        var go$readdir = noReaddirOptionVersions.test(process.version) ? function go$readdir2(path3, options2, cb2, startTime) {
          return fs$readdir(path3, fs$readdirCallback(
            path3,
            options2,
            cb2,
            startTime
          ));
        } : function go$readdir2(path3, options2, cb2, startTime) {
          return fs$readdir(path3, options2, fs$readdirCallback(
            path3,
            options2,
            cb2,
            startTime
          ));
        };
        return go$readdir(path2, options, cb);
        function fs$readdirCallback(path3, options2, cb2, startTime) {
          return function(err, files) {
            if (err && (err.code === "EMFILE" || err.code === "ENFILE"))
              enqueue([
                go$readdir,
                [path3, options2, cb2],
                err,
                startTime || Date.now(),
                Date.now()
              ]);
            else {
              if (files && files.sort)
                files.sort();
              if (typeof cb2 === "function")
                cb2.call(this, err, files);
            }
          };
        }
      }
      if (process.version.substr(0, 4) === "v0.8") {
        var legStreams = legacy(fs3);
        ReadStream = legStreams.ReadStream;
        WriteStream = legStreams.WriteStream;
      }
      var fs$ReadStream = fs3.ReadStream;
      if (fs$ReadStream) {
        ReadStream.prototype = Object.create(fs$ReadStream.prototype);
        ReadStream.prototype.open = ReadStream$open;
      }
      var fs$WriteStream = fs3.WriteStream;
      if (fs$WriteStream) {
        WriteStream.prototype = Object.create(fs$WriteStream.prototype);
        WriteStream.prototype.open = WriteStream$open;
      }
      Object.defineProperty(fs3, "ReadStream", {
        get: function() {
          return ReadStream;
        },
        set: function(val) {
          ReadStream = val;
        },
        enumerable: true,
        configurable: true
      });
      Object.defineProperty(fs3, "WriteStream", {
        get: function() {
          return WriteStream;
        },
        set: function(val) {
          WriteStream = val;
        },
        enumerable: true,
        configurable: true
      });
      var FileReadStream = ReadStream;
      Object.defineProperty(fs3, "FileReadStream", {
        get: function() {
          return FileReadStream;
        },
        set: function(val) {
          FileReadStream = val;
        },
        enumerable: true,
        configurable: true
      });
      var FileWriteStream = WriteStream;
      Object.defineProperty(fs3, "FileWriteStream", {
        get: function() {
          return FileWriteStream;
        },
        set: function(val) {
          FileWriteStream = val;
        },
        enumerable: true,
        configurable: true
      });
      function ReadStream(path2, options) {
        if (this instanceof ReadStream)
          return fs$ReadStream.apply(this, arguments), this;
        else
          return ReadStream.apply(Object.create(ReadStream.prototype), arguments);
      }
      function ReadStream$open() {
        var that = this;
        open(that.path, that.flags, that.mode, function(err, fd) {
          if (err) {
            if (that.autoClose)
              that.destroy();
            that.emit("error", err);
          } else {
            that.fd = fd;
            that.emit("open", fd);
            that.read();
          }
        });
      }
      function WriteStream(path2, options) {
        if (this instanceof WriteStream)
          return fs$WriteStream.apply(this, arguments), this;
        else
          return WriteStream.apply(Object.create(WriteStream.prototype), arguments);
      }
      function WriteStream$open() {
        var that = this;
        open(that.path, that.flags, that.mode, function(err, fd) {
          if (err) {
            that.destroy();
            that.emit("error", err);
          } else {
            that.fd = fd;
            that.emit("open", fd);
          }
        });
      }
      function createReadStream(path2, options) {
        return new fs3.ReadStream(path2, options);
      }
      function createWriteStream(path2, options) {
        return new fs3.WriteStream(path2, options);
      }
      var fs$open = fs3.open;
      fs3.open = open;
      function open(path2, flags, mode, cb) {
        if (typeof mode === "function")
          cb = mode, mode = null;
        return go$open(path2, flags, mode, cb);
        function go$open(path3, flags2, mode2, cb2, startTime) {
          return fs$open(path3, flags2, mode2, function(err, fd) {
            if (err && (err.code === "EMFILE" || err.code === "ENFILE"))
              enqueue([go$open, [path3, flags2, mode2, cb2], err, startTime || Date.now(), Date.now()]);
            else {
              if (typeof cb2 === "function")
                cb2.apply(this, arguments);
            }
          });
        }
      }
      return fs3;
    }
    function enqueue(elem) {
      debug("ENQUEUE", elem[0].name, elem[1]);
      fs2[gracefulQueue].push(elem);
      retry();
    }
    var retryTimer;
    function resetQueue() {
      var now = Date.now();
      for (var i = 0; i < fs2[gracefulQueue].length; ++i) {
        if (fs2[gracefulQueue][i].length > 2) {
          fs2[gracefulQueue][i][3] = now;
          fs2[gracefulQueue][i][4] = now;
        }
      }
      retry();
    }
    function retry() {
      clearTimeout(retryTimer);
      retryTimer = void 0;
      if (fs2[gracefulQueue].length === 0)
        return;
      var elem = fs2[gracefulQueue].shift();
      var fn = elem[0];
      var args = elem[1];
      var err = elem[2];
      var startTime = elem[3];
      var lastTime = elem[4];
      if (startTime === void 0) {
        debug("RETRY", fn.name, args);
        fn.apply(null, args);
      } else if (Date.now() - startTime >= 6e4) {
        debug("TIMEOUT", fn.name, args);
        var cb = args.pop();
        if (typeof cb === "function")
          cb.call(null, err);
      } else {
        var sinceAttempt = Date.now() - lastTime;
        var sinceStart = Math.max(lastTime - startTime, 1);
        var desiredDelay = Math.min(sinceStart * 1.2, 100);
        if (sinceAttempt >= desiredDelay) {
          debug("RETRY", fn.name, args);
          fn.apply(null, args.concat([startTime]));
        } else {
          fs2[gracefulQueue].push(elem);
        }
      }
      if (retryTimer === void 0) {
        retryTimer = setTimeout(retry, 0);
      }
    }
  }
});

// ../../node_modules/fs-extra/lib/fs/index.js
var require_fs = __commonJS({
  "../../node_modules/fs-extra/lib/fs/index.js"(exports) {
    "use strict";
    var u = require_universalify().fromCallback;
    var fs2 = require_graceful_fs();
    var api = [
      "access",
      "appendFile",
      "chmod",
      "chown",
      "close",
      "copyFile",
      "fchmod",
      "fchown",
      "fdatasync",
      "fstat",
      "fsync",
      "ftruncate",
      "futimes",
      "lchmod",
      "lchown",
      "link",
      "lstat",
      "mkdir",
      "mkdtemp",
      "open",
      "opendir",
      "readdir",
      "readFile",
      "readlink",
      "realpath",
      "rename",
      "rm",
      "rmdir",
      "stat",
      "symlink",
      "truncate",
      "unlink",
      "utimes",
      "writeFile"
    ].filter((key) => {
      return typeof fs2[key] === "function";
    });
    Object.assign(exports, fs2);
    api.forEach((method) => {
      exports[method] = u(fs2[method]);
    });
    exports.exists = function(filename, callback) {
      if (typeof callback === "function") {
        return fs2.exists(filename, callback);
      }
      return new Promise((resolve) => {
        return fs2.exists(filename, resolve);
      });
    };
    exports.read = function(fd, buffer, offset, length, position, callback) {
      if (typeof callback === "function") {
        return fs2.read(fd, buffer, offset, length, position, callback);
      }
      return new Promise((resolve, reject) => {
        fs2.read(fd, buffer, offset, length, position, (err, bytesRead, buffer2) => {
          if (err) return reject(err);
          resolve({ bytesRead, buffer: buffer2 });
        });
      });
    };
    exports.write = function(fd, buffer, ...args) {
      if (typeof args[args.length - 1] === "function") {
        return fs2.write(fd, buffer, ...args);
      }
      return new Promise((resolve, reject) => {
        fs2.write(fd, buffer, ...args, (err, bytesWritten, buffer2) => {
          if (err) return reject(err);
          resolve({ bytesWritten, buffer: buffer2 });
        });
      });
    };
    exports.readv = function(fd, buffers, ...args) {
      if (typeof args[args.length - 1] === "function") {
        return fs2.readv(fd, buffers, ...args);
      }
      return new Promise((resolve, reject) => {
        fs2.readv(fd, buffers, ...args, (err, bytesRead, buffers2) => {
          if (err) return reject(err);
          resolve({ bytesRead, buffers: buffers2 });
        });
      });
    };
    exports.writev = function(fd, buffers, ...args) {
      if (typeof args[args.length - 1] === "function") {
        return fs2.writev(fd, buffers, ...args);
      }
      return new Promise((resolve, reject) => {
        fs2.writev(fd, buffers, ...args, (err, bytesWritten, buffers2) => {
          if (err) return reject(err);
          resolve({ bytesWritten, buffers: buffers2 });
        });
      });
    };
    if (typeof fs2.realpath.native === "function") {
      exports.realpath.native = u(fs2.realpath.native);
    } else {
      process.emitWarning(
        "fs.realpath.native is not a function. Is fs being monkey-patched?",
        "Warning",
        "fs-extra-WARN0003"
      );
    }
  }
});

// ../../node_modules/fs-extra/lib/mkdirs/utils.js
var require_utils = __commonJS({
  "../../node_modules/fs-extra/lib/mkdirs/utils.js"(exports, module) {
    "use strict";
    var path2 = __require("path");
    module.exports.checkPath = function checkPath(pth) {
      if (process.platform === "win32") {
        const pathHasInvalidWinCharacters = /[<>:"|?*]/.test(pth.replace(path2.parse(pth).root, ""));
        if (pathHasInvalidWinCharacters) {
          const error = new Error(`Path contains invalid characters: ${pth}`);
          error.code = "EINVAL";
          throw error;
        }
      }
    };
  }
});

// ../../node_modules/fs-extra/lib/mkdirs/make-dir.js
var require_make_dir = __commonJS({
  "../../node_modules/fs-extra/lib/mkdirs/make-dir.js"(exports, module) {
    "use strict";
    var fs2 = require_fs();
    var { checkPath } = require_utils();
    var getMode = (options) => {
      const defaults = { mode: 511 };
      if (typeof options === "number") return options;
      return { ...defaults, ...options }.mode;
    };
    module.exports.makeDir = async (dir, options) => {
      checkPath(dir);
      return fs2.mkdir(dir, {
        mode: getMode(options),
        recursive: true
      });
    };
    module.exports.makeDirSync = (dir, options) => {
      checkPath(dir);
      return fs2.mkdirSync(dir, {
        mode: getMode(options),
        recursive: true
      });
    };
  }
});

// ../../node_modules/fs-extra/lib/mkdirs/index.js
var require_mkdirs = __commonJS({
  "../../node_modules/fs-extra/lib/mkdirs/index.js"(exports, module) {
    "use strict";
    var u = require_universalify().fromPromise;
    var { makeDir: _makeDir, makeDirSync } = require_make_dir();
    var makeDir = u(_makeDir);
    module.exports = {
      mkdirs: makeDir,
      mkdirsSync: makeDirSync,
      // alias
      mkdirp: makeDir,
      mkdirpSync: makeDirSync,
      ensureDir: makeDir,
      ensureDirSync: makeDirSync
    };
  }
});

// ../../node_modules/fs-extra/lib/path-exists/index.js
var require_path_exists = __commonJS({
  "../../node_modules/fs-extra/lib/path-exists/index.js"(exports, module) {
    "use strict";
    var u = require_universalify().fromPromise;
    var fs2 = require_fs();
    function pathExists(path2) {
      return fs2.access(path2).then(() => true).catch(() => false);
    }
    module.exports = {
      pathExists: u(pathExists),
      pathExistsSync: fs2.existsSync
    };
  }
});

// ../../node_modules/fs-extra/lib/util/utimes.js
var require_utimes = __commonJS({
  "../../node_modules/fs-extra/lib/util/utimes.js"(exports, module) {
    "use strict";
    var fs2 = require_fs();
    var u = require_universalify().fromPromise;
    async function utimesMillis(path2, atime, mtime) {
      const fd = await fs2.open(path2, "r+");
      let closeErr = null;
      try {
        await fs2.futimes(fd, atime, mtime);
      } finally {
        try {
          await fs2.close(fd);
        } catch (e) {
          closeErr = e;
        }
      }
      if (closeErr) {
        throw closeErr;
      }
    }
    function utimesMillisSync(path2, atime, mtime) {
      const fd = fs2.openSync(path2, "r+");
      fs2.futimesSync(fd, atime, mtime);
      return fs2.closeSync(fd);
    }
    module.exports = {
      utimesMillis: u(utimesMillis),
      utimesMillisSync
    };
  }
});

// ../../node_modules/fs-extra/lib/util/stat.js
var require_stat = __commonJS({
  "../../node_modules/fs-extra/lib/util/stat.js"(exports, module) {
    "use strict";
    var fs2 = require_fs();
    var path2 = __require("path");
    var u = require_universalify().fromPromise;
    function getStats(src, dest, opts) {
      const statFunc = opts.dereference ? (file) => fs2.stat(file, { bigint: true }) : (file) => fs2.lstat(file, { bigint: true });
      return Promise.all([
        statFunc(src),
        statFunc(dest).catch((err) => {
          if (err.code === "ENOENT") return null;
          throw err;
        })
      ]).then(([srcStat, destStat]) => ({ srcStat, destStat }));
    }
    function getStatsSync(src, dest, opts) {
      let destStat;
      const statFunc = opts.dereference ? (file) => fs2.statSync(file, { bigint: true }) : (file) => fs2.lstatSync(file, { bigint: true });
      const srcStat = statFunc(src);
      try {
        destStat = statFunc(dest);
      } catch (err) {
        if (err.code === "ENOENT") return { srcStat, destStat: null };
        throw err;
      }
      return { srcStat, destStat };
    }
    async function checkPaths(src, dest, funcName, opts) {
      const { srcStat, destStat } = await getStats(src, dest, opts);
      if (destStat) {
        if (areIdentical(srcStat, destStat)) {
          const srcBaseName = path2.basename(src);
          const destBaseName = path2.basename(dest);
          if (funcName === "move" && srcBaseName !== destBaseName && srcBaseName.toLowerCase() === destBaseName.toLowerCase()) {
            return { srcStat, destStat, isChangingCase: true };
          }
          throw new Error("Source and destination must not be the same.");
        }
        if (srcStat.isDirectory() && !destStat.isDirectory()) {
          throw new Error(`Cannot overwrite non-directory '${dest}' with directory '${src}'.`);
        }
        if (!srcStat.isDirectory() && destStat.isDirectory()) {
          throw new Error(`Cannot overwrite directory '${dest}' with non-directory '${src}'.`);
        }
      }
      if (srcStat.isDirectory() && isSrcSubdir(src, dest)) {
        throw new Error(errMsg(src, dest, funcName));
      }
      return { srcStat, destStat };
    }
    function checkPathsSync(src, dest, funcName, opts) {
      const { srcStat, destStat } = getStatsSync(src, dest, opts);
      if (destStat) {
        if (areIdentical(srcStat, destStat)) {
          const srcBaseName = path2.basename(src);
          const destBaseName = path2.basename(dest);
          if (funcName === "move" && srcBaseName !== destBaseName && srcBaseName.toLowerCase() === destBaseName.toLowerCase()) {
            return { srcStat, destStat, isChangingCase: true };
          }
          throw new Error("Source and destination must not be the same.");
        }
        if (srcStat.isDirectory() && !destStat.isDirectory()) {
          throw new Error(`Cannot overwrite non-directory '${dest}' with directory '${src}'.`);
        }
        if (!srcStat.isDirectory() && destStat.isDirectory()) {
          throw new Error(`Cannot overwrite directory '${dest}' with non-directory '${src}'.`);
        }
      }
      if (srcStat.isDirectory() && isSrcSubdir(src, dest)) {
        throw new Error(errMsg(src, dest, funcName));
      }
      return { srcStat, destStat };
    }
    async function checkParentPaths(src, srcStat, dest, funcName) {
      const srcParent = path2.resolve(path2.dirname(src));
      const destParent = path2.resolve(path2.dirname(dest));
      if (destParent === srcParent || destParent === path2.parse(destParent).root) return;
      let destStat;
      try {
        destStat = await fs2.stat(destParent, { bigint: true });
      } catch (err) {
        if (err.code === "ENOENT") return;
        throw err;
      }
      if (areIdentical(srcStat, destStat)) {
        throw new Error(errMsg(src, dest, funcName));
      }
      return checkParentPaths(src, srcStat, destParent, funcName);
    }
    function checkParentPathsSync(src, srcStat, dest, funcName) {
      const srcParent = path2.resolve(path2.dirname(src));
      const destParent = path2.resolve(path2.dirname(dest));
      if (destParent === srcParent || destParent === path2.parse(destParent).root) return;
      let destStat;
      try {
        destStat = fs2.statSync(destParent, { bigint: true });
      } catch (err) {
        if (err.code === "ENOENT") return;
        throw err;
      }
      if (areIdentical(srcStat, destStat)) {
        throw new Error(errMsg(src, dest, funcName));
      }
      return checkParentPathsSync(src, srcStat, destParent, funcName);
    }
    function areIdentical(srcStat, destStat) {
      return destStat.ino && destStat.dev && destStat.ino === srcStat.ino && destStat.dev === srcStat.dev;
    }
    function isSrcSubdir(src, dest) {
      const srcArr = path2.resolve(src).split(path2.sep).filter((i) => i);
      const destArr = path2.resolve(dest).split(path2.sep).filter((i) => i);
      return srcArr.every((cur, i) => destArr[i] === cur);
    }
    function errMsg(src, dest, funcName) {
      return `Cannot ${funcName} '${src}' to a subdirectory of itself, '${dest}'.`;
    }
    module.exports = {
      // checkPaths
      checkPaths: u(checkPaths),
      checkPathsSync,
      // checkParent
      checkParentPaths: u(checkParentPaths),
      checkParentPathsSync,
      // Misc
      isSrcSubdir,
      areIdentical
    };
  }
});

// ../../node_modules/fs-extra/lib/copy/copy.js
var require_copy = __commonJS({
  "../../node_modules/fs-extra/lib/copy/copy.js"(exports, module) {
    "use strict";
    var fs2 = require_fs();
    var path2 = __require("path");
    var { mkdirs } = require_mkdirs();
    var { pathExists } = require_path_exists();
    var { utimesMillis } = require_utimes();
    var stat = require_stat();
    async function copy(src, dest, opts = {}) {
      if (typeof opts === "function") {
        opts = { filter: opts };
      }
      opts.clobber = "clobber" in opts ? !!opts.clobber : true;
      opts.overwrite = "overwrite" in opts ? !!opts.overwrite : opts.clobber;
      if (opts.preserveTimestamps && process.arch === "ia32") {
        process.emitWarning(
          "Using the preserveTimestamps option in 32-bit node is not recommended;\n\n	see https://github.com/jprichardson/node-fs-extra/issues/269",
          "Warning",
          "fs-extra-WARN0001"
        );
      }
      const { srcStat, destStat } = await stat.checkPaths(src, dest, "copy", opts);
      await stat.checkParentPaths(src, srcStat, dest, "copy");
      const include = await runFilter(src, dest, opts);
      if (!include) return;
      const destParent = path2.dirname(dest);
      const dirExists = await pathExists(destParent);
      if (!dirExists) {
        await mkdirs(destParent);
      }
      await getStatsAndPerformCopy(destStat, src, dest, opts);
    }
    async function runFilter(src, dest, opts) {
      if (!opts.filter) return true;
      return opts.filter(src, dest);
    }
    async function getStatsAndPerformCopy(destStat, src, dest, opts) {
      const statFn = opts.dereference ? fs2.stat : fs2.lstat;
      const srcStat = await statFn(src);
      if (srcStat.isDirectory()) return onDir(srcStat, destStat, src, dest, opts);
      if (srcStat.isFile() || srcStat.isCharacterDevice() || srcStat.isBlockDevice()) return onFile(srcStat, destStat, src, dest, opts);
      if (srcStat.isSymbolicLink()) return onLink(destStat, src, dest, opts);
      if (srcStat.isSocket()) throw new Error(`Cannot copy a socket file: ${src}`);
      if (srcStat.isFIFO()) throw new Error(`Cannot copy a FIFO pipe: ${src}`);
      throw new Error(`Unknown file: ${src}`);
    }
    async function onFile(srcStat, destStat, src, dest, opts) {
      if (!destStat) return copyFile(srcStat, src, dest, opts);
      if (opts.overwrite) {
        await fs2.unlink(dest);
        return copyFile(srcStat, src, dest, opts);
      }
      if (opts.errorOnExist) {
        throw new Error(`'${dest}' already exists`);
      }
    }
    async function copyFile(srcStat, src, dest, opts) {
      await fs2.copyFile(src, dest);
      if (opts.preserveTimestamps) {
        if (fileIsNotWritable(srcStat.mode)) {
          await makeFileWritable(dest, srcStat.mode);
        }
        const updatedSrcStat = await fs2.stat(src);
        await utimesMillis(dest, updatedSrcStat.atime, updatedSrcStat.mtime);
      }
      return fs2.chmod(dest, srcStat.mode);
    }
    function fileIsNotWritable(srcMode) {
      return (srcMode & 128) === 0;
    }
    function makeFileWritable(dest, srcMode) {
      return fs2.chmod(dest, srcMode | 128);
    }
    async function onDir(srcStat, destStat, src, dest, opts) {
      if (!destStat) {
        await fs2.mkdir(dest);
      }
      const items = await fs2.readdir(src);
      await Promise.all(items.map(async (item) => {
        const srcItem = path2.join(src, item);
        const destItem = path2.join(dest, item);
        const include = await runFilter(srcItem, destItem, opts);
        if (!include) return;
        const { destStat: destStat2 } = await stat.checkPaths(srcItem, destItem, "copy", opts);
        return getStatsAndPerformCopy(destStat2, srcItem, destItem, opts);
      }));
      if (!destStat) {
        await fs2.chmod(dest, srcStat.mode);
      }
    }
    async function onLink(destStat, src, dest, opts) {
      let resolvedSrc = await fs2.readlink(src);
      if (opts.dereference) {
        resolvedSrc = path2.resolve(process.cwd(), resolvedSrc);
      }
      if (!destStat) {
        return fs2.symlink(resolvedSrc, dest);
      }
      let resolvedDest = null;
      try {
        resolvedDest = await fs2.readlink(dest);
      } catch (e) {
        if (e.code === "EINVAL" || e.code === "UNKNOWN") return fs2.symlink(resolvedSrc, dest);
        throw e;
      }
      if (opts.dereference) {
        resolvedDest = path2.resolve(process.cwd(), resolvedDest);
      }
      if (stat.isSrcSubdir(resolvedSrc, resolvedDest)) {
        throw new Error(`Cannot copy '${resolvedSrc}' to a subdirectory of itself, '${resolvedDest}'.`);
      }
      if (stat.isSrcSubdir(resolvedDest, resolvedSrc)) {
        throw new Error(`Cannot overwrite '${resolvedDest}' with '${resolvedSrc}'.`);
      }
      await fs2.unlink(dest);
      return fs2.symlink(resolvedSrc, dest);
    }
    module.exports = copy;
  }
});

// ../../node_modules/fs-extra/lib/copy/copy-sync.js
var require_copy_sync = __commonJS({
  "../../node_modules/fs-extra/lib/copy/copy-sync.js"(exports, module) {
    "use strict";
    var fs2 = require_graceful_fs();
    var path2 = __require("path");
    var mkdirsSync = require_mkdirs().mkdirsSync;
    var utimesMillisSync = require_utimes().utimesMillisSync;
    var stat = require_stat();
    function copySync(src, dest, opts) {
      if (typeof opts === "function") {
        opts = { filter: opts };
      }
      opts = opts || {};
      opts.clobber = "clobber" in opts ? !!opts.clobber : true;
      opts.overwrite = "overwrite" in opts ? !!opts.overwrite : opts.clobber;
      if (opts.preserveTimestamps && process.arch === "ia32") {
        process.emitWarning(
          "Using the preserveTimestamps option in 32-bit node is not recommended;\n\n	see https://github.com/jprichardson/node-fs-extra/issues/269",
          "Warning",
          "fs-extra-WARN0002"
        );
      }
      const { srcStat, destStat } = stat.checkPathsSync(src, dest, "copy", opts);
      stat.checkParentPathsSync(src, srcStat, dest, "copy");
      if (opts.filter && !opts.filter(src, dest)) return;
      const destParent = path2.dirname(dest);
      if (!fs2.existsSync(destParent)) mkdirsSync(destParent);
      return getStats(destStat, src, dest, opts);
    }
    function getStats(destStat, src, dest, opts) {
      const statSync = opts.dereference ? fs2.statSync : fs2.lstatSync;
      const srcStat = statSync(src);
      if (srcStat.isDirectory()) return onDir(srcStat, destStat, src, dest, opts);
      else if (srcStat.isFile() || srcStat.isCharacterDevice() || srcStat.isBlockDevice()) return onFile(srcStat, destStat, src, dest, opts);
      else if (srcStat.isSymbolicLink()) return onLink(destStat, src, dest, opts);
      else if (srcStat.isSocket()) throw new Error(`Cannot copy a socket file: ${src}`);
      else if (srcStat.isFIFO()) throw new Error(`Cannot copy a FIFO pipe: ${src}`);
      throw new Error(`Unknown file: ${src}`);
    }
    function onFile(srcStat, destStat, src, dest, opts) {
      if (!destStat) return copyFile(srcStat, src, dest, opts);
      return mayCopyFile(srcStat, src, dest, opts);
    }
    function mayCopyFile(srcStat, src, dest, opts) {
      if (opts.overwrite) {
        fs2.unlinkSync(dest);
        return copyFile(srcStat, src, dest, opts);
      } else if (opts.errorOnExist) {
        throw new Error(`'${dest}' already exists`);
      }
    }
    function copyFile(srcStat, src, dest, opts) {
      fs2.copyFileSync(src, dest);
      if (opts.preserveTimestamps) handleTimestamps(srcStat.mode, src, dest);
      return setDestMode(dest, srcStat.mode);
    }
    function handleTimestamps(srcMode, src, dest) {
      if (fileIsNotWritable(srcMode)) makeFileWritable(dest, srcMode);
      return setDestTimestamps(src, dest);
    }
    function fileIsNotWritable(srcMode) {
      return (srcMode & 128) === 0;
    }
    function makeFileWritable(dest, srcMode) {
      return setDestMode(dest, srcMode | 128);
    }
    function setDestMode(dest, srcMode) {
      return fs2.chmodSync(dest, srcMode);
    }
    function setDestTimestamps(src, dest) {
      const updatedSrcStat = fs2.statSync(src);
      return utimesMillisSync(dest, updatedSrcStat.atime, updatedSrcStat.mtime);
    }
    function onDir(srcStat, destStat, src, dest, opts) {
      if (!destStat) return mkDirAndCopy(srcStat.mode, src, dest, opts);
      return copyDir(src, dest, opts);
    }
    function mkDirAndCopy(srcMode, src, dest, opts) {
      fs2.mkdirSync(dest);
      copyDir(src, dest, opts);
      return setDestMode(dest, srcMode);
    }
    function copyDir(src, dest, opts) {
      fs2.readdirSync(src).forEach((item) => copyDirItem(item, src, dest, opts));
    }
    function copyDirItem(item, src, dest, opts) {
      const srcItem = path2.join(src, item);
      const destItem = path2.join(dest, item);
      if (opts.filter && !opts.filter(srcItem, destItem)) return;
      const { destStat } = stat.checkPathsSync(srcItem, destItem, "copy", opts);
      return getStats(destStat, srcItem, destItem, opts);
    }
    function onLink(destStat, src, dest, opts) {
      let resolvedSrc = fs2.readlinkSync(src);
      if (opts.dereference) {
        resolvedSrc = path2.resolve(process.cwd(), resolvedSrc);
      }
      if (!destStat) {
        return fs2.symlinkSync(resolvedSrc, dest);
      } else {
        let resolvedDest;
        try {
          resolvedDest = fs2.readlinkSync(dest);
        } catch (err) {
          if (err.code === "EINVAL" || err.code === "UNKNOWN") return fs2.symlinkSync(resolvedSrc, dest);
          throw err;
        }
        if (opts.dereference) {
          resolvedDest = path2.resolve(process.cwd(), resolvedDest);
        }
        if (stat.isSrcSubdir(resolvedSrc, resolvedDest)) {
          throw new Error(`Cannot copy '${resolvedSrc}' to a subdirectory of itself, '${resolvedDest}'.`);
        }
        if (stat.isSrcSubdir(resolvedDest, resolvedSrc)) {
          throw new Error(`Cannot overwrite '${resolvedDest}' with '${resolvedSrc}'.`);
        }
        return copyLink(resolvedSrc, dest);
      }
    }
    function copyLink(resolvedSrc, dest) {
      fs2.unlinkSync(dest);
      return fs2.symlinkSync(resolvedSrc, dest);
    }
    module.exports = copySync;
  }
});

// ../../node_modules/fs-extra/lib/copy/index.js
var require_copy2 = __commonJS({
  "../../node_modules/fs-extra/lib/copy/index.js"(exports, module) {
    "use strict";
    var u = require_universalify().fromPromise;
    module.exports = {
      copy: u(require_copy()),
      copySync: require_copy_sync()
    };
  }
});

// ../../node_modules/fs-extra/lib/remove/index.js
var require_remove = __commonJS({
  "../../node_modules/fs-extra/lib/remove/index.js"(exports, module) {
    "use strict";
    var fs2 = require_graceful_fs();
    var u = require_universalify().fromCallback;
    function remove(path2, callback) {
      fs2.rm(path2, { recursive: true, force: true }, callback);
    }
    function removeSync(path2) {
      fs2.rmSync(path2, { recursive: true, force: true });
    }
    module.exports = {
      remove: u(remove),
      removeSync
    };
  }
});

// ../../node_modules/fs-extra/lib/empty/index.js
var require_empty = __commonJS({
  "../../node_modules/fs-extra/lib/empty/index.js"(exports, module) {
    "use strict";
    var u = require_universalify().fromPromise;
    var fs2 = require_fs();
    var path2 = __require("path");
    var mkdir = require_mkdirs();
    var remove = require_remove();
    var emptyDir = u(async function emptyDir2(dir) {
      let items;
      try {
        items = await fs2.readdir(dir);
      } catch {
        return mkdir.mkdirs(dir);
      }
      return Promise.all(items.map((item) => remove.remove(path2.join(dir, item))));
    });
    function emptyDirSync(dir) {
      let items;
      try {
        items = fs2.readdirSync(dir);
      } catch {
        return mkdir.mkdirsSync(dir);
      }
      items.forEach((item) => {
        item = path2.join(dir, item);
        remove.removeSync(item);
      });
    }
    module.exports = {
      emptyDirSync,
      emptydirSync: emptyDirSync,
      emptyDir,
      emptydir: emptyDir
    };
  }
});

// ../../node_modules/fs-extra/lib/ensure/file.js
var require_file = __commonJS({
  "../../node_modules/fs-extra/lib/ensure/file.js"(exports, module) {
    "use strict";
    var u = require_universalify().fromPromise;
    var path2 = __require("path");
    var fs2 = require_fs();
    var mkdir = require_mkdirs();
    async function createFile(file) {
      let stats;
      try {
        stats = await fs2.stat(file);
      } catch {
      }
      if (stats && stats.isFile()) return;
      const dir = path2.dirname(file);
      let dirStats = null;
      try {
        dirStats = await fs2.stat(dir);
      } catch (err) {
        if (err.code === "ENOENT") {
          await mkdir.mkdirs(dir);
          await fs2.writeFile(file, "");
          return;
        } else {
          throw err;
        }
      }
      if (dirStats.isDirectory()) {
        await fs2.writeFile(file, "");
      } else {
        await fs2.readdir(dir);
      }
    }
    function createFileSync(file) {
      let stats;
      try {
        stats = fs2.statSync(file);
      } catch {
      }
      if (stats && stats.isFile()) return;
      const dir = path2.dirname(file);
      try {
        if (!fs2.statSync(dir).isDirectory()) {
          fs2.readdirSync(dir);
        }
      } catch (err) {
        if (err && err.code === "ENOENT") mkdir.mkdirsSync(dir);
        else throw err;
      }
      fs2.writeFileSync(file, "");
    }
    module.exports = {
      createFile: u(createFile),
      createFileSync
    };
  }
});

// ../../node_modules/fs-extra/lib/ensure/link.js
var require_link = __commonJS({
  "../../node_modules/fs-extra/lib/ensure/link.js"(exports, module) {
    "use strict";
    var u = require_universalify().fromPromise;
    var path2 = __require("path");
    var fs2 = require_fs();
    var mkdir = require_mkdirs();
    var { pathExists } = require_path_exists();
    var { areIdentical } = require_stat();
    async function createLink(srcpath, dstpath) {
      let dstStat;
      try {
        dstStat = await fs2.lstat(dstpath);
      } catch {
      }
      let srcStat;
      try {
        srcStat = await fs2.lstat(srcpath);
      } catch (err) {
        err.message = err.message.replace("lstat", "ensureLink");
        throw err;
      }
      if (dstStat && areIdentical(srcStat, dstStat)) return;
      const dir = path2.dirname(dstpath);
      const dirExists = await pathExists(dir);
      if (!dirExists) {
        await mkdir.mkdirs(dir);
      }
      await fs2.link(srcpath, dstpath);
    }
    function createLinkSync(srcpath, dstpath) {
      let dstStat;
      try {
        dstStat = fs2.lstatSync(dstpath);
      } catch {
      }
      try {
        const srcStat = fs2.lstatSync(srcpath);
        if (dstStat && areIdentical(srcStat, dstStat)) return;
      } catch (err) {
        err.message = err.message.replace("lstat", "ensureLink");
        throw err;
      }
      const dir = path2.dirname(dstpath);
      const dirExists = fs2.existsSync(dir);
      if (dirExists) return fs2.linkSync(srcpath, dstpath);
      mkdir.mkdirsSync(dir);
      return fs2.linkSync(srcpath, dstpath);
    }
    module.exports = {
      createLink: u(createLink),
      createLinkSync
    };
  }
});

// ../../node_modules/fs-extra/lib/ensure/symlink-paths.js
var require_symlink_paths = __commonJS({
  "../../node_modules/fs-extra/lib/ensure/symlink-paths.js"(exports, module) {
    "use strict";
    var path2 = __require("path");
    var fs2 = require_fs();
    var { pathExists } = require_path_exists();
    var u = require_universalify().fromPromise;
    async function symlinkPaths(srcpath, dstpath) {
      if (path2.isAbsolute(srcpath)) {
        try {
          await fs2.lstat(srcpath);
        } catch (err) {
          err.message = err.message.replace("lstat", "ensureSymlink");
          throw err;
        }
        return {
          toCwd: srcpath,
          toDst: srcpath
        };
      }
      const dstdir = path2.dirname(dstpath);
      const relativeToDst = path2.join(dstdir, srcpath);
      const exists = await pathExists(relativeToDst);
      if (exists) {
        return {
          toCwd: relativeToDst,
          toDst: srcpath
        };
      }
      try {
        await fs2.lstat(srcpath);
      } catch (err) {
        err.message = err.message.replace("lstat", "ensureSymlink");
        throw err;
      }
      return {
        toCwd: srcpath,
        toDst: path2.relative(dstdir, srcpath)
      };
    }
    function symlinkPathsSync(srcpath, dstpath) {
      if (path2.isAbsolute(srcpath)) {
        const exists2 = fs2.existsSync(srcpath);
        if (!exists2) throw new Error("absolute srcpath does not exist");
        return {
          toCwd: srcpath,
          toDst: srcpath
        };
      }
      const dstdir = path2.dirname(dstpath);
      const relativeToDst = path2.join(dstdir, srcpath);
      const exists = fs2.existsSync(relativeToDst);
      if (exists) {
        return {
          toCwd: relativeToDst,
          toDst: srcpath
        };
      }
      const srcExists = fs2.existsSync(srcpath);
      if (!srcExists) throw new Error("relative srcpath does not exist");
      return {
        toCwd: srcpath,
        toDst: path2.relative(dstdir, srcpath)
      };
    }
    module.exports = {
      symlinkPaths: u(symlinkPaths),
      symlinkPathsSync
    };
  }
});

// ../../node_modules/fs-extra/lib/ensure/symlink-type.js
var require_symlink_type = __commonJS({
  "../../node_modules/fs-extra/lib/ensure/symlink-type.js"(exports, module) {
    "use strict";
    var fs2 = require_fs();
    var u = require_universalify().fromPromise;
    async function symlinkType(srcpath, type) {
      if (type) return type;
      let stats;
      try {
        stats = await fs2.lstat(srcpath);
      } catch {
        return "file";
      }
      return stats && stats.isDirectory() ? "dir" : "file";
    }
    function symlinkTypeSync(srcpath, type) {
      if (type) return type;
      let stats;
      try {
        stats = fs2.lstatSync(srcpath);
      } catch {
        return "file";
      }
      return stats && stats.isDirectory() ? "dir" : "file";
    }
    module.exports = {
      symlinkType: u(symlinkType),
      symlinkTypeSync
    };
  }
});

// ../../node_modules/fs-extra/lib/ensure/symlink.js
var require_symlink = __commonJS({
  "../../node_modules/fs-extra/lib/ensure/symlink.js"(exports, module) {
    "use strict";
    var u = require_universalify().fromPromise;
    var path2 = __require("path");
    var fs2 = require_fs();
    var { mkdirs, mkdirsSync } = require_mkdirs();
    var { symlinkPaths, symlinkPathsSync } = require_symlink_paths();
    var { symlinkType, symlinkTypeSync } = require_symlink_type();
    var { pathExists } = require_path_exists();
    var { areIdentical } = require_stat();
    async function createSymlink(srcpath, dstpath, type) {
      let stats;
      try {
        stats = await fs2.lstat(dstpath);
      } catch {
      }
      if (stats && stats.isSymbolicLink()) {
        const [srcStat, dstStat] = await Promise.all([
          fs2.stat(srcpath),
          fs2.stat(dstpath)
        ]);
        if (areIdentical(srcStat, dstStat)) return;
      }
      const relative = await symlinkPaths(srcpath, dstpath);
      srcpath = relative.toDst;
      const toType = await symlinkType(relative.toCwd, type);
      const dir = path2.dirname(dstpath);
      if (!await pathExists(dir)) {
        await mkdirs(dir);
      }
      return fs2.symlink(srcpath, dstpath, toType);
    }
    function createSymlinkSync(srcpath, dstpath, type) {
      let stats;
      try {
        stats = fs2.lstatSync(dstpath);
      } catch {
      }
      if (stats && stats.isSymbolicLink()) {
        const srcStat = fs2.statSync(srcpath);
        const dstStat = fs2.statSync(dstpath);
        if (areIdentical(srcStat, dstStat)) return;
      }
      const relative = symlinkPathsSync(srcpath, dstpath);
      srcpath = relative.toDst;
      type = symlinkTypeSync(relative.toCwd, type);
      const dir = path2.dirname(dstpath);
      const exists = fs2.existsSync(dir);
      if (exists) return fs2.symlinkSync(srcpath, dstpath, type);
      mkdirsSync(dir);
      return fs2.symlinkSync(srcpath, dstpath, type);
    }
    module.exports = {
      createSymlink: u(createSymlink),
      createSymlinkSync
    };
  }
});

// ../../node_modules/fs-extra/lib/ensure/index.js
var require_ensure = __commonJS({
  "../../node_modules/fs-extra/lib/ensure/index.js"(exports, module) {
    "use strict";
    var { createFile, createFileSync } = require_file();
    var { createLink, createLinkSync } = require_link();
    var { createSymlink, createSymlinkSync } = require_symlink();
    module.exports = {
      // file
      createFile,
      createFileSync,
      ensureFile: createFile,
      ensureFileSync: createFileSync,
      // link
      createLink,
      createLinkSync,
      ensureLink: createLink,
      ensureLinkSync: createLinkSync,
      // symlink
      createSymlink,
      createSymlinkSync,
      ensureSymlink: createSymlink,
      ensureSymlinkSync: createSymlinkSync
    };
  }
});

// ../../node_modules/jsonfile/utils.js
var require_utils2 = __commonJS({
  "../../node_modules/jsonfile/utils.js"(exports, module) {
    function stringify(obj, { EOL = "\n", finalEOL = true, replacer = null, spaces } = {}) {
      const EOF = finalEOL ? EOL : "";
      const str = JSON.stringify(obj, replacer, spaces);
      return str.replace(/\n/g, EOL) + EOF;
    }
    function stripBom(content) {
      if (Buffer.isBuffer(content)) content = content.toString("utf8");
      return content.replace(/^\uFEFF/, "");
    }
    module.exports = { stringify, stripBom };
  }
});

// ../../node_modules/jsonfile/index.js
var require_jsonfile = __commonJS({
  "../../node_modules/jsonfile/index.js"(exports, module) {
    var _fs;
    try {
      _fs = require_graceful_fs();
    } catch (_) {
      _fs = __require("fs");
    }
    var universalify = require_universalify();
    var { stringify, stripBom } = require_utils2();
    async function _readFile(file, options = {}) {
      if (typeof options === "string") {
        options = { encoding: options };
      }
      const fs2 = options.fs || _fs;
      const shouldThrow = "throws" in options ? options.throws : true;
      let data = await universalify.fromCallback(fs2.readFile)(file, options);
      data = stripBom(data);
      let obj;
      try {
        obj = JSON.parse(data, options ? options.reviver : null);
      } catch (err) {
        if (shouldThrow) {
          err.message = `${file}: ${err.message}`;
          throw err;
        } else {
          return null;
        }
      }
      return obj;
    }
    var readFile2 = universalify.fromPromise(_readFile);
    function readFileSync(file, options = {}) {
      if (typeof options === "string") {
        options = { encoding: options };
      }
      const fs2 = options.fs || _fs;
      const shouldThrow = "throws" in options ? options.throws : true;
      try {
        let content = fs2.readFileSync(file, options);
        content = stripBom(content);
        return JSON.parse(content, options.reviver);
      } catch (err) {
        if (shouldThrow) {
          err.message = `${file}: ${err.message}`;
          throw err;
        } else {
          return null;
        }
      }
    }
    async function _writeFile(file, obj, options = {}) {
      const fs2 = options.fs || _fs;
      const str = stringify(obj, options);
      await universalify.fromCallback(fs2.writeFile)(file, str, options);
    }
    var writeFile = universalify.fromPromise(_writeFile);
    function writeFileSync(file, obj, options = {}) {
      const fs2 = options.fs || _fs;
      const str = stringify(obj, options);
      return fs2.writeFileSync(file, str, options);
    }
    var jsonfile = {
      readFile: readFile2,
      readFileSync,
      writeFile,
      writeFileSync
    };
    module.exports = jsonfile;
  }
});

// ../../node_modules/fs-extra/lib/json/jsonfile.js
var require_jsonfile2 = __commonJS({
  "../../node_modules/fs-extra/lib/json/jsonfile.js"(exports, module) {
    "use strict";
    var jsonFile = require_jsonfile();
    module.exports = {
      // jsonfile exports
      readJson: jsonFile.readFile,
      readJsonSync: jsonFile.readFileSync,
      writeJson: jsonFile.writeFile,
      writeJsonSync: jsonFile.writeFileSync
    };
  }
});

// ../../node_modules/fs-extra/lib/output-file/index.js
var require_output_file = __commonJS({
  "../../node_modules/fs-extra/lib/output-file/index.js"(exports, module) {
    "use strict";
    var u = require_universalify().fromPromise;
    var fs2 = require_fs();
    var path2 = __require("path");
    var mkdir = require_mkdirs();
    var pathExists = require_path_exists().pathExists;
    async function outputFile(file, data, encoding = "utf-8") {
      const dir = path2.dirname(file);
      if (!await pathExists(dir)) {
        await mkdir.mkdirs(dir);
      }
      return fs2.writeFile(file, data, encoding);
    }
    function outputFileSync(file, ...args) {
      const dir = path2.dirname(file);
      if (!fs2.existsSync(dir)) {
        mkdir.mkdirsSync(dir);
      }
      fs2.writeFileSync(file, ...args);
    }
    module.exports = {
      outputFile: u(outputFile),
      outputFileSync
    };
  }
});

// ../../node_modules/fs-extra/lib/json/output-json.js
var require_output_json = __commonJS({
  "../../node_modules/fs-extra/lib/json/output-json.js"(exports, module) {
    "use strict";
    var { stringify } = require_utils2();
    var { outputFile } = require_output_file();
    async function outputJson(file, data, options = {}) {
      const str = stringify(data, options);
      await outputFile(file, str, options);
    }
    module.exports = outputJson;
  }
});

// ../../node_modules/fs-extra/lib/json/output-json-sync.js
var require_output_json_sync = __commonJS({
  "../../node_modules/fs-extra/lib/json/output-json-sync.js"(exports, module) {
    "use strict";
    var { stringify } = require_utils2();
    var { outputFileSync } = require_output_file();
    function outputJsonSync(file, data, options) {
      const str = stringify(data, options);
      outputFileSync(file, str, options);
    }
    module.exports = outputJsonSync;
  }
});

// ../../node_modules/fs-extra/lib/json/index.js
var require_json = __commonJS({
  "../../node_modules/fs-extra/lib/json/index.js"(exports, module) {
    "use strict";
    var u = require_universalify().fromPromise;
    var jsonFile = require_jsonfile2();
    jsonFile.outputJson = u(require_output_json());
    jsonFile.outputJsonSync = require_output_json_sync();
    jsonFile.outputJSON = jsonFile.outputJson;
    jsonFile.outputJSONSync = jsonFile.outputJsonSync;
    jsonFile.writeJSON = jsonFile.writeJson;
    jsonFile.writeJSONSync = jsonFile.writeJsonSync;
    jsonFile.readJSON = jsonFile.readJson;
    jsonFile.readJSONSync = jsonFile.readJsonSync;
    module.exports = jsonFile;
  }
});

// ../../node_modules/fs-extra/lib/move/move.js
var require_move = __commonJS({
  "../../node_modules/fs-extra/lib/move/move.js"(exports, module) {
    "use strict";
    var fs2 = require_fs();
    var path2 = __require("path");
    var { copy } = require_copy2();
    var { remove } = require_remove();
    var { mkdirp } = require_mkdirs();
    var { pathExists } = require_path_exists();
    var stat = require_stat();
    async function move(src, dest, opts = {}) {
      const overwrite = opts.overwrite || opts.clobber || false;
      const { srcStat, isChangingCase = false } = await stat.checkPaths(src, dest, "move", opts);
      await stat.checkParentPaths(src, srcStat, dest, "move");
      const destParent = path2.dirname(dest);
      const parsedParentPath = path2.parse(destParent);
      if (parsedParentPath.root !== destParent) {
        await mkdirp(destParent);
      }
      return doRename(src, dest, overwrite, isChangingCase);
    }
    async function doRename(src, dest, overwrite, isChangingCase) {
      if (!isChangingCase) {
        if (overwrite) {
          await remove(dest);
        } else if (await pathExists(dest)) {
          throw new Error("dest already exists.");
        }
      }
      try {
        await fs2.rename(src, dest);
      } catch (err) {
        if (err.code !== "EXDEV") {
          throw err;
        }
        await moveAcrossDevice(src, dest, overwrite);
      }
    }
    async function moveAcrossDevice(src, dest, overwrite) {
      const opts = {
        overwrite,
        errorOnExist: true,
        preserveTimestamps: true
      };
      await copy(src, dest, opts);
      return remove(src);
    }
    module.exports = move;
  }
});

// ../../node_modules/fs-extra/lib/move/move-sync.js
var require_move_sync = __commonJS({
  "../../node_modules/fs-extra/lib/move/move-sync.js"(exports, module) {
    "use strict";
    var fs2 = require_graceful_fs();
    var path2 = __require("path");
    var copySync = require_copy2().copySync;
    var removeSync = require_remove().removeSync;
    var mkdirpSync = require_mkdirs().mkdirpSync;
    var stat = require_stat();
    function moveSync(src, dest, opts) {
      opts = opts || {};
      const overwrite = opts.overwrite || opts.clobber || false;
      const { srcStat, isChangingCase = false } = stat.checkPathsSync(src, dest, "move", opts);
      stat.checkParentPathsSync(src, srcStat, dest, "move");
      if (!isParentRoot(dest)) mkdirpSync(path2.dirname(dest));
      return doRename(src, dest, overwrite, isChangingCase);
    }
    function isParentRoot(dest) {
      const parent = path2.dirname(dest);
      const parsedPath = path2.parse(parent);
      return parsedPath.root === parent;
    }
    function doRename(src, dest, overwrite, isChangingCase) {
      if (isChangingCase) return rename(src, dest, overwrite);
      if (overwrite) {
        removeSync(dest);
        return rename(src, dest, overwrite);
      }
      if (fs2.existsSync(dest)) throw new Error("dest already exists.");
      return rename(src, dest, overwrite);
    }
    function rename(src, dest, overwrite) {
      try {
        fs2.renameSync(src, dest);
      } catch (err) {
        if (err.code !== "EXDEV") throw err;
        return moveAcrossDevice(src, dest, overwrite);
      }
    }
    function moveAcrossDevice(src, dest, overwrite) {
      const opts = {
        overwrite,
        errorOnExist: true,
        preserveTimestamps: true
      };
      copySync(src, dest, opts);
      return removeSync(src);
    }
    module.exports = moveSync;
  }
});

// ../../node_modules/fs-extra/lib/move/index.js
var require_move2 = __commonJS({
  "../../node_modules/fs-extra/lib/move/index.js"(exports, module) {
    "use strict";
    var u = require_universalify().fromPromise;
    module.exports = {
      move: u(require_move()),
      moveSync: require_move_sync()
    };
  }
});

// ../../node_modules/fs-extra/lib/index.js
var require_lib = __commonJS({
  "../../node_modules/fs-extra/lib/index.js"(exports, module) {
    "use strict";
    module.exports = {
      // Export promiseified graceful-fs:
      ...require_fs(),
      // Export extra methods:
      ...require_copy2(),
      ...require_empty(),
      ...require_ensure(),
      ...require_json(),
      ...require_mkdirs(),
      ...require_move2(),
      ...require_output_file(),
      ...require_path_exists(),
      ...require_remove()
    };
  }
});

// src/providers/ethereum/txs.ts
import {
  elizaLogger,
  embed
} from "@elizaos/core";
var relativeTxsCount = 100;
var EthTxsProvider = class {
  constructor(blockchainDataTableName) {
    this.blockchainDataTableName = blockchainDataTableName;
  }
  async get(runtime, message, state) {
    const blockchainDataRuntimeManager = runtime.getMemoryManager(this.blockchainDataTableName);
    elizaLogger.log("Retrieving onchain eth txs from blockchain data runtime manager...");
    const embedding = await embed(runtime, message.content.text);
    const memories = await blockchainDataRuntimeManager.searchMemoriesByEmbedding(
      embedding,
      {
        roomId: message.agentId,
        count: relativeTxsCount,
        match_threshold: 0.1
      }
    );
    return concatenateMemories(memories);
  }
};
function concatenateMemories(memories) {
  const prefix = "A list of relevant on-chain ethereum txs content: ";
  const concatenatedContent = memories.map((memory) => memory.content.text).join(" ");
  return prefix + concatenatedContent;
}

// src/evaluators/data_evaluator.ts
import {
  elizaLogger as elizaLogger2
} from "@elizaos/core";
var dataEvaluator = {
  alwaysRun: false,
  name: "GET_ONCHAIN_DATA",
  similes: ["GET_ONCHAIN_INFO"],
  description: "Evaluates for onchain data",
  validate: async (runtime, message) => true,
  handler: async (runtime, message) => {
    elizaLogger2.log("GET_ONCHAIN_DATA evaluator handler called");
    return true;
  },
  examples: []
};

// src/data_service.ts
var fs = __toESM(require_lib());
import { Service, MemoryManager } from "@elizaos/core";
import { stringToUuid } from "@elizaos/core";
import * as path from "path";
var BLOCKCHAIN_DATA_TABLE_NAME = "d.a.t.a-blockchain-data";
var DataService = class extends Service {
  static serviceType = ServiceType.INTIFACE;
  runtime = null;
  blockchainDataManager = null;
  async initialize(runtime) {
    console.log("Initializing carv data service");
    this.runtime = runtime;
    this.blockchainDataManager = new MemoryManager({
      runtime,
      tableName: BLOCKCHAIN_DATA_TABLE_NAME
    });
    this.runtime.registerMemoryManager(this.blockchainDataManager);
    console.log("Reading transactions from file...", path.join(__dirname, "txs.json"));
    const txs = await readJsonFile(path.join(__dirname, "txs.json"));
    for (const tx of txs) {
      console.log("Adding transaction to memory:", tx);
      const txInfo = formatTransaction(tx);
      const memory = {
        id: stringToUuid(txInfo),
        content: {
          text: txInfo
        },
        roomId: runtime.agentId,
        agentId: runtime.agentId,
        userId: runtime.agentId,
        createdAt: Date.now()
      };
      await this.blockchainDataManager.addEmbeddingToMemory(memory);
      console.log("got memory:!!!!!");
      await this.blockchainDataManager.createMemory(memory);
    }
  }
  getBlockchainDataTableName() {
    return BLOCKCHAIN_DATA_TABLE_NAME;
  }
};
async function readJsonFile(filePath) {
  try {
    const fileContent = await fs.readFile(filePath, "utf-8");
    const jsonArray = JSON.parse(fileContent);
    if (!Array.isArray(jsonArray)) {
      throw new Error("The file content is not a list of JSON objects.");
    }
    return jsonArray;
  } catch (error) {
    console.error("Error reading or parsing the file:", error);
    throw error;
  }
}
function formatTransaction(transaction) {
  return `Transaction of type ${transaction.type} from ${transaction.from} to ${transaction.to} with value ${transaction.value}. Transaction hash: ${transaction.txHash}, nonce: ${transaction.nonce}, block hash: ${transaction.blockHash}, block number: ${transaction.blockNum}.`;
}

// src/index.ts
var onchainDataPlugin = {
  name: "onchain data plugin",
  description: "Enables onchain data fetching",
  actions: [],
  providers: [new EthTxsProvider(BLOCKCHAIN_DATA_TABLE_NAME)],
  evaluators: [dataEvaluator],
  // separate examples will be added for services and clients
  services: [new DataService()],
  clients: []
};

```

`/Users/tuyukai/go/src/github.com/carv-protocol/d.a.t.a/eliza/plugin-d.a.t.a/dist/index.d.ts`:

```ts
import { Plugin } from '@elizaos/core';

declare const onchainDataPlugin: Plugin;

export { onchainDataPlugin };

```

`/Users/tuyukai/go/src/github.com/carv-protocol/d.a.t.a/eliza/plugin-d.a.t.a/package.json`:

```json
{
    "name": "@elizaos/plugin-d.a.t.a",
    "version": "0.0.1",
    "types": "dist/index.d.ts",
    "description": "D.A.T.A framework, provide on-chain/off-chain knowledge to AGI",
    "main": "dist/index.js",
    "scripts": {
        "build": "tsup src/index.ts --format cjs --dts",
        "dev": "tsup src/index.ts --format cjs --dts --watch",
        "lint": "eslint --fix  --cache .",
        "test": "vitest run",
        "test:watch": "vitest",
        "test:coverage": "vitest run --coverage"
    },
    "dependencies": {
        "@elizaos/core": "workspace:*"
    },
    "repository": {
        "type": "git",
        "url": "git+https://github.com/carv-protocol/eliza-d.a.t.a.git"
    },
    "keywords": [
        "data",
        "AI",
        "Agent",
        "AGI",
        "plugin",
        "CARV"
    ],
    "author": "CARV",
    "license": "ISC",
    "bugs": {
        "url": "https://github.com/carv-protocol/eliza-d.a.t.a/issues"
    },
    "homepage": "https://github.com/carv-protocol/eliza-d.a.t.a#readme"
}

```

`/Users/tuyukai/go/src/github.com/carv-protocol/d.a.t.a/eliza/plugin-d.a.t.a/tsconfig.json`:

```json
{
    "extends": "../core/tsconfig.json",
    "compilerOptions": {
        "outDir": "dist",
        "rootDir": "src",
        "types": [
            "node"
        ]
    },
    "include": [
        "src"
    ]
}
```

`/Users/tuyukai/go/src/github.com/carv-protocol/d.a.t.a/eliza/plugin-d.a.t.a/vitest.config.ts`:

```ts
import { defineConfig } from "vitest/config";
import path from "path";

export default defineConfig({
    test: {
        environment: "node",
        testTimeout: 120000,
        coverage: {
            reporter: ["text", "json", "html"],
            exclude: ["node_modules/", "dist/"],
        },
    },
    resolve: {
        alias: {
            "@": path.resolve(__dirname, "./src"),
        },
    },
});

```

`/Users/tuyukai/go/src/github.com/carv-protocol/d.a.t.a/eliza/plugin-d.a.t.a/src/providers/ethereum/sequelize.ts`:

```ts
import {
    Provider,
    IAgentRuntime,
    Memory,
    State,
    elizaLogger,
} from "@elizaos/core";

export class TxProvider {
    chain: string;
    constructor(chain: string) {
        this.chain = chain;
    }

    getSequelizeUrl(): string {
        return `https://${this.chain}.sequelize.com`;
    }

    getDataSchema(): string {
        return `
        Ethereum Transaction Data Schema:
        - hash: string (unique transaction hash)
        - blockNumber: number (block containing the transaction)
        - timestamp: datetime (transaction timestamp)
        - from: string (sender address)
        - to: string (recipient address)
        - value: string (transaction amount in wei)
        - gasUsed: string (gas consumed)
        - status: boolean (transaction success)

        Common Query Patterns:
        1. Get transactions by address:
           - Filter by from/to address
           - Time range optional
           - Pagination support

        2. Get transactions by block:
           - Filter by block number/range
           - Support for latest blocks

        3. Get transaction details:
           - Lookup by transaction hash
           - Returns full transaction data

        Query Parameters:
        - address: Ethereum address (0x...)
        - startBlock: number
        - endBlock: number
        - startTime: ISO datetime
        - endTime: ISO datetime
        - limit: number (default 100)
        - offset: number (default 0)

        Response Format:
        {
          transactions: [{
            hash: string,
            blockNumber: number,
            timestamp: string,
            from: string,
            to: string,
            value: string,
            gasUsed: string,
            status: boolean
          }],
          pagination: {
            total: number,
            limit: number,
            offset: number
          }
        }

        Error Handling:
        - Invalid address format
        - Block number out of range
        - Invalid time range
        - Rate limiting considerations
        `;
    }
}

export const txsSequelizeProvider = (runtime: IAgentRuntime) => {
    const chain = "ethereum-mainnet";
    return new TxProvider(chain);
};

export const sequelizeProvider: Provider = {
    get: async (
        runtime: IAgentRuntime,
        message: Memory,
        state: State
    ): Promise<string | null> => {
        elizaLogger.log("==== pis Retrieving from d.a.t.a provider...");
        try {
            const txsProvider = txsSequelizeProvider(runtime);
            const url = txsProvider.getSequelizeUrl();
            const schema = txsProvider.getDataSchema();

            elizaLogger.log("==== pis url: ", url);

            // Return context information for AI to understand the data structure and query patterns
            return `
            Ethereum Mainnet Transaction Database Context:

            Database URL: ${url}

            ${schema}

            Usage Notes:
            1. Always validate input parameters before querying
            2. Consider using pagination for large result sets
            3. Include error handling for failed queries
            4. Cache frequently accessed data when possible
            5. Monitor rate limits and query performance

            This database provides access to all Ethereum mainnet transactions with standardized query patterns and response formats.
            `;
        } catch (error) {
            elizaLogger.error("Error in d.a.t.a provider:", error);
            return null;
        }
    },
};

```

`/Users/tuyukai/go/src/github.com/carv-protocol/d.a.t.a/eliza/plugin-d.a.t.a/src/providers/ethereum/block.ts`:

```ts
import {
  Provider,
  IAgentRuntime,
  Memory,
  State,
  elizaLogger,
  embed,
} from "@elizaos/core";

const relativeTxsCount = 100;

export class EthTxsProvider implements Provider {
constructor(private blockchainDataTableName: string) {}
async get(runtime: IAgentRuntime, message: Memory, state: State): Promise<string | null> {
      // Data retrieval logic for the provider
      const blockchainDataRuntimeManager = runtime.getMemoryManager(this.blockchainDataTableName);
      elizaLogger.log("Retrieving onchain eth txs from blockchain data runtime manager...");

      const embedding = await embed(runtime, message.content.text);
      const memories = await blockchainDataRuntimeManager.searchMemoriesByEmbedding(
        embedding,
        {
            roomId: message.agentId,
            count: relativeTxsCount,
            match_threshold: 0.1,
        }
      );
      return concatenateMemories(memories);
  }
};

function concatenateMemories(memories: Memory[]): string {
  const prefix = "A list of relevant on-chain ethereum txs content: ";
  const concatenatedContent = memories.map(memory => memory.content.text).join(' ');
  return prefix + concatenatedContent;
}
```

`/Users/tuyukai/go/src/github.com/carv-protocol/d.a.t.a/eliza/plugin-d.a.t.a/src/providers/ethereum/txs.ts`:

```ts
import {
  Provider,
  IAgentRuntime,
  Memory,
  State,
  elizaLogger,
  embed,
} from "@elizaos/core";

const relativeTxsCount = 100;

export class EthTxsProvider implements Provider {
constructor(private blockchainDataTableName: string) {}
async get(runtime: IAgentRuntime, message: Memory, state: State): Promise<string | null> {
      // Data retrieval logic for the provider
      const blockchainDataRuntimeManager = runtime.getMemoryManager(this.blockchainDataTableName);
      elizaLogger.log("Retrieving onchain eth txs from blockchain data runtime manager...");

      const embedding = await embed(runtime, message.content.text);
      const memories = await blockchainDataRuntimeManager.searchMemoriesByEmbedding(
        embedding,
        {
            roomId: message.agentId,
            count: relativeTxsCount,
            match_threshold: 0.1,
        }
      );
      return concatenateMemories(memories);
  }
};

function concatenateMemories(memories: Memory[]): string {
  const prefix = "A list of relevant on-chain ethereum txs content: ";
  const concatenatedContent = memories.map(memory => memory.content.text).join(' ');
  return prefix + concatenatedContent;
}
```

`/Users/tuyukai/go/src/github.com/carv-protocol/d.a.t.a/eliza/plugin-d.a.t.a/src/providers/ethereum/database.ts`:

```ts
import {
    Provider,
    IAgentRuntime,
    Memory,
    State,
    elizaLogger,
    generateMessageResponse,
    ModelClass,
    stringToUuid,
    getEmbeddingZeroVector,
} from "@elizaos/core";

// API response interface for query results
interface IQueryResult {
    success: boolean;
    data: any[];
    metadata: {
        total: number;
        queryTime: string;
        queryType: "transaction" | "token" | "aggregate" | "unknown";
        executionTime: number;
        cached: boolean;
        pagination?: {
            currentPage: number;
            totalPages: number;
            hasMore: boolean;
        };
    };
    error?: {
        code: string;
        message: string;
        details?: any;
    };
}
// API response interface
interface IApiResponse {
    code: number;
    msg: string;
    data: {
        column_infos: string[];
        rows: {
            items: (string | number)[];
        }[];
    };
}

export class DatabaseProvider {
    private chain: string;
    private readonly API_URL: string;
    private readonly AUTH_TOKEN: string;

    constructor(chain: string, runtime: IAgentRuntime) {
        this.chain = chain;
        this.API_URL = runtime.getSetting("DATA_API_KEY");
        this.AUTH_TOKEN = runtime.getSetting("DATA_AUTH_TOKEN");
    }

    public extractSQLQuery(preResponse: any): string | null {
        try {
            // Try to parse if input is string
            let jsonData = preResponse;
            if (typeof preResponse === "string") {
                try {
                    jsonData = JSON.parse(preResponse);
                } catch (e) {
                    elizaLogger.error(
                        "Failed to parse preResponse as JSON:",
                        e
                    );
                    return null;
                }
            }

            // Function to recursively search for SQL query in object
            const findSQLQuery = (obj: any): string | null => {
                // Base cases
                if (!obj) return null;

                // If string, check if it's a SQL query
                if (typeof obj === "string") {
                    const sqlPattern = /^\s*(SELECT|WITH)\s+[\s\S]+?(?:;|$)/i;
                    const commentPattern = /--.*$|\/\*[\s\S]*?\*\//gm;

                    // Clean and check the string
                    const cleanStr = obj.replace(commentPattern, "").trim();
                    if (sqlPattern.test(cleanStr)) {
                        // Validate SQL safety
                        const unsafeKeywords = [
                            "drop",
                            "delete",
                            "update",
                            "insert",
                            "alter",
                            "create",
                        ];
                        const isUnsafe = unsafeKeywords.some((keyword) =>
                            cleanStr.toLowerCase().includes(keyword)
                        );

                        if (!isUnsafe) {
                            return cleanStr;
                        }
                    }
                    return null;
                }

                // If array, search each element
                if (Array.isArray(obj)) {
                    for (const item of obj) {
                        const result = findSQLQuery(item);
                        if (result) return result;
                    }
                    return null;
                }

                // If object, search each value
                if (typeof obj === "object") {
                    for (const key of Object.keys(obj)) {
                        // Prioritize 'query' field in sql object
                        if (key.toLowerCase() === "query" && obj.sql) {
                            const result = findSQLQuery(obj[key]);
                            if (result) return result;
                        }
                    }

                    // Search other fields
                    for (const key of Object.keys(obj)) {
                        const result = findSQLQuery(obj[key]);
                        if (result) return result;
                    }
                }

                return null;
            };

            // Start the search
            const sqlQuery = findSQLQuery(jsonData);

            if (!sqlQuery) {
                elizaLogger.warn("No valid SQL query found in preResponse");
                return null;
            }
            return sqlQuery;
        } catch (error) {
            elizaLogger.error("Error in extractSQLQuery:", error);
            return null;
        }
    }

    private async sendSqlQuery(sql: string): Promise<IApiResponse> {
        try {
            const response = await fetch(this.API_URL, {
                method: "POST",
                headers: {
                    "Content-Type": "application/json",
                    Authorization: this.AUTH_TOKEN,
                },
                body: JSON.stringify({
                    sql_content: sql,
                }),
            });

            if (!response.ok) {
                throw new Error(`HTTP error! status: ${response.status}`);
            }

            const data = await response.json();
            return data as IApiResponse;
        } catch (error) {
            elizaLogger.error("Error sending SQL query to API:", error);
            throw error;
        }
    }

    // Transform API response data
    private transformApiResponse(apiResponse: IApiResponse): any[] {
        const { column_infos, rows } = apiResponse.data;

        return rows.map((row) => {
            const rowData: Record<string, any> = {};
            row.items.forEach((value, index) => {
                const columnName = column_infos[index];
                rowData[columnName] = value;
            });
            return rowData;
        });
    }

    // Execute query
    private async executeQuery(sql: string): Promise<IQueryResult> {
        try {
            // Validate query
            if (!sql || sql.length > 5000) {
                throw new Error("Invalid SQL query length");
            }

            const queryType = sql.toLowerCase().includes("token_transfers")
                ? "token"
                : sql.toLowerCase().includes("count")
                    ? "aggregate"
                    : "transaction";

            // Send query to API
            const apiResponse = await this.sendSqlQuery(sql);

            // Check API response status
            if (apiResponse.code !== 0) {
                throw new Error(`API Error: ${apiResponse.msg}`);
            }

            // Transform data
            const transformedData = this.transformApiResponse(apiResponse);

            const queryResult: IQueryResult = {
                success: true,
                data: transformedData,
                metadata: {
                    total: transformedData.length,
                    queryTime: new Date().toISOString(),
                    queryType: queryType as
                        | "token"
                        | "aggregate"
                        | "transaction",
                    executionTime: 0,
                    cached: false,
                },
            };

            return queryResult;
        } catch (error) {
            elizaLogger.error("Query execution failed:", error);
            return {
                success: false,
                data: [],
                metadata: {
                    total: 0,
                    queryTime: new Date().toISOString(),
                    queryType: "unknown",
                    executionTime: 0,
                    cached: false,
                },
                error: {
                    code: error.code || "EXECUTION_ERROR",
                    message: error.message || "Unknown error occurred",
                    details: error,
                },
            };
        }
    }

    public async query(sql: string): Promise<IQueryResult> {
        return this.executeQuery(sql);
    }

    getDatabaseSchema(): string {
        return `
        CREATE EXTERNAL TABLE transactions(
            hash string,
            nonce bigint,
            block_hash string,
            block_number bigint,
            block_timestamp timestamp,
            date string,
            transaction_index bigint,
            from_address string,
            to_address string,
            value double,
            gas bigint,
            gas_price bigint,
            input string,
            max_fee_per_gas bigint,
            max_priority_fee_per_gas bigint,
            transaction_type bigint
        ) PARTITIONED BY (date string)
        ROW FORMAT SERDE 'org.apache.hadoop.hive.ql.io.parquet.serde.ParquetHiveSerDe'
        STORED AS INPUTFORMAT 'org.apache.hadoop.hive.ql.io.parquet.MapredParquetInputFormat'
        OUTPUTFORMAT 'org.apache.hadoop.hive.ql.io.parquet.MapredParquetOutputFormat';

        CREATE EXTERNAL TABLE token_transfers(
            token_address string,
            from_address string,
            to_address string,
            value double,
            transaction_hash string,
            log_index bigint,
            block_timestamp timestamp,
            date string,
            block_number bigint,
            block_hash string
        ) PARTITIONED BY (date string)
        ROW FORMAT SERDE 'org.apache.hadoop.hive.ql.io.parquet.serde.ParquetHiveSerDe'
        STORED AS INPUTFORMAT 'org.apache.hadoop.hive.ql.io.parquet.MapredParquetInputFormat'
        OUTPUTFORMAT 'org.apache.hadoop.hive.ql.io.parquet.MapredParquetOutputFormat';
        `;
    }

    getQueryExamples(): string {
        return `
        Common Query Examples:

        1. Find Most Active Addresses in Last 7 Days:
        WITH address_activity AS (
            SELECT
                from_address AS address,
                COUNT(*) AS tx_count
            FROM
                eth.transactions
            WHERE date_parse(date, '%Y-%m-%d') >= date_add('day', -7, current_date)
            GROUP BY
                from_address
            UNION ALL
            SELECT
                to_address AS address,
                COUNT(*) AS tx_count
            FROM
                eth.transactions
            WHERE
                date_parse(date, '%Y-%m-%d') >= date_add('day', -7, current_date)
            GROUP BY
                to_address
        )
        SELECT
            address,
            SUM(tx_count) AS total_transactions
        FROM
            address_activity
        GROUP BY
            address
        ORDER BY
            total_transactions DESC
        LIMIT 10;

        2. Analyze Address Transaction Statistics (Last 30 Days):
        WITH recent_transactions AS (
            SELECT
                from_address,
                to_address,
                value,
                block_timestamp,
                CASE
                    WHEN from_address = :address THEN 'outgoing'
                    WHEN to_address = :address THEN 'incoming'
                    ELSE 'other'
                END AS transaction_type
            FROM eth.transactions
            WHERE date >= date_format(date_add('day', -30, current_date), '%Y-%m-%d')
                AND (from_address = :address OR to_address = :address)
        )
        SELECT
            transaction_type,
            COUNT(*) AS transaction_count,
            SUM(CASE WHEN transaction_type = 'outgoing' THEN value ELSE 0 END) AS total_outgoing_value,
            SUM(CASE WHEN transaction_type = 'incoming' THEN value ELSE 0 END) AS total_incoming_value
        FROM recent_transactions
        GROUP BY transaction_type;

        3. Token Transfer Analysis:
        WITH filtered_transactions AS (
            SELECT
                token_address,
                from_address,
                to_address,
                value,
                block_timestamp
            FROM eth.token_transfers
            WHERE token_address = :token_address
                AND date >= :start_date
        )
        SELECT
            COUNT(*) AS transaction_count,
            SUM(value) AS total_transaction_value,
            MAX(value) AS max_transaction_value,
            MIN(value) AS min_transaction_value,
            MAX_BY(from_address, value) AS max_value_from_address,
            MAX_BY(to_address, value) AS max_value_to_address,
            MIN_BY(from_address, value) AS min_value_from_address,
            MIN_BY(to_address, value) AS min_value_to_address
        FROM filtered_transactions;

        Note: Replace :address, :token_address, and :start_date with actual values when querying.
        `;
    }

    getQueryTemplate(): string {
        return `
        # Database Schema
        {{databaseSchema}}

        # Query Examples
        {{queryExamples}}

        # User's Query
        {{userQuery}}

        # Query Guidelines:
        1. Time Range Requirements:
           - ALWAYS include time range limitations in queries
           - Default to last 3 months if no specific time range is mentioned
           - Use date_parse(date, '%Y-%m-%d') >= date_add('month', -3, current_date) for default time range
           - Adjust time range based on user's specific requirements

        2. Query Optimization:
           - Include appropriate LIMIT clauses
           - Use proper indexing columns (date, address, block_number)
           - Consider partitioning by date
           - Add WHERE clauses for efficient filtering

        3. Response Format Requirements:
           You MUST respond in the following JSON format:
           {
             "sql": {
               "query": "your SQL query string",
               "explanation": "brief explanation of the query",
               "timeRange": "specified time range in the query"
             },
             "analysis": {
               "overview": {
                 "totalTransactions": "number",
                 "timeSpan": "time period covered",
                 "keyMetrics": ["list of important metrics"]
               },
               "patterns": {
                 "transactionPatterns": ["identified patterns"],
                 "addressBehavior": ["address analysis"],
                 "temporalTrends": ["time-based trends"]
               },
               "statistics": {
                 "averages": {},
                 "distributions": {},
                 "anomalies": []
               },
               "insights": ["key insights from the data"],
               "recommendations": ["suggested actions or areas for further investigation"]
             }
           }

        4. Analysis Requirements:
           - Focus on recent data patterns
           - Identify trends and anomalies
           - Provide statistical analysis
           - Include risk assessment
           - Suggest further investigations

        Example Response:
        {
          "sql": {
            "query": "WITH recent_txs AS (SELECT * FROM eth.transactions WHERE date_parse(date, '%Y-%m-%d') >= date_add('month', -3, current_date))...",
            "explanation": "Query fetches last 3 months of transactions with aggregated metrics",
            "timeRange": "Last 3 months"
          },
          "analysis": {
            "overview": {
              "totalTransactions": 1000000,
              "timeSpan": "2024-01-01 to 2024-03-12",
              "keyMetrics": ["Average daily transactions: 11000", "Peak day: 2024-02-15"]
            },
            "patterns": {
              "transactionPatterns": ["High volume during Asian trading hours", "Weekend dips in activity"],
              "addressBehavior": ["5 addresses responsible for 30% of volume", "Increasing DEX activity"],
              "temporalTrends": ["Growing transaction volume", "Decreasing gas costs"]
            },
            "statistics": {
              "averages": {
                "dailyTransactions": 11000,
                "gasPrice": "25 gwei"
              },
              "distributions": {
                "valueRanges": ["0-1 ETH: 60%", "1-10 ETH: 30%", ">10 ETH: 10%"]
              },
              "anomalies": ["Unusual spike in gas prices on 2024-02-01"]
            },
            "insights": [
              "Growing DeFi activity indicated by smart contract interactions",
              "Whale addresses showing increased accumulation"
            ],
            "recommendations": [
              "Monitor growing gas usage trend",
              "Track new active addresses for potential market signals"
            ]
          }
        }
        `;
    }

    getAnalysisInstruction(): string {
        return `
            1. Data Overview:
                - Analyze the overall pattern in the query results
                - Identify key metrics and their significance
                - Note any unusual or interesting patterns

            2. Transaction Analysis:
                - Examine transaction values and their distribution
                - Analyze gas usage patterns
                - Evaluate transaction frequency and timing
                - Identify significant transactions or patterns

            3. Address Behavior:
                - Analyze address interactions
                - Identify frequent participants
                - Evaluate transaction patterns for specific addresses
                - Note any suspicious or interesting behavior

            4. Temporal Patterns:
                - Analyze time-based patterns
                - Identify peak activity periods
                - Note any temporal anomalies
                - Consider seasonal or cyclical patterns

            5. Token Analysis (if applicable):
                - Examine token transfer patterns
                - Analyze token holder behavior
                - Evaluate token concentration
                - Note significant token movements

            6. Statistical Insights:
                - Provide relevant statistical measures
                - Compare with typical blockchain metrics
                - Highlight significant deviations
                - Consider historical context

            7. Risk Assessment:
                - Identify potential suspicious activities
                - Note any unusual patterns
                - Flag potential security concerns
                - Consider regulatory implications

            Please provide a comprehensive analysis of the Ethereum blockchain data based on these ethereum information.
            Focus on significant patterns, anomalies, and insights that would be valuable for understanding the blockchain activity.
            Use technical blockchain terminology and provide specific examples from the data to support your analysis.

            Note: This analysis is based on simulated data for demonstration purposes.
        `;
    }
}

export const databaseProvider = (runtime: IAgentRuntime) => {
    const chain = "ethereum-mainnet";
    return new DatabaseProvider(chain, runtime);
};

export const ethereumDataProvider: Provider = {
    get: async (
        runtime: IAgentRuntime,
        message: Memory,
        state: State
    ): Promise<string | null> => {
        try {
            const provider = databaseProvider(runtime);
            const schema = provider.getDatabaseSchema();
            const examples = provider.getQueryExamples();
            const template = provider.getQueryTemplate();

            if (!state) {
                state = (await runtime.composeState(message)) as State;
            } else {
                state = await runtime.updateRecentMessageState(state);
            }

            const buildContext = template
                .replace("{{databaseSchema}}", schema)
                .replace("{{queryExamples}}", examples)
                .replace("{{userQuery}}", message.content.text || "");

            const context = JSON.stringify({
                user: runtime.agentId,
                content: buildContext,
                action: "NONE",
            });

            const preResponse = await generateMessageResponse({
                runtime: runtime,
                context: context,
                modelClass: ModelClass.LARGE,
            });

            const userMessage = {
                agentId: runtime.agentId,
                roomId: message.roomId,
                userId: message.userId,
                content: message.content,
            };

            // Save response to memory
            const preResponseMessage: Memory = {
                id: stringToUuid(message.id + "-" + runtime.agentId),
                ...userMessage,
                userId: runtime.agentId,
                content: preResponse,
                embedding: getEmbeddingZeroVector(),
                createdAt: Date.now(),
            };

            await runtime.messageManager.createMemory(preResponseMessage);
            await runtime.updateRecentMessageState(state);

            // Check for SQL query in the response using class method
            const sqlQuery = provider.extractSQLQuery(preResponse);
            if (sqlQuery) {
                elizaLogger.log("%%%% D.A.T.A. Generated SQL query:", sqlQuery);
                const analysisInstruction = provider.getAnalysisInstruction();
                try {
                    // Call query method on provider
                    const queryResult = await provider.query(sqlQuery);

                    elizaLogger.log("%%%% D.A.T.A. queryResult", queryResult);
                    // Return combined context with query results and analysis instructions
                    return `
                    # query by user
                    ${message.content.text}

                    # query result
                    ${JSON.stringify(queryResult, null, 2)}

                    # Analysis Instructions
                    ${analysisInstruction}
                    `;
                } catch (error) {
                    elizaLogger.error("Error executing query:", error);
                    return context;
                }
            } else {
                elizaLogger.log("no sql query found in user message");
            }
            return context;
        } catch (error) {
            elizaLogger.error("Error in ethereum data provider:", error);
            return null;
        }
    },
};

`/Users/tuyukai/go/src/github.com/carv-protocol/d.a.t.a/eliza/plugin-d.a.t.a/src/evaluators/data_evaluator.ts`:

```ts
import {
  Evaluator,
  IAgentRuntime,
  Memory,
  elizaLogger,
} from "@elizaos/core";

export const dataEvaluator: Evaluator = {
  alwaysRun: false,
  name: "GET_ONCHAIN_DATA",
  similes: ["GET_ONCHAIN_INFO"],
  description: "Evaluates for onchain data",
  validate: async (runtime: IAgentRuntime, message: Memory) => true,
  handler: async (runtime: IAgentRuntime, message: Memory) => {
      // Evaluation logic here
      elizaLogger.log("GET_ONCHAIN_DATA evaluator handler called");
      return true;
  },
  examples: [],
};
```

`/Users/tuyukai/go/src/github.com/carv-protocol/d.a.t.a/eliza/plugin-d.a.t.a/src/actions/fetchTransaction.ts`:

```ts
import {
    Action,
    HandlerCallback,
    IAgentRuntime,
    Memory,
    State,
    elizaLogger,
    composeContext,
    generateObject,
    ModelClass,
} from "@elizaos/core";
import {
    DatabaseProvider,
    databaseProvider,
} from "../providers/ethereum/database";
import { fetchTransactionTemplate } from "../templates";

// Query parameter interface with stricter types
interface FetchTransactionParams {
    address?: string;
    startDate?: string;
    endDate?: string;
    minValue?: string;
    maxValue?: string;
    limit?: number;
    orderBy?: "block_timestamp" | "value" | "gas_price";
    orderDirection?: "ASC" | "DESC";
}

// Response interface with enhanced metadata
interface TransactionQueryResult {
    success: boolean;
    data: any[];
    metadata: {
        total: number;
        queryTime: string;
        queryType: "transaction" | "token" | "aggregate" | "unknown";
        executionTime: number;
        cached: boolean;
        queryDetails?: {
            params: FetchTransactionParams;
            query: string;
            paramValidation?: string[];
        };
    };
    error?: {
        code: string;
        message: string;
        details?: any;
    };
}

export class FetchTransactionAction {
    constructor(private dbProvider: DatabaseProvider) { }

    private validateParams(params: FetchTransactionParams): string[] {
        const validationMessages: string[] = [];

        // Date format validation
        const dateRegex = /^\d{4}-\d{2}-\d{2}$/;
        if (params.startDate && !dateRegex.test(params.startDate)) {
            validationMessages.push(
                `Invalid start date format: ${params.startDate}`
            );
        }
        if (params.endDate && !dateRegex.test(params.endDate)) {
            validationMessages.push(
                `Invalid end date format: ${params.endDate}`
            );
        }

        // Address format validation
        if (params.address && !/^0x[a-fA-F0-9]{40}$/.test(params.address)) {
            validationMessages.push(
                `Invalid address format: ${params.address}`
            );
        }

        // Value validation
        if (params.minValue && isNaN(parseFloat(params.minValue))) {
            validationMessages.push(
                `Invalid minimum value: ${params.minValue}`
            );
        }
        if (params.maxValue && isNaN(parseFloat(params.maxValue))) {
            validationMessages.push(
                `Invalid maximum value: ${params.maxValue}`
            );
        }

        // Limit validation
        if (params.limit) {
            if (isNaN(params.limit)) {
                validationMessages.push(`Invalid limit: must be a number`);
            } else if (params.limit < 1 || params.limit > 100) {
                validationMessages.push(
                    `Invalid limit: ${params.limit}. Must be between 1 and 100`
                );
            }
        }

        // Order validation
        const validOrderBy = ["block_timestamp", "value", "gas_price"];
        if (params.orderBy && !validOrderBy.includes(params.orderBy)) {
            validationMessages.push(
                `Invalid orderBy: ${params.orderBy}. Must be one of: ${validOrderBy.join(
                    ", "
                )}`
            );
        }

        const validOrderDirection = ["ASC", "DESC"];
        if (
            params.orderDirection &&
            !validOrderDirection.includes(params.orderDirection)
        ) {
            validationMessages.push(
                `Invalid orderDirection: ${params.orderDirection
                }. Must be one of: ${validOrderDirection.join(", ")}`
            );
        }

        return validationMessages;
    }

    private buildSqlQuery(params: FetchTransactionParams): string {
        const conditions: string[] = [];

        // Add time range condition
        if (!params.startDate) {
            conditions.push(
                "date_parse(date, '%Y-%m-%d') >= date_add('month', -3, current_date)"
            );
        } else {
            conditions.push(`date >= '${params.startDate}'`);
            if (params.endDate) {
                conditions.push(`date <= '${params.endDate}'`);
            }
        }

        // Add address condition
        if (params.address) {
            conditions.push(
                `(from_address = '${params.address}' OR to_address = '${params.address}')`
            );
        }

        // Add value conditions
        if (params.minValue) {
            // Convert ETH to Wei for comparison
            const minValueWei = (parseFloat(params.minValue) * 1e18).toString();
            conditions.push(`value >= ${minValueWei}`);
        }
        if (params.maxValue) {
            const maxValueWei = (parseFloat(params.maxValue) * 1e18).toString();
            conditions.push(`value <= ${maxValueWei}`);
        }

        // Build the final query
        const query = `
            SELECT
                hash,
                block_number,
                block_timestamp,
                from_address,
                to_address,
                value / 1e18 as value_eth,
                gas,
                gas_price
            FROM eth.transactions
            WHERE ${conditions.join(" AND ")}
            ORDER BY ${params.orderBy || "block_timestamp"} ${params.orderDirection || "DESC"
            }
            LIMIT ${params.limit || 10}
        `;

        return query.trim();
    }

    public async fetchTransactions(
        message: string,
        runtime: IAgentRuntime,
        state: State
    ): Promise<TransactionQueryResult> {
        try {
            // Parse parameters using LLM
            const context = composeContext({
                state,
                template: fetchTransactionTemplate,
            });

            const paramsJson = (await generateObject({
                runtime,
                context,
                modelClass: ModelClass.SMALL,
            })) as FetchTransactionParams;

            // Validate parameters
            const validationMessages = this.validateParams(paramsJson);
            if (validationMessages.length > 0) {
                throw new Error(validationMessages.join("; "));
            }

            // Build and execute query
            const sqlQuery = this.buildSqlQuery(paramsJson);
            elizaLogger.log("Generated SQL query:", sqlQuery);

            const result = (await this.dbProvider.query(
                sqlQuery
            )) as TransactionQueryResult;

            // Enhance result with query details
            if (result.success) {
                result.metadata.queryDetails = {
                    params: paramsJson,
                    query: sqlQuery,
                    paramValidation: validationMessages,
                };
            }

            return result;
        } catch (error) {
            elizaLogger.error("Error fetching transactions:", error);
            return {
                success: false,
                data: [],
                metadata: {
                    total: 0,
                    queryTime: new Date().toISOString(),
                    queryType: "transaction",
                    executionTime: 0,
                    cached: false,
                },
                error: {
                    code: "FETCH_ERROR",
                    message: error.message,
                    details: error,
                },
            };
        }
    }
}

export const fetchTransactionAction: Action = {
    name: "fetch_transactions",
    description: "Fetch Ethereum transactions based on various criteria",
    similes: [
        "get transactions",
        "show transfers",
        "display eth transactions",
        "find transactions",
        "search transfers",
        "check transactions",
        "view transfers",
        "list transactions",
        "recent transactions",
        "transaction history",
    ],
    examples: [
        [
            {
                user: "user",
                content: {
                    text: "Show me the latest 10 Ethereum transactions",
                    action: "FETCH_TRANSACTIONS",
                },
            },
        ],
        [
            {
                user: "user",
                content: {
                    text: "Get transactions for address 0x742d35Cc6634C0532925a3b844Bc454e4438f44e",
                    action: "FETCH_TRANSACTIONS",
                },
            },
        ],
        [
            {
                user: "user",
                content: {
                    text: "Find transactions above 1 ETH from last month",
                    action: "FETCH_TRANSACTIONS",
                },
            },
        ],
    ],
    validate: async (runtime: IAgentRuntime) => {
        const apiKey = runtime.getSetting("DATA_API_KEY");
        const authToken = runtime.getSetting("DATA_AUTH_TOKEN");
        return !!(apiKey && authToken);
    },
    handler: async (
        runtime: IAgentRuntime,
        message: Memory,
        state: State,
        _options: any,
        callback?: HandlerCallback
    ) => {
        try {
            const provider = databaseProvider(runtime);
            const action = new FetchTransactionAction(provider);

            const result = await action.fetchTransactions(
                message.content.text,
                runtime,
                state
            );

            if (callback) {
                if (result.success) {
                    const params = result.metadata.queryDetails?.params;
                    let details = "";
                    if (params) {
                        details = `
- Address: ${params.address || "any"}
- Date Range: ${params.startDate || "last 3 months"} to ${params.endDate || "now"
                            }
- Value Range: ${params.minValue ? `>${params.minValue} ETH` : "any"} ${params.maxValue ? `to <${params.maxValue} ETH` : ""
                            }
- Showing: ${params.limit || 10} transactions
- Ordered by: ${params.orderBy || "timestamp"} ${params.orderDirection || "DESC"
                            }`;
                    }

                    callback({
                        text: `Found ${result.metadata.total
                            } transactions with the following criteria:${details}\n\nHere are the details:`,
                        content: {
                            success: true,
                            data: result.data,
                            metadata: result.metadata,
                        },
                    });
                } else {
                    callback({
                        text: `Error fetching transactions: ${result.error?.message}`,
                        content: { error: result.error },
                    });
                }
            }

            return result.success;
        } catch (error) {
            elizaLogger.error("Error in fetch transaction action:", error);
            if (callback) {
                callback({
                    text: `Error fetching transactions: ${error.message}`,
                    content: { error: error.message },
                });
            }
            return false;
        }
    },
};

export default fetchTransactionAction;

```

`/Users/tuyukai/go/src/github.com/carv-protocol/d.a.t.a/eliza/plugin-d.a.t.a/src/templates/index.ts`:

```ts
export const fetchTransactionTemplate = `Respond with a JSON markdown block containing only the extracted values. Use null for any values that cannot be determined.

Example response:
\`\`\`json
{
    "address": "0x742d35Cc6634C0532925a3b844Bc454e4438f44e",
    "startDate": "2024-01-01",
    "endDate": "2024-03-01",
    "minValue": "1.5",
    "maxValue": null,
    "limit": 10,
    "orderBy": "block_timestamp",
    "orderDirection": "DESC"
}
\`\`\`

{{recentMessages}}

Given the recent messages, extract the following information about the transaction query:
- Wallet address to query (if any)
- Start date (YYYY-MM-DD format)
- End date (YYYY-MM-DD format)
- Minimum value in ETH (if any)
- Maximum value in ETH (if any)
- Number of transactions to return (default 10, max 100)
- Order by field (block_timestamp, value, or gas_price)
- Order direction (ASC or DESC)

Respond with a JSON markdown block containing only the extracted values.`;

```

`/Users/tuyukai/go/src/github.com/carv-protocol/d.a.t.a/eliza/plugin-d.a.t.a/src/index.ts`:

```ts
import { Plugin } from "@elizaos/core";
import { ethereumDataProvider } from "./providers/ethereum/database";

export const onchainDataPlugin: Plugin = {
    name: "onchain data plugin",
    description: "Enables onchain data fetching",
    actions: [],
    providers: [ethereumDataProvider],
    evaluators: [],
    services: [],
    clients: [],
};

```

`/Users/tuyukai/go/src/github.com/carv-protocol/d.a.t.a/eliza/plugin-d.a.t.a/src/data_service.ts`:

```ts
import {
    IAgentRuntime,
    IMemoryManager,
    Service,
    Memory,
    MemoryManager,
} from "@elizaos/core";
import { stringToUuid, ServiceType } from "@elizaos/core";
import * as fs from "fs";
import * as path from "path";
import { txsArray } from "./txs.json.ts";
// const DATA_FILE_PATH = path.join(__dirname, "txs.json");

export const BLOCKCHAIN_DATA_TABLE_NAME = "d.a.t.a-blockchain-data";

type Transaction = {
    type: string;
    from: string;
    to: string;
    value: string;
    txHash: string;
    nonce: string;
    blockHash: string;
    blockNum: string;
};

export class DataService extends Service {
    static serviceType: ServiceType = ServiceType.INTIFACE;
    private runtime: IAgentRuntime | null = null;
    private blockchainDataManager: IMemoryManager | null = null;

    async initialize(runtime: IAgentRuntime): Promise<void> {
        console.log("Initializing carv data service");
        this.runtime = runtime;

        this.blockchainDataManager = new MemoryManager({
            runtime: runtime,
            tableName: BLOCKCHAIN_DATA_TABLE_NAME,
        });

        this.runtime.registerMemoryManager(this.blockchainDataManager);

        // console.log("Reading transactions from file...", DATA_FILE_PATH);
        // const txs = await readJsonFile(DATA_FILE_PATH);
        const txs = txsArray;
        for (const tx of txs) {
            console.log("Adding transaction to memory:", tx);

            const txInfo = formatTransaction(tx);
            const memory: Memory = {
                id: stringToUuid(txInfo),
                content: {
                    text: txInfo,
                },
                roomId: runtime.agentId,
                agentId: runtime.agentId,
                userId: runtime.agentId,
                createdAt: Date.now(),
            };
            await this.blockchainDataManager.addEmbeddingToMemory(memory);
            console.log("got memory:!!!!!");
            await this.blockchainDataManager.createMemory(memory);
        }
    }

    getBlockchainDataTableName(): string {
        return BLOCKCHAIN_DATA_TABLE_NAME;
    }
}

async function readJsonFile(filePath: string): Promise<Transaction[]> {
    try {
        // Read the file content
        const fileContent = await fs.readFileSync(filePath, "utf-8");

        // Parse the JSON content
        const jsonArray = JSON.parse(fileContent);

        // Check if the content is an array
        if (!Array.isArray(jsonArray)) {
            throw new Error("The file content is not a list of JSON objects.");
        }

        // Return the JSON objects
        return jsonArray as Transaction[];
    } catch (error) {
        console.error("Error reading or parsing the file:", error);
        throw error;
    }
}

function formatTransaction(transaction: Transaction): string {
    return `Transaction of type ${transaction.type} from ${transaction.from} to ${transaction.to} with value ${transaction.value}. Transaction hash: ${transaction.txHash}, nonce: ${transaction.nonce}, block hash: ${transaction.blockHash}, block number: ${transaction.blockNum}.`;
}

```

`/Users/tuyukai/go/src/github.com/carv-protocol/d.a.t.a/README.md`:

```md

# D.A.T.A
The D.A.T.A Framework (Data Authentication, Trust, and Attestation) is a cutting-edge solution developed by CARV to empower AI agents with unparalleled access to high-quality on-chain and off-chain data. Designed as a plugin for AI frameworks like Eliza, D.A.T.A enables AI agents to fetch, process, and act on data autonomously, fostering a new era of intelligent, data-driven decision-making.

## Documents

[D.A.T.A Framework](https://docs.carv.io/d.a.t.a.-ai-framework/introduction)

At its core, D.A.T.A bridges the gap between raw data and actionable insights by providing a complete lifecycle for data interaction. With robust integration into blockchain networks like Ethereum and Solana, as well as access to off-chain information through advanced tools such as vector databases, D.A.T.A equips AI agents to understand, interact with, and respond to the world more effectively.

Key highlights of the D.A.T.A Framework:

- On-Chain Data Access: Fetch blockchain data, such as balances, transaction histories, and activity metrics, through scalable backend architectures using tools like AWS Lambda, Google Cloud Functions, and Amazon Athena.
- Off-Chain Data Integration: Enrich on-chain insights with contextual data, including user profiles, token metadata, and market information.
- Autonomous Decision-Making: Allow AI agents to determine and execute actions based on data, from sending alerts to performing on-chain transactions.
- Cross-Chain Insights: Aggregate and unify data across multiple blockchains, enabling comprehensive understanding and decision-making.
- Memory Sharing: Enable collaborative intelligence among AI agents through shared on-chain memory and a centralized knowledge repository.

The D.A.T.A Framework is a game-changer for developers, providing the tools needed to build powerful, intelligent AI agents that can autonomously interact with decentralized ecosystems, make data-driven decisions, and evolve over time.
```

`/Users/tuyukai/go/src/github.com/carv-protocol/d.a.t.a/zerepy/d.a.t.a_connection.py`:

```py
import logging
import json
import os
from typing import Dict, Any, List, Optional, TypedDict, Union
from datetime import datetime, timedelta
import aiohttp
from dotenv import load_dotenv, set_key
from src.connections.base_connection import BaseConnection, Action, ActionParameter

logger = logging.getLogger("connections.data_connection")

class DataConnectionError(Exception):
    """Base exception for Data connection errors"""
    pass

class DataConfigurationError(DataConnectionError):
    """Raised when there are configuration/credential issues"""
    pass

class DataAPIError(DataConnectionError):
    """Raised when Data API requests fail"""
    pass

class QueryResult(TypedDict):
    success: bool
    data: List[Any]
    metadata: Dict[str, Any]
    error: Optional[Dict[str, Any]]

class ApiResponse(TypedDict):
    code: int
    msg: str
    data: Dict[str, Any]

class DataConnection(BaseConnection):
    def __init__(self, config: Dict[str, Any]):
        super().__init__(config)
        self.chain = config.get("chain", "ethereum-mainnet")
        self.api_url = os.getenv("DATA_API_KEY")
        self.auth_token = os.getenv("DATA_AUTH_TOKEN")

    @property
    def is_llm_provider(self) -> bool:
        return False

    def validate_config(self, config: Dict[str, Any]) -> Dict[str, Any]:
        """Validate Data configuration from JSON"""
        required_fields = ["chain"]
        missing_fields = [field for field in required_fields if field not in config]
        if missing_fields:
            raise ValueError(f"Missing required configuration fields: {', '.join(missing_fields)}")
        return config

    def register_actions(self) -> None:
        """Register available Data actions"""
        self.actions = {
            "execute-query": Action(
                name="execute-query",
                parameters=[
                    ActionParameter("sql", True, str, "SQL query to execute"),
                ],
                description="Execute a SQL query on the blockchain data"
            ),
            "get-schema": Action(
                name="get-schema",
                parameters=[],
                description="Get the database schema"
            ),
            "get-examples": Action(
                name="get-examples",
                parameters=[],
                description="Get query examples"
            )
        }

    def _extract_sql_query(self, pre_response: Union[str, Dict[str, Any]]) -> Optional[str]:
        """Extract SQL query from response"""
        try:
            # Parse JSON if string
            json_data = pre_response
            if isinstance(pre_response, str):
                try:
                    json_data = json.loads(pre_response)
                except json.JSONDecodeError:
                    logger.error("Failed to parse pre_response as JSON")
                    return None

            def find_sql_query(obj: Any) -> Optional[str]:
                # Base cases
                if not obj:
                    return None

                # String case
                if isinstance(obj, str):
                    sql_pattern = r'^\s*(SELECT|WITH)\s+[\s\S]+?(?:;|$)'
                    comment_pattern = r'--.*$|\/\*[\s\S]*?\*\/'
                    
                    # Clean and validate string
                    clean_str = obj.strip()
                    if not clean_str:
                        return None
                        
                    # Check for unsafe keywords
                    unsafe_keywords = ['drop', 'delete', 'update', 'insert', 'alter', 'create']
                    if any(keyword in clean_str.lower() for keyword in unsafe_keywords):
                        return None
                        
                    import re
                    if re.match(sql_pattern, clean_str, re.IGNORECASE):
                        return clean_str
                    return None

                # Array case
                if isinstance(obj, list):
                    for item in obj:
                        result = find_sql_query(item)
                        if result:
                            return result
                    return None

                # Object case
                if isinstance(obj, dict):
                    # Prioritize 'query' field in sql object
                    if 'query' in obj and 'sql' in obj:
                        result = find_sql_query(obj['query'])
                        if result:
                            return result

                    # Search other fields
                    for value in obj.values():
                        result = find_sql_query(value)
                        if result:
                            return result
                    return None

                return None

            sql_query = find_sql_query(json_data)
            if not sql_query:
                logger.warning("No valid SQL query found in pre_response")
            return sql_query

        except Exception as e:
            logger.error(f"Error in extract_sql_query: {str(e)}")
            return None

    async def _send_sql_query(self, sql: str) -> ApiResponse:
        """Send SQL query to API"""
        try:
            async with aiohttp.ClientSession() as session:
                async with session.post(
                    self.api_url,
                    headers={
                        "Content-Type": "application/json",
                        "Authorization": self.auth_token
                    },
                    json={"sql_content": sql}
                ) as response:
                    if response.status != 200:
                        raise DataAPIError(f"HTTP error! status: {response.status}")
                    return await response.json()
        except Exception as e:
            logger.error(f"Error sending SQL query to API: {str(e)}")
            raise

    def _transform_api_response(self, api_response: ApiResponse) -> List[Dict[str, Any]]:
        """Transform API response data"""
        column_infos = api_response["data"]["column_infos"]
        rows = api_response["data"]["rows"]

        transformed_data = []
        for row in rows:
            row_data = {}
            for i, value in enumerate(row["items"]):
                column_name = column_infos[i]
                row_data[column_name] = value
            transformed_data.append(row_data)

        return transformed_data

    async def execute_query(self, sql: str) -> QueryResult:
        """Execute query with proper error handling"""
        try:
            # Validate query
            if not sql or len(sql) > 5000:
                raise DataAPIError("Invalid SQL query length")

            # Determine query type
            query_type = "token" if "token_transfers" in sql.lower() else \
                        "aggregate" if "count" in sql.lower() else \
                        "transaction"

            # Send query
            api_response = await self._send_sql_query(sql)

            # Check response status
            if api_response["code"] != 0:
                raise DataAPIError(f"API Error: {api_response['msg']}")

            # Transform data
            transformed_data = self._transform_api_response(api_response)

            return {
                "success": True,
                "data": transformed_data,
                "metadata": {
                    "total": len(transformed_data),
                    "queryTime": datetime.now().isoformat(),
                    "queryType": query_type,
                    "executionTime": 0,
                    "cached": False
                },
                "error": None
            }

        except Exception as e:
            logger.error(f"Query execution failed: {str(e)}")
            return {
                "success": False,
                "data": [],
                "metadata": {
                    "total": 0,
                    "queryTime": datetime.now().isoformat(),
                    "queryType": "unknown",
                    "executionTime": 0,
                    "cached": False
                },
                "error": {
                    "code": getattr(e, "code", "EXECUTION_ERROR"),
                    "message": str(e),
                    "details": str(e)
                }
            }

    def get_database_schema(self) -> str:
        """Get database schema"""
        return """
        CREATE EXTERNAL TABLE transactions(
            hash string,
            nonce bigint,
            block_hash string,
            block_number bigint,
            block_timestamp timestamp,
            date string,
            transaction_index bigint,
            from_address string,
            to_address string,
            value double,
            gas bigint,
            gas_price bigint,
            input string,
            max_fee_per_gas bigint,
            max_priority_fee_per_gas bigint,
            transaction_type bigint
        ) PARTITIONED BY (date string)
        ROW FORMAT SERDE 'org.apache.hadoop.hive.ql.io.parquet.serde.ParquetHiveSerDe'
        STORED AS INPUTFORMAT 'org.apache.hadoop.hive.ql.io.parquet.MapredParquetInputFormat'
        OUTPUTFORMAT 'org.apache.hadoop.hive.ql.io.parquet.MapredParquetOutputFormat';

        CREATE EXTERNAL TABLE token_transfers(
            token_address string,
            from_address string,
            to_address string,
            value double,
            transaction_hash string,
            log_index bigint,
            block_timestamp timestamp,
            date string,
            block_number bigint,
            block_hash string
        ) PARTITIONED BY (date string)
        ROW FORMAT SERDE 'org.apache.hadoop.hive.ql.io.parquet.serde.ParquetHiveSerDe'
        STORED AS INPUTFORMAT 'org.apache.hadoop.hive.ql.io.parquet.MapredParquetInputFormat'
        OUTPUTFORMAT 'org.apache.hadoop.hive.ql.io.parquet.MapredParquetOutputFormat';
        """

    def get_query_examples(self) -> str:
        """Get query examples"""
        return """
        Common Query Examples:

        1. Find Most Active Addresses in Last 7 Days:
        WITH address_activity AS (
            SELECT
                from_address AS address,
                COUNT(*) AS tx_count
            FROM
                eth.transactions
            WHERE date_parse(date, '%Y-%m-%d') >= date_add('day', -7, current_date)
            GROUP BY
                from_address
            UNION ALL
            SELECT
                to_address AS address,
                COUNT(*) AS tx_count
            FROM
                eth.transactions
            WHERE
                date_parse(date, '%Y-%m-%d') >= date_add('day', -7, current_date)
            GROUP BY
                to_address
        )
        SELECT
            address,
            SUM(tx_count) AS total_transactions
        FROM
            address_activity
        GROUP BY
            address
        ORDER BY
            total_transactions DESC
        LIMIT 10;

        2. Analyze Address Transaction Statistics (Last 30 Days):
        WITH recent_transactions AS (
            SELECT
                from_address,
                to_address,
                value,
                block_timestamp,
                CASE
                    WHEN from_address = :address THEN 'outgoing'
                    WHEN to_address = :address THEN 'incoming'
                    ELSE 'other'
                END AS transaction_type
            FROM eth.transactions
            WHERE date >= date_format(date_add('day', -30, current_date), '%Y-%m-%d')
                AND (from_address = :address OR to_address = :address)
        )
        SELECT
            transaction_type,
            COUNT(*) AS transaction_count,
            SUM(CASE WHEN transaction_type = 'outgoing' THEN value ELSE 0 END) AS total_outgoing_value,
            SUM(CASE WHEN transaction_type = 'incoming' THEN value ELSE 0 END) AS total_incoming_value
        FROM recent_transactions
        GROUP BY transaction_type;
        """

    def configure(self) -> bool:
        """Configure the Data connection"""
        logger.info("\n📊 DATA API SETUP")

        if self.is_configured():
            logger.info("\nData API is already configured.")
            response = input("Do you want to reconfigure? (y/n): ")
            if response.lower() != 'y':
                return True

        logger.info("\n📝 Please enter your Data API credentials:")
        api_key = input("Enter your Data API URL: ")
        auth_token = input("Enter your Data Auth Token: ")

        try:
            if not os.path.exists('.env'):
                with open('.env', 'w') as f:
                    f.write('')

            set_key('.env', 'DATA_API_KEY', api_key)
            set_key('.env', 'DATA_AUTH_TOKEN', auth_token)

            logger.info("\n✅ Data API configuration successfully saved!")
            return True

        except Exception as e:
            logger.error(f"Configuration failed: {e}")
            return False

    def is_configured(self, verbose: bool = False) -> bool:
        """Check if Data API credentials are configured"""
        try:
            load_dotenv()
            api_key = os.getenv('DATA_API_KEY')
            auth_token = os.getenv('DATA_AUTH_TOKEN')

            if not api_key or not auth_token:
                if verbose:
                    logger.info("Data API credentials not found")
                return False

            return True

        except Exception as e:
            if verbose:
                logger.debug(f"Configuration check failed: {e}")
            return False

    def perform_action(self, action_name: str, kwargs: Dict[str, Any]) -> Any:
        """Execute a Data action with validation"""
        if action_name not in self.actions:
            raise KeyError(f"Unknown action: {action_name}")

        action = self.actions[action_name]
        errors = action.validate_params(kwargs)
        if errors:
            raise ValueError(f"Invalid parameters: {', '.join(errors)}")

        method_name = action_name.replace('-', '_')
        method = getattr(self, method_name)
        return method(**kwargs)
```
```

`/Users/tuyukai/go/src/github.com/carv-protocol/d.a.t.a/zerepy/d.a.t.a_connection.py`:

```py
import logging
import json
import os
from typing import Dict, Any, List, Optional, TypedDict, Union
from datetime import datetime, timedelta
import aiohttp
from dotenv import load_dotenv, set_key
from src.connections.base_connection import BaseConnection, Action, ActionParameter

logger = logging.getLogger("connections.data_connection")

class DataConnectionError(Exception):
    """Base exception for Data connection errors"""
    pass

class DataConfigurationError(DataConnectionError):
    """Raised when there are configuration/credential issues"""
    pass

class DataAPIError(DataConnectionError):
    """Raised when Data API requests fail"""
    pass

class QueryResult(TypedDict):
    success: bool
    data: List[Any]
    metadata: Dict[str, Any]
    error: Optional[Dict[str, Any]]

class ApiResponse(TypedDict):
    code: int
    msg: str
    data: Dict[str, Any]

class DataConnection(BaseConnection):
    def __init__(self, config: Dict[str, Any]):
        super().__init__(config)
        self.chain = config.get("chain", "ethereum-mainnet")
        self.api_url = os.getenv("DATA_API_KEY")
        self.auth_token = os.getenv("DATA_AUTH_TOKEN")

    @property
    def is_llm_provider(self) -> bool:
        return False

    def validate_config(self, config: Dict[str, Any]) -> Dict[str, Any]:
        """Validate Data configuration from JSON"""
        required_fields = ["chain"]
        missing_fields = [field for field in required_fields if field not in config]
        if missing_fields:
            raise ValueError(f"Missing required configuration fields: {', '.join(missing_fields)}")
        return config

    def register_actions(self) -> None:
        """Register available Data actions"""
        self.actions = {
            "execute-query": Action(
                name="execute-query",
                parameters=[
                    ActionParameter("sql", True, str, "SQL query to execute"),
                ],
                description="Execute a SQL query on the blockchain data"
            ),
            "get-schema": Action(
                name="get-schema",
                parameters=[],
                description="Get the database schema"
            ),
            "get-examples": Action(
                name="get-examples",
                parameters=[],
                description="Get query examples"
            )
        }

    def _extract_sql_query(self, pre_response: Union[str, Dict[str, Any]]) -> Optional[str]:
        """Extract SQL query from response"""
        try:
            # Parse JSON if string
            json_data = pre_response
            if isinstance(pre_response, str):
                try:
                    json_data = json.loads(pre_response)
                except json.JSONDecodeError:
                    logger.error("Failed to parse pre_response as JSON")
                    return None

            def find_sql_query(obj: Any) -> Optional[str]:
                # Base cases
                if not obj:
                    return None

                # String case
                if isinstance(obj, str):
                    sql_pattern = r'^\s*(SELECT|WITH)\s+[\s\S]+?(?:;|$)'
                    comment_pattern = r'--.*$|\/\*[\s\S]*?\*\/'
                    
                    # Clean and validate string
                    clean_str = obj.strip()
                    if not clean_str:
                        return None
                        
                    # Check for unsafe keywords
                    unsafe_keywords = ['drop', 'delete', 'update', 'insert', 'alter', 'create']
                    if any(keyword in clean_str.lower() for keyword in unsafe_keywords):
                        return None
                        
                    import re
                    if re.match(sql_pattern, clean_str, re.IGNORECASE):
                        return clean_str
                    return None

                # Array case
                if isinstance(obj, list):
                    for item in obj:
                        result = find_sql_query(item)
                        if result:
                            return result
                    return None

                # Object case
                if isinstance(obj, dict):
                    # Prioritize 'query' field in sql object
                    if 'query' in obj and 'sql' in obj:
                        result = find_sql_query(obj['query'])
                        if result:
                            return result

                    # Search other fields
                    for value in obj.values():
                        result = find_sql_query(value)
                        if result:
                            return result
                    return None

                return None

            sql_query = find_sql_query(json_data)
            if not sql_query:
                logger.warning("No valid SQL query found in pre_response")
            return sql_query

        except Exception as e:
            logger.error(f"Error in extract_sql_query: {str(e)}")
            return None

    async def _send_sql_query(self, sql: str) -> ApiResponse:
        """Send SQL query to API"""
        try:
            async with aiohttp.ClientSession() as session:
                async with session.post(
                    self.api_url,
                    headers={
                        "Content-Type": "application/json",
                        "Authorization": self.auth_token
                    },
                    json={"sql_content": sql}
                ) as response:
                    if response.status != 200:
                        raise DataAPIError(f"HTTP error! status: {response.status}")
                    return await response.json()
        except Exception as e:
            logger.error(f"Error sending SQL query to API: {str(e)}")
            raise

    def _transform_api_response(self, api_response: ApiResponse) -> List[Dict[str, Any]]:
        """Transform API response data"""
        column_infos = api_response["data"]["column_infos"]
        rows = api_response["data"]["rows"]

        transformed_data = []
        for row in rows:
            row_data = {}
            for i, value in enumerate(row["items"]):
                column_name = column_infos[i]
                row_data[column_name] = value
            transformed_data.append(row_data)

        return transformed_data

    async def execute_query(self, sql: str) -> QueryResult:
        """Execute query with proper error handling"""
        try:
            # Validate query
            if not sql or len(sql) > 5000:
                raise DataAPIError("Invalid SQL query length")

            # Determine query type
            query_type = "token" if "token_transfers" in sql.lower() else \
                        "aggregate" if "count" in sql.lower() else \
                        "transaction"

            # Send query
            api_response = await self._send_sql_query(sql)

            # Check response status
            if api_response["code"] != 0:
                raise DataAPIError(f"API Error: {api_response['msg']}")

            # Transform data
            transformed_data = self._transform_api_response(api_response)

            return {
                "success": True,
                "data": transformed_data,
                "metadata": {
                    "total": len(transformed_data),
                    "queryTime": datetime.now().isoformat(),
                    "queryType": query_type,
                    "executionTime": 0,
                    "cached": False
                },
                "error": None
            }

        except Exception as e:
            logger.error(f"Query execution failed: {str(e)}")
            return {
                "success": False,
                "data": [],
                "metadata": {
                    "total": 0,
                    "queryTime": datetime.now().isoformat(),
                    "queryType": "unknown",
                    "executionTime": 0,
                    "cached": False
                },
                "error": {
                    "code": getattr(e, "code", "EXECUTION_ERROR"),
                    "message": str(e),
                    "details": str(e)
                }
            }

    def get_database_schema(self) -> str:
        """Get database schema"""
        return """
        CREATE EXTERNAL TABLE transactions(
            hash string,
            nonce bigint,
            block_hash string,
            block_number bigint,
            block_timestamp timestamp,
            date string,
            transaction_index bigint,
            from_address string,
            to_address string,
            value double,
            gas bigint,
            gas_price bigint,
            input string,
            max_fee_per_gas bigint,
            max_priority_fee_per_gas bigint,
            transaction_type bigint
        ) PARTITIONED BY (date string)
        ROW FORMAT SERDE 'org.apache.hadoop.hive.ql.io.parquet.serde.ParquetHiveSerDe'
        STORED AS INPUTFORMAT 'org.apache.hadoop.hive.ql.io.parquet.MapredParquetInputFormat'
        OUTPUTFORMAT 'org.apache.hadoop.hive.ql.io.parquet.MapredParquetOutputFormat';

        CREATE EXTERNAL TABLE token_transfers(
            token_address string,
            from_address string,
            to_address string,
            value double,
            transaction_hash string,
            log_index bigint,
            block_timestamp timestamp,
            date string,
            block_number bigint,
            block_hash string
        ) PARTITIONED BY (date string)
        ROW FORMAT SERDE 'org.apache.hadoop.hive.ql.io.parquet.serde.ParquetHiveSerDe'
        STORED AS INPUTFORMAT 'org.apache.hadoop.hive.ql.io.parquet.MapredParquetInputFormat'
        OUTPUTFORMAT 'org.apache.hadoop.hive.ql.io.parquet.MapredParquetOutputFormat';
        """

    def get_query_examples(self) -> str:
        """Get query examples"""
        return """
        Common Query Examples:

        1. Find Most Active Addresses in Last 7 Days:
        WITH address_activity AS (
            SELECT
                from_address AS address,
                COUNT(*) AS tx_count
            FROM
                eth.transactions
            WHERE date_parse(date, '%Y-%m-%d') >= date_add('day', -7, current_date)
            GROUP BY
                from_address
            UNION ALL
            SELECT
                to_address AS address,
                COUNT(*) AS tx_count
            FROM
                eth.transactions
            WHERE
                date_parse(date, '%Y-%m-%d') >= date_add('day', -7, current_date)
            GROUP BY
                to_address
        )
        SELECT
            address,
            SUM(tx_count) AS total_transactions
        FROM
            address_activity
        GROUP BY
            address
        ORDER BY
            total_transactions DESC
        LIMIT 10;

        2. Analyze Address Transaction Statistics (Last 30 Days):
        WITH recent_transactions AS (
            SELECT
                from_address,
                to_address,
                value,
                block_timestamp,
                CASE
                    WHEN from_address = :address THEN 'outgoing'
                    WHEN to_address = :address THEN 'incoming'
                    ELSE 'other'
                END AS transaction_type
            FROM eth.transactions
            WHERE date >= date_format(date_add('day', -30, current_date), '%Y-%m-%d')
                AND (from_address = :address OR to_address = :address)
        )
        SELECT
            transaction_type,
            COUNT(*) AS transaction_count,
            SUM(CASE WHEN transaction_type = 'outgoing' THEN value ELSE 0 END) AS total_outgoing_value,
            SUM(CASE WHEN transaction_type = 'incoming' THEN value ELSE 0 END) AS total_incoming_value
        FROM recent_transactions
        GROUP BY transaction_type;
        """

    def configure(self) -> bool:
        """Configure the Data connection"""
        logger.info("\n📊 DATA API SETUP")

        if self.is_configured():
            logger.info("\nData API is already configured.")
            response = input("Do you want to reconfigure? (y/n): ")
            if response.lower() != 'y':
                return True

        logger.info("\n📝 Please enter your Data API credentials:")
        api_key = input("Enter your Data API URL: ")
        auth_token = input("Enter your Data Auth Token: ")

        try:
            if not os.path.exists('.env'):
                with open('.env', 'w') as f:
                    f.write('')

            set_key('.env', 'DATA_API_KEY', api_key)
            set_key('.env', 'DATA_AUTH_TOKEN', auth_token)

            logger.info("\n✅ Data API configuration successfully saved!")
            return True

        except Exception as e:
            logger.error(f"Configuration failed: {e}")
            return False

    def is_configured(self, verbose: bool = False) -> bool:
        """Check if Data API credentials are configured"""
        try:
            load_dotenv()
            api_key = os.getenv('DATA_API_KEY')
            auth_token = os.getenv('DATA_AUTH_TOKEN')

            if not api_key or not auth_token:
                if verbose:
                    logger.info("Data API credentials not found")
                return False

            return True

        except Exception as e:
            if verbose:
                logger.debug(f"Configuration check failed: {e}")
            return False

    def perform_action(self, action_name: str, kwargs: Dict[str, Any]) -> Any:
        """Execute a Data action with validation"""
        if action_name not in self.actions:
            raise KeyError(f"Unknown action: {action_name}")

        action = self.actions[action_name]
        errors = action.validate_params(kwargs)
        if errors:
            raise ValueError(f"Invalid parameters: {', '.join(errors)}")

        method_name = action_name.replace('-', '_')
        method = getattr(self, method_name)
        return method(**kwargs)
```